{"./":{"url":"./","title":"Introduction","keywords":"","body":"GitBook 使用教程 Jupyter jupyter入门之Matplotlib jupyter入门之numpy jupyter入门之pandas jupyter入门之pandas（完） jupyter入门之图像处理 python基础 Dateutil ZIP 前端 vue-cli 3.0版本webpack打包 前端vue优化 后端 django整理 crm管理系统 Django Debug Toolbar的安装及使用 Django Redis django中将重要信息存储并引用 drf技术扩展 Memcached Nginx+Gunicorn部署django 七牛云存储资源 数据库索引相关操作 用户权限验证 简约博客 项目总结 flask整理 Flask+Vue快速打造个人网站（一） Flask+Vue快速打造个人网站（三） Flask+Vue快速打造个人网站（二） flask应用 flask进阶 I Survey python脚本实现Mp4在Safari播放 uwsgi部署flask项目 上传文件接口 常见认证机制 Celery token机制 web安全防范 关于主流web框架选择问题 前后端分离项目之python后端部署前的操作 后台框架选型 后端记录 多人协作项目git免密登录 工具 gitbook入门 Markdown 常见坑 pip安装遇到的坑 Reactive-native 运行错误 服务器错误排查 扩展 Item 2 MAC上比较好用的开发工具 RPC,微服务架构 with关键字与上下文管理 前端高级工程师进阶扩展 数据库 Mongo Mongo Engine NoSQL数据库 Postgresql Related Name 机器学习 SVM与K-Means Tensor Flow 决策树与贝叶斯 特征工程 用机器学习的方法来学习机器学习 题海 爬虫 10 分布式爬虫 boss直聘 csdn爬虫 get.post请求及cookie pyspider使用 scrapy入门 scrapy实战 selenium使用 spider项目 tinyproxy代理服务器的搭建 tinyproxy定时任务 使用CrawlSpider做通用爬虫 如何使你的爬虫更健壮 常用工具、多线程及分布式 异步IO 爬虫常见问题 爬虫进阶（入门） 简单爬虫 线程锁 进程线程 进程线程协程简单关系 电子书 《GitHub入门与实践》一书小记 运维 centos新服务器使用 Docker Django Doker了解 Linux Pro LinuxPro（二） osi模型：应表会传网数物 七层协议模型 shell，解放我的双手 ssh免密登陆 Supervisor supervisor管理进程 使用dockerfile自定义镜像 使用supervisor进行scrapy-redis部署 开发模式 管理容器 面试点 基础面试 目前掌握的技术 直击痛点的python面试题 项目面试 Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"jupyter/jupyter入门之Matplotlib.html":{"url":"jupyter/jupyter入门之Matplotlib.html","title":"jupyter入门之Matplotlib","keywords":"","body":"jupyter入门之Matplotlib 一. Matplotlib基础知识 Matplotlib中的基本图表包括的元素 x轴和y轴 axis 水平和垂直的轴线 轴标签 axisLabel 水平和垂直的轴标签 x轴和y轴刻度 tick 刻度标示坐标轴的分隔，包括最小刻度和最大刻度 x轴和y轴刻度标签 tick label 表示特定坐标轴的值 绘图区域（坐标系） axes 实际绘图的区域 画布 figure 呈现所有的坐标系 可以用下图来概括 1.子画布 依赖导入 import numpy as np import pandas as pd from pandas import Series,DataFrame import matplotlib.pyplot as plt %matplotlib inline 只含单一曲线的图 1、可以使用多个plot函数（推荐），在一个图中绘制多个曲线 2、也可以在一个plot函数中传入多对X,Y值，在一个图中绘制多个曲线 x = np.arange(0,10,step=1) s1 = Series(x,index=list('abcdefghij')) s2 = Series(x**2,index=s1.index) # 索引和值一起设置 plt.plot(x,x*2,x,x*3,x,x) 设置子画布 axes = plt.subplot() # 221的含义是将画布分为两行两列， # axes1在第一个位置 axes1 = plt.subplot(2,2,1) axes1.plot(x,x**2) # axes2在第四个位置 axes2 = plt.subplot(2,2,4) axes2.plot(x,x**3) # 子画布可以按照不同的比例进行拆分，可以在一个版面内共存， # 但是，如果两个子画布有重叠部分，后绘制的画布会覆盖先绘制的画布 axes2 = plt.subplot(2,2,1) axes2.plot(x,x**2) axes1 = plt.subplot(3,3,1) axes1.plot(x,x*2) 网格线 2. 绘制正弦余弦 x = np.linspace(0,2*np.pi,100) y1 = np.sin(x) y2 = np.cos(x) # 自动选择最近的画布进行开启 plt.grid(True) plt.plot(x,y1,x,y2) axes1 = plt.subplot(221) # True 开启网格线 # linewidth设置网格线的粗细 axes1.grid(True,linewidth=2) axes2 = plt.subplot(222) # alpha设置透明度 0-1之间的数 axes2.grid(True,alpha=0.5) axes3 = plt.subplot(223) axes3.grid(True,color='green') axes4 = plt.subplot(224) # 设置网格线方向 # linestyle/ls设置虚线 axes4.grid(True,axis='x') 使用plt.grid方法可以开启网格线，使用plt面向对象的方法，创建多个子图显示不同网格线 lw代表linewidth，线的粗细 alpha表示线的明暗程度 color代表颜色 axis显示轴向 坐标轴界限 plt.axis([xmin,xmax,ymin,ymax]) plt.plot(x,y1) # 注意：必须x,y轴同时设置界限 plt.axis([-1,7.2,-2,2]) # 画圆 x = np.linspace(-1,1,100) f = lambda x:(1-x**2)**0.5 plt.plot(x,f(x),x,-f(x)) # 使用axis函数设置坐标轴的显示风格 plt.axis('equal') axes = plt.subplot(111) axes.plot(x,f(x),x,-f(x)) axes.axis('equal') xlim方法和ylim方法 除了plt.axis方法，还可以通过xlim，ylim方法设置坐标轴范围 # 分别设置x轴和y轴界限 plt.plot(x,y2) plt.xlim(-2,20) plt.ylim(-2,2) 面向对象风格设置画布样式 axes = plt.subplot(111) axes.plot(x,y2) # 使用面向对象风格设置画布样式，如果属性找不到，就使用set_XX函数设置 # .+Tab键,快捷查看对象属性和方法 axes.set_xlim(-5,5) axes.set_ylim(-5,5) 坐标轴标签 xlabel方法和ylabel方法 plt.ylabel('y = x^2 + 5',rotation = 60)旋转 color 标签颜色 fontsize 字体大小 rotation 旋转角度 plt.plot(x,y1) plt.xlabel('x_label',fontsize=35) plt.ylabel('y-label',color='pink',fontsize=40) axes = plt.subplot(111) axes.plot(x,y1) # HTML支持的字体设置，在这里都可以使用color fontsize linestyle linewidth # fontsize 字体大小 # color 字体颜色 # rotation 旋转角度 axes.set_ylabel('y_label',fontsize=30,color='red',rotation=90) # 使用字典设置字体 axes.set_xlabel('x_label',fontdict={ 'fontsize':40, 'color':'blue' }) 标题 plt.title()方法 loc {left,center,right} color 标签颜色 fontsize 字体大小 rotation 旋转角度 plt.plot(x,y1) # loc设置标题显示位置（left,right,center） plt.title('SIN(X)',fontsize=60,color='purple',rotation=45,loc='left') 注：matplotlib显示中文如何处理? Windows： from pylab import mpl # 指定默认字体 mpl.rcParams['font.sans-serif'] = ['FangSong'] # 解决保存图像是负号'-'显示为方块的问题 mpl.rcParams['axes.unicode_minus'] = False Mac: import numpy as np import matplotlib import matplotlib.pyplot as plt from matplotlib.font_manager import FontProperties # 导入本地字体文件 font = FontProperties(fname='/Library/Fonts/Songti.ttc') # Fixing random state for reproducibility np.random.seed(19680801) # 设置为FALSE 解决负号显示为框的问题 matplotlib.rcParams['axes.unicode_minus'] = False fig, ax = plt.subplots() ax.plot(10*np.random.randn(100), 10*np.random.randn(100), 'o') # 用fontproperties参数设置字体为自定义字体 ax.set_title(u'散点图',fontproperties=font,fontsize=30) plt.show() 3.图例 原始方法 # 相关依赖导入 ... df = DataFrame(data=np.random.randint(0,100,size=(5,3)),columns=list('abc')) df.plot() legend方法 两种传参方法： 分别在plot函数中增加label参数,再调用legend()方法显示 直接在legend方法中传入字符串列表 x = np.arange(0,10,step=1) plt.plot(x,x,x,x*2,x,x/2) # 传入一个列表参数，参数内容就是每一个data的说明 plt.legend(['fast','normal','slow']) 第二种传参 plt.plot(x,x,label='normal') plt.plot(x,x*2,label='fast') plt.plot(x,x/2,label='slow') plt.legend(loc=10) loc参数 loc参数用于设置图例标签的位置，一般在legend函数内 matplotlib已经预定义好几种数字表示的位置 loc参数可以是2元素的元组，表示图例左下角的坐标 [0,0] 左下 [0,1] 左上 [1,0] 右下 [1,1] 右上 图例也可以超过图的界限loc = (-0.1,0.9) plt.plot(x,x,label='normal') plt.plot(x,x*2,label='fast') plt.plot(x,x/2,label='slow') plt.legend(loc=[0.4,1.1],ncol=3) ncol参数 ncol控制图例中有几列,在legend中设置ncol,需要设置loc linestyle、color、marker 修改线条样式 4.保存图片 使用figure对象的savefig的函数 filename 含有文件路径的字符串或Python的文件型对象。图像格式由文件扩展名推断得出，例如，.pdf推断出PDF，.png推断出PNG （“png”、“pdf”、“svg”、“ps”、“eps”……） dpi 图像分辨率（每英寸点数），默认为100 facecolor 图像的背景色，默认为“w”（白色） # 获取figure对象（画布对象） # 设置画布的背景色 figure = plt.figure(facecolor='cyan') # 设置坐标系的背景色 axes = plt.subplot(111,facecolor='blue') # 设置线条的颜色 axes.plot(x,x,x,x*2,color='yellow') # dpi设置像素大小 figure.savefig('18011.png',dpi=50) 二. 设置plot的风格和样式 plot语句中支持除X,Y以外的参数，以字符串形式存在，来控制颜色、线型、点型等要素，语法形式为： plt.plot(X, Y, 'format', ...) 1.点和线的样式 颜色 参数color或c plt.plot(x,x,color='c') 常用色彩名与别名 透明度 alpha plt.plot(x,x,alpha=0.3,color='r') 背景色 设置背景色，通过plt.subplot()方法传入facecolor参数，来设置坐标系的背景色 # 设置坐标系的背景 plt.subplot(facecolor='red') plt.plot(x,np.sin(x)) 线型 参数linestyle或ls plt.plot(x,x,ls=':') plt.plot(x,x*2,ls='steps') plt.plot(x,x/2,ls='None') 常用线型 线宽 linewidth或lw参数 不同宽度的破折线 dashes参数 eg.dashes = [20,50,5,2,10,5] 设置破折号序列各段的宽度 # dashes自定义破折线的样式 plt.plot(x,x,dashes=[10,2,3,5]) 点型 marker 设置点形 markersize 设置点形大小 x = np.random.randint(0,10,size=6) plt.plot(x,marker='s',markersize=60) 常用点型参数设置 plt.plot(x,marker='4',markersize=30) plt.plot(x,x*2,marker='3',markersize=40) 标记 描述 标记 描述 's' 正方形 'p' 五边形 'h' 六边形1 'H' 六边形2 '8' 八边形 plt.plot(np.random.randint(0,10,size=10),marker='h',markersize=20,label='六边形1',) plt.plot(np.random.randint(0,10,size=10),marker='H',markersize=20,label='六边形2') plt.plot(np.random.randint(0,10,size=10),marker='8',markersize=20,label='八边形') plt.plot(np.random.randint(0,10,size=10),marker='p',markersize=20,label='五边形') plt.legend() 标记参数 2.多参数连用 颜色、点型、线型，可以把几种参数写在一个字符串内进行设置 'r-.o' # 参数连用需要对不同的线进行分别设置 plt.plot(x,x,'r:v',x,x*2,'b--h') 更多点和线的设置 markeredgecolor = 'green', markeredgewidth = 2, markerfacecolor = 'purple' 多个曲线同一设置 属性名声明,不可以多参数连用 plt.plot(x1, y1, x2, y2, fmt, ...) x = np.arange(0,10,1) # 统一设置 plt.plot(x,x,x,x*2,x,x/2,linewidth=3,color='blue') # 分别设置 plt.plot(x,x,'r-.s',x,x*2,'b--h') 多曲线不同设置 多个都进行设置时，多参数连用 plt.plot(x1, y1, fmt1, x2, y2, fmt2, ...) 三种设置方式 向方法传入关键字参数 import matplotlib as mpl 对实例使用一系列的setter方法 plt.plot()方法返回一个包含所有线的列表，设置每一个线需要获取该线对象 eg: lines = plt.plot(); line = lines[0] line.set_linewith() line.set_linestyle() line.set_color() 对坐标系使用一系列的setter方法 axes = plt.subplot()获取坐标系 set_title() set_facecolor() set_xticks、set_yticks 设置刻度值 set_xticklabels、set_yticklabels 设置刻度名称 例： lines = plt.plot(x,x,x,x*2,x,x/2) # 根据line对象设置line的属性 lines[0].set_linewidth(3) lines[1].set_color('cyan') lines[2].set_linestyle('--') X、Y轴坐标刻度 plt.xticks()和plt.yticks()方法 需指定刻度值和刻度名称 plt.xticks([刻度列表],[名称列表]) 支持fontsize、rotation、color等参数设置 x = np.linspace(0,2*np.pi,100) plt.plot(x,np.sin(x)) # xticks函数设置坐标轴刻度和标签 plt.xticks([0,np.pi/2,np.pi,3*np.pi/2,2*np.pi],['0','π/2','π','3π/2','2π']) axes = plt.subplot(111) axes.plot(x,np.sin(x)) # 使用画布对象设置坐标轴刻度和坐标轴刻度标签 axes.set_xticks([0,np.pi/2,np.pi,3*np.pi/2,2*np.pi]) axes.set_xticklabels(['0','$\\pi$/2','π','3π/2','2π'],fontsize=20) axes.set_yticks([-1,0,1]) axes.set_yticklabels(['min',0,'max'],fontsize=20) 三、2D图形 1.直方图 【直方图的参数只有一个x！！！不像条形图需要传入x,y】 hist()的参数 bins 可以是一个bin数量的整数值，也可以是表示bin的一个序列。默认值为10 normed 如果值为True，直方图的值将进行归一化处理，形成概率密度，默认值为False color 指定直方图的颜色。可以是单一颜色值或颜色的序列。如果指定了多个数据集合，颜色序列将会设置为相同的顺序。如果未指定，将会使用一个默认的线条颜色 orientation 通过设置orientation为horizontal创建水平直方图。默认值为vertical x = np.random.randint(0,100,size=100) plt.hist(x,bins=5) 2. 条形图 【条形图有两个参数x,y】 width 纵向设置条形宽度 height 横向设置条形高度 bar()、barh() x = np.array([2,5,7,9,4,3,8]) # x 就是底轴刻度标签 # height 就是柱形图的柱高 # width 表示柱宽 (0-1取值) plt.bar(x=list('abcdefg'),height=x,width=1) # y 就是底轴刻度标签 # width 就是柱形图的柱高 # height 表示柱宽 plt.barh(y=list('abcdefg'),width=x,height=1) 3. 饼图 【饼图也只有一个参数x！】 pie() 饼图适合展示各部分占总体的比例，条形图适合比较各部分的大小 普通部分占满饼图 x = [45,6] plt.pie(x) 普通未占满饼图 x = [0.4,0.3] plt.pie(x) 饼图阴影、分裂等属性设置 labels参数设置每一块的标签； labeldistance参数设置标签距离圆心的距离（比例值,只能设置一个浮点小数） autopct参数设置比例值的显示格式(%1.1f%%)； pctdistance参数设置比例值文字距离圆心的距离 explode参数设置每一块顶点距圆形的长度（比例值,列表）； colors参数设置每一块的颜色（列表）； shadow参数为布尔值，设置是否绘制阴影 startangle参数设置饼图起始角度 # labeldistance只能设置一个小数 plt.pie(x,labels=['男','女'],labeldistance=0.3, autopct=\"%1.2f%%\",pctdistance=0.8, explode=[0.1,0.0], colors=['blue','yellow'], startangle=90, shadow=True) 4. 散点图 【 散点图需要两个参数x,y，但此时x不是表示x轴的刻度，而是每个点的横坐标！】 scatter() x = np.random.normal(size=1000) y = np.random.normal(size=1000) # 生成1000个随机颜色 colors = np.random.random(size=(1000,3)) # 使用c参数来设置颜色 plt.scatter(x,y,marker='d',c=colors) 四.3D图 曲面图 导包 from mpl_toolkits.mplot3d.axes3d import Axes3D 使用mershgrid函数切割x,y轴 X,Y = np.meshgrid(x, y) 创建3d坐标系 axes = plt.subplot(projection='3d') 绘制3d图形 p = axes.plot_surface(X,Y,Z,color='red',cmap='summer',rstride=5,cstride=5) 添加colorbar plt.colorbar(p,shrink=0.5) 例> def createZ(x,y): return np.sin(x) + np.cos(y)*2 - np.pi/5 from mpl_toolkits.mplot3d.axes3d import Axes3D axes = plt.subplot(projection='3d') x = np.linspace(-np.pi,np.pi,100) y = np.linspace(-np.pi,np.pi,100) # 使用x,y生成一组网格 X,Y = np.meshgrid(x,y) Z = createZ(X,Y) p = axes.plot_surface(X,Y,Z,cmap='summer',rstride=5,cstride=5) plt.colorbar(p,shrink=0.5) 五.玫瑰图/极坐标条形图 创建极坐标，设置polar属性 plt.axes(polar = True) 绘制极坐标条形图 index = np.arange(-np.pi,np.pi,2*np.pi/6) plt.bar(x=index ,height = [1,2,3,4,5,6] ,width = 2*np.pi/6) # 玫瑰图 x = np.array([2,5,7,9,4,3,8]) plt.axes(polar = True) plt.bar(x=list('abcdefg'),height=x) 六.图形内的文字、注释、箭头 1.控制文字属性的方法 Prop 所有的方法会返回一个matplotlib.text.Text对象 # 画布 figure = plt.figure() # figure.text(x=1,y=1,s='这是figure对象的注释') figure.suptitle('figure title') # 等差数列 x = np.linspace(-np.pi,np.pi,100) axes1 = plt.subplot(221) axes1.plot(x,np.sin(x)) # x,y是以坐标轴刻度为准的一组数据，描述了注释信息的注释位置 axes1.text(x=0.5,y=0.5,s='这是一段注释') axes2 = plt.subplot(222) axes2.plot(x,np.cos(x),color='red') 2.图形内的文字 text() 3. 注释 annotate() xy参数设置箭头指示的位置 xytext参数设置注释文字的位置 arrowprops参数以字典的形式设置箭头的样式 width参数设置箭头长方形部分的宽度 headlength参数设置箭头尖端的长度， headwidth参数设置箭头尖端底部的宽度 shrink参数设置箭头顶点、尾部与指示点、注释文字的距离（比例值），可以理解为控制箭头的长度 figure = plt.figure(figsize=(8,6)) axes = plt.subplot(111) axes.plot(x,np.sin(x)) # s 注释的文字 # xy 箭头指向的位置 # xytext 注释文字放置的位置 # arrowprops没有arrowstyle键的设置方式 axes.annotate(s='this is a annotate',xy=(0.5,0.5),xytext=(0.8,0.8), arrowprops = { 'width':10, 'headlength':12, 'headwidth':15, 'shrink':0.2 }) # 使用arrowstyle设置箭头样式 axes.annotate(s='this is a good text',xy=(-0.2,-0.2),xytext=(-0.4,-0.6), arrowprops = { 'arrowstyle': 'fancy' }) Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"jupyter/jupyter入门之numpy.html":{"url":"jupyter/jupyter入门之numpy.html","title":"jupyter入门之numpy","keywords":"","body":"jupyter入门之numpy 安装 jupyter下载地址 下载安装成功之后打开 jupyter使用 点击base(root)后的小三角（open with Jupiter notebook） 默认打开8888端口 cell 每一个cell都可以放一段独立的代码, 独立运行 ctrl+enter 切换cell的编辑格式 在cell的选中状态下使用y/m y:code格式 markdown模式 删除cell x 删除选中cell dd 删除选中cell shift+tab查看api tab自动补全 新建文件 numpy语法 import numpy as np # 查看版本 np.__version__ # '1.14.3' 1. np.array a1 = np.array([1,2,3,4,5]) >>> array([1,2,3,4,5]) a2 = np.array([1.0,2,3,4,5]) >>> array([1.,2.,3.,4.,5.]) 2. 使用np的routines函数创建数组 np.ones(shape,) n1 = np.ones(shape=5) >>> array([1., 1., 1., 1., 1.]) n2 = np.ones(shape=(3,3)) >>> array([[1., 1., 1.], [1., 1., 1.], [1., 1., 1.]]) n3 = np.ones(shape=(3,3,2)) >>> array([[[1., 1.], [1., 1.], [1., 1.]], [[1., 1.], [1., 1.], [1., 1.]], [[1., 1.], [1., 1.], [1., 1.]]]) np.zeros # 5行3列以0填充 n4 = np.zeros(shape=(5,3)) np.full # 3行2列以6填充 n5 = np.full(shape=(3,2),fill_value=6) np.eye # 5行5列单位矩阵 np.eye(N=5) np.linspace # 0到10分为10等份 末尾不计 np.linspace(0,10,10,endpoint=False) >>> array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]) n.arange # start,stop,step np.arange(0,10,1) >>> array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) 3 正态分布 # 标准正太分布 方差为1，期望为0 np.random.randn(3,2) # 普通正太分布 指定方差和期望值 np.random.normal(loc=1.75,scale=0.3,size=10) ndarray属性 4个必记参数： ndim：维度 shape：形状（各维度的长度） size：总长度 dtype：元素类型 # 0-100随机 5行4列 n = np.random.randint(0,100,size=(5,4)) n.ndim >>> 2 n.shape >>> (5,4) n.size >>> 20 n.dtype >>> dtype=('int32') ndarray基本操作 1. 索引 n1[0] # numpy特有 方便取值 n1[0,0] 2. 切片 # start:stop:step n1[0:3] # 取反 n1[::-1] # 索引2，3行，之后取索引2，3列 n2[2:4,2:4] # 索引0,1列 n2[:,:2] # 主要看第几维在变 n3[:,:,:2] 3. 变形 # 变为1维数组 n2.reshape(36) n2.reshape(36,) # 1行36列 n2.reshape(1,36) # 36行1列 n2.reshape(36,1) 4. 级联 np.concatenate() 级联需要注意的点： 级联的参数是列表：一定要加中括号或小括号 维度必须相同 形状相符 【重点】级联的方向默认是shape这个tuple的第一个值所代表的维度方向 可通过axis参数改变级联的方向 np.concatenate((n1,n2),axis=1) h:horizontal 横向 v:vertical 纵向 np.hstack与np.vstack 水平级联与垂直级联,处理自己，进行维度的变更 5.切分 与级联类似，三个函数完成切分工作： np.split np.vsplit np.hsplit # 1.如果indices_or_sections设置为整数，必须保证在切割的维度上是可以被这个整数整除的 # 2.如果是1-D array,[m,n] 意味着按照如下方式切割 [0:m] [m:n] [n:] result = np.split(n,indices_or_sections=3,axis=1) 6.副本 # 浅拷贝 不会修改n1的值 cn = n1.copy() ndarray聚合操作 求和 np.sum 标准差 np.std() np.nan 最大最小值 np.max/np.min 其他运算 Function Name NaN-safe Version Description np.sum np.nansum Compute sum of elements np.prod np.nanprod Compute product of elements np.mean np.nanmean Compute mean of elements np.std np.nanstd Compute standard deviation np.var np.nanvar Compute variance np.min np.nanmin Find minimum value np.max np.nanmax Find maximum value np.argmin np.nanargmin Find index of minimum value np.argmax np.nanargmax Find index of maximum value np.median np.nanmedian Compute median of elements np.percentile np.nanpercentile Compute rank-based statistics of elements np.any N/A Evaluate whether any elements are true np.all N/A Evaluate whether all elements are true np.power 幂运算 ndarray的矩阵操作 1. 加减乘除 2. 矩阵积np.dot() n3 = np.array([[1,2],[3,4]]) np.dot(n3,n3) >>> array([[ 7, 10], [15, 22]]) 3. 广播机制(重点) 【重要】ndarray广播机制的两条规则 规则一：为缺失的维度补1 规则二：假定缺失元素用已有值填充 # 计算一个数组中每个元素与平均值的差 n1 = np.random.randint(0,100,size=10) n1-n1.mean() # axis=1求每一行的平均值 n2.mean(axis=1).reshape((-1,1)) # 求每一行的每一个元素与该行的平均值的差 n2 - n2.mean(axis=1).reshape((-1,1)) 1. 快速排序 np.sort()与ndarray.sort()都可以，但有区别： np.sort()不改变输入 ndarray.sort()本地处理，不占用空间，但改变输入 2. 部分排序 np.partition(a,k) 有的时候我们不是对全部数据感兴趣，我们可能只对最小或最大的一部分感兴趣。 当k为正时，我们想要得到最小的k个数 当k为负时，我们想要得到最大的k个数 补充： 数据合并 x >>> array([1,2,3,1,2,3,1,2,3]) y >>> array([10,20,30,10,20,30,10,20,30]) np.c_[x,y] >>> array([ [1,10], [2,10], [3,10], [1,20], ... ]) 随机数固定 np.random.seed(0) Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"jupyter/jupyter入门之pandas.html":{"url":"jupyter/jupyter入门之pandas.html","title":"jupyter入门之pandas","keywords":"","body":"jupyter入门之pandas 一. 什么是pandas pandas是基于NumPy的一种工具 该工具是为了解决数据分析任务而创建的 pandas纳入了大量库及一些标准的数据模型，提供了高效的操作大型数据集所需要的工具 pandas提供了大量能使我们快速便捷地处理数据的函数与方法 它是python成为强大而高效的数据分析环境的重要因素之一 导入 # 三剑客 import numpy import pandas from pandas import Series,DataFrame 二. Series Series是一种类似于一维数组的对象，由下面两个部分组成 values: 一组数据 (ndarrary类型) index: 相关的数据索引标签 1.创建 由列表或numpy数组创建 s1 = Series([1,2,3,4]) >>> 0 1 1 2 2 3 3 4 dtype: int64 s1 = np.array([1,2,3,4]) >>> array([1, 2, 3, 4]) 特别地，由ndarray创建的是引用，而不是副本。对Series元素的改变也会改变原来的ndarray对象中的元素。（列表没有这种情况） 由字典创建 dic = { 'name':'dancer', 'age':19, 'address':'hangzhou' } s3 = Series(data=dic) 2. 索引和切片 loc为显示切片（通过键），iloc为隐式切片（通过索引） # 访问单个元素 s[indexname] s.loc[indexname] 推荐 s[loc] s.iloc[loc] 推荐 # 访问多个元素 s[[indexname1,indexname2]] s.loc[[indexname1,indexname2]] 推荐 s[[loc1,loc2]] s.iloc[[loc1,loc2]] 推荐 3. 基本概念 可以把Series看成一个定长的有序字典 可以通过shape（维度），size（长度），index（键）,values（值）等得到series的属性 4.基本运算 运算的原则就是索引对齐，如果缺失索引，对应位置补np.nan NaN 是np.nan在pandas中的显示形式 可以使用pd.isnull()，pd.notnull()，或自带isnull(),notnull()函数检测缺失数据 可以使用isnull和any结合,来查看某一列或一行数据中是否存在缺失值 # 需求：提取s3对象中的所有非空数据 s3[s3.notnull()] # 提取空值的索引 s3[s3.isnull()].index # Sereis,对象可以使用一个与该对象等长的bool_list列表作为index访问数组元素 # 碰到True,就把对应位置的值返回 s3[[True,False,True,True,True,False]] # name属性为Series对象添加列索引 s3.name = 'haha' pandas会自动处理空值 add() sub() mul() div() Series之间的运算 在运算中自动对齐不同索引的数据 如果索引不对应，则补NaN # fill_value设置空值的填充值 s1.add(s2,fill_value=0) 三. DataFrame DataFrame是一个【表格型】的数据结构，可以看做是【由Series组成的字典】（共用同一个索引）。DataFrame由按一定顺序排列的多列数据组成。设计初衷是将Series的使用场景从一维拓展到多维。DataFrame既有行索引，也有列索引。 行索引：index 列索引：columns 值：values（numpy的二维数组） 1. 创建 最常用的方法是传递一个字典来创建。DataFrame以字典的键作为每一【列】的名称，以字典的值（一个数组）作为每一列。 dic = { '张三':[150,150,150,300], '李四':[0,0,0,0] } DataFrame(data=dic,index=['语文','数学','英语','理综']) 也可以用下面代码实现创建 data = [[0,150],[0,150],[0,150],[0,300]] index = ['语文','数学','英语','理综'] columns = ['李四','张三'] df = DataFrame(data=data,index=index,columns=columns) >>> 李四 张三 语文 0 150 数学 0 150 英语 0 150 理综 0 300 此外，DataFrame会自动加上每一行的索引（和Series一样）。 同Series一样，若传入的列与字典的键不匹配，则相应的值为NaN。 2. 索引 (1) 对列进行索引 - 通过类似字典的方式 - 通过属性的方式 可以将DataFrame的列获取为一个Series。返回的Series拥有原DataFrame相同的索引，且name属性也已经设置好了，就是相应的列名。 (2) 对行进行索引 - 使用.ix[]来进行行索引 - 使用.loc[]加index来进行行索引 - 使用.iloc[]加整数来进行行索引 同样返回一个Series，index为原来的columns。 (3) 对元素索引的方法 - 使用列索引 - 使用行索引(iloc[3,1]相当于两个参数;iloc[[3,3]] 里面的[3,3]看做一个参数) - 使用values属性（二维numpy数组） # 索引行 df.loc[indexname] 推荐 df.iloc[loc] # 索引列 df[columnname] 推荐 df.columnname # 索引元素 df.loc[indexname].loc[columnname] df[columnname].loc[indexname] df.loc[indexname,columnname] 推荐 3. 切片 【注意】 直接用中括号时： 索引表示的是列索引 切片表示的是行切片 # 行切片 df[indexname1:indexname2] df.loc[indexname1:indexname2] 推荐 # 列切片 df.loc[:,columnname1:columnname2] 推荐 # 行列切片 df.loc[indexname1:indexname2,columnname1:columnname2] 推荐 df.iloc[indexloc1:indexloc2,columnloc1:columnloc2] 4. 运算 下面是Python 操作符与pandas操作函数的对应表： Python Operator Pandas Method(s) + add() - sub(), subtract() * mul(), multiply() / truediv(), div(), divide() // floordiv() % mod() ** pow() # df1与df2相加 空值以0填充 df1.add(df2,fill_value=0) # DataFrame跟Series对象相加，fill_value不可用 df1.add(DataFrame(s1),fill_value=0) # 求平均成绩 score1 = DataFrame(data=np.random.randint(0,100,size=(5,3)), index=['dancer','lucy','tom','jack','rose'], columns=['python','java','C++']) score2 = DataFrame(data=np.random.randint(0,100,size=(5,3)), index=['dancer','lucy','tom','jack','rose'], columns=['python','java','C++']) (score1 + score2)/2. 1） DataFrame之间的运算 同Series一样： 在运算中自动对齐相同索引的数据 如果索引不对应，则补NaN 2） Series与DataFrame之间的运算 【重要】 使用Python操作符：以行为单位操作（参数必须是行），对所有行都有效。（类似于numpy中二维数组与一维数组的运算，但可能出现NaN） 使用pandas操作函数： axis=0：以列为单位操作（参数必须是列），对所有列都有效。 axis=1：以行为单位操作（参数必须是行），对所有行都有效。 axis=0（0 == index 行）：以列为单位操作（参数必须是列），对所有列都有效。 axis=1（1 == columns 列）：以行为单位操作（参数必须是行），对所有行都有效。 fill_value在df和series之间运算时，不能使用 四. 处理丢失数据 丢失数据有两种, None np.nan(NaN) 1. None None是Python自带的，其类型为python object。因此，None不能参与到任何计算中。 type(None) >>> NoneType object类型，不能直接参与运算 而且object类型的运算比int类型的运算慢很多 2.np.nan(NaN) type(np.nan) >>> float np.nan是浮点类型 可以参与计算，但计算结果为NaN 可以使用np.nan*()函数类计算nan，此时视nan为0 pandas中None与np.nan的操作 isnull() notnull() dropna(): 过滤丢失数据 fillna(): 填充丢失数据 (1)判断函数 isnull() notnull() 过滤函数 dropna # 默认过滤有空值的行 df1.dropna(axis=0) # 过滤有空值的列 df1.dropna(axis=1) # how参数指定过滤规则 # any 行或列中存在空值就过滤 # all 行或列中全部为空就过滤 df1.dropna(axis=0,how='all') 填充函数 fillna df1.fillna(value=None,method=None,axis=0,limit=1) # value 填充的值 # method 填充方式 - ffill 向前 - bfill 向后 - pading - backfill # axis 填充的轴 # limit 填充次数 五. pandas层次化索引 我们已经知道了Series和DataFrame DataFrame是Series对象的扩展 那么为什么还要学层次化索引呢？ 生活中的表格形式不是一成不变的 图中这种表如果使用DataFrame创建的话会相当麻烦 1.创建多层行索引 # 使用DataFrame创建 index1 = [['first','second'],['大米','白面','猪肉']] index2 = [['first','大米'],['first','白面'],['first','猪肉'],['second','大米'],['second','白面'],['second','猪肉']] index3 = [['first','first','first','second','second','second'],['大米','白面','猪肉','大米','白面','猪肉']] data = np.random.randint(0,200,size=(6,4)) columns = ['dancer','lucy','tom','jerry'] df = DataFrame(data=data,index=index3,columns=columns) 显然 重复写索引是很麻烦的事 这里我们可以使用pandas的显示构造 pandas.MultiIndex 它提供了三种方法 pandas.MultiIndex.from_array mindex3 = pd.MultiIndex.from_arrays(index3,names=('季度','品类')) df = DataFrame(data=data,columns=columns,index=mindex3) pandas.MultiIndex.from_tuple mindex2 = pd.MultiIndex.from_tuples(index2,names=('季度','品类')) df2 = DataFrame(data=data,index=mindex2,columns=columns) pandas.MultiIndex.from_product # 推荐 mindex3 = pd.MultiIndex.from_product(index1,names=('季度','品类')) df3 = DataFrame(data=data,index=mindex3,columns=columns) 2. 多层列索引 columns = pd.MultiIndex.from_product([['first','seoncd'],['大米','白面','猪肉']]) data = np.random.randint(0,100,size=(4,6)) index = df3.columns df4 = DataFrame(data=data,index=index,columns=columns) df4 >>> 季度 first second 品类 大米 白面 猪肉 大米 白面 猪肉 dancer 50 94 71 46 72 22 lucy 94 61 96 15 76 12 tom 72 97 63 18 8 22 jerry 98 5 73 84 84 31 3. 多层索引对象的索引与切片操作 【重要】对于Series来说，直接中括号[]与使用.loc()完全一样，推荐使用.loc中括号索引和切片。 index = pd.MultiIndex.from_product([['期中','期末'],['语文','数学','英语']]) data = np.random.randint(0,150,size=6) s1 = Series(data=data,index=index) s1 >>> 期中 语文 112 数学 145 英语 59 期末 语文 128 数学 91 英语 129 dtype: int32 1) 索引 # 把多级索引变成单级索引访问 s1.loc['期中'].loc['语文'] s1['期中'].iloc[0] s1.loc['期中','语文'] >>> 结果都为 112 2）切片 可以直接使用iloc对value切片 但可读性不高 多级索引切片 可使用显示索引 # 期中考试的语文数学成绩 s1.loc['期中'].loc['语文':'数学'] # 编程单级索引进行列切片 （不能跨区） df['期中'].loc[:,'语文':'数学'] 不能跨区切片 比如获取期中到期末的语文成绩 那如果我们需要进行跨区切片该怎么办呢？ 这里引入一个stack概念(索引的堆) 4. 索引的堆(stack) stack() unstack() level 多级索引从外向内 依次的编号是0.1.2... 使用stack()时, level设置为几，就把哪一级索引消失 使用unstack()的时候，level等于哪一个，哪一个就消失，出现在列里 注意不要在变化的途中变成了Series对象 六. pandas的拼接操作 pandas的拼接分为两种： 级联：pd.concat, pd.append 合并：pd.merge, pd.join 回顾 numpy的级联 n1 = np.random.randint(0,10,size=(3,3)) n2 = np.random.randint(10,20,size=(3,2)) np.concatenate((n1,n2),axis=1) >>> array([[ 4, 0, 4, 14, 17], [ 5, 9, 2, 10, 17], [ 2, 3, 1, 15, 11]]) 1. 使用pd.concat()级联 # 注：create_DF为封装的一个用于生成DataFrame对象的函数 #第一个参数是index 第二个参数是column df1 = create_DF(list('12345'),list('ABCDE')) df2 = create_DF(list('12345'),list('BCDEF')) # 两个DataFrame对象级联操作 pd.concat((df1,df2),axis=1) Create_DF def create_DF(index,columns): return DataFrame({j:[j+i for i in index] for j in columns},index=index) pandas中，级联允许形状不同，缺失的索引补充NaN pandas使用pd.concat函数，与np.concatenate函数类似，只是多了一些参数： objs axis=0 join='outer' join_axes=None ignore_index=False 索引没有特定含义的时候，可以使用如下方法处理索引重复的问题 pd.concat((df1,df2),ignore_index=True) 索引有特定含义，可以引入keys参数，对两张表做分区说明 pd.concat((df1,df2),keys=['上学期','下学期']) 应该更多的采用外连接来级联，保留更多的原始数据 inner内连接，取交集 outer外连接, 取并集 pd.concat((df1,df2),join='outer') 1) pandas与series级联 # 使用级联处理 s = Series(data=[100,99],index=['张三','李四'],name='计算机') # pandas可以直接跟series对象进行级联 pd.concat((score,s),axis=1) 2) 不匹配级联 不匹配指的是级联的维度的索引不一致。例如纵向级联时列索引不一致，横向级联时行索引不一致 有3种连接方式： 外连接：补NaN（默认模式） 内连接：只连接匹配的项 连接指定轴 join_axes 3) append()函数 由于在后面级联的使用非常普遍，因此有一个函数append专门用于在后面添加 注意:append函数只是沿着axis=0的方向进行级联 2.使用pd.merge()合并 merge与concat的区别在于，merge需要依据某一共同列来进行合并 使用pd.merge()合并时，会自动根据两者相同column名称的那一列，作为key来进行合并。 注意每一列元素的顺序不要求一致 merge的合并也分一对一 多对一 多对多 就是合并相同columns 1) 一对一合并 以手机型号为合并基准 2) 多对一合并 同样以手机型号为基准 3) 多对多合并 当有多个相同列时 可以以其中任何一个相同列为基准合并 也可以多个相同列为基准 4) key的规范化 1. 如果两个表中，只有一列内容是有重复的，就参考这一列进行合并 2. 默认的合并方式，只取交集 3. how 设置合并的参考列 4. 合并就是以列为参考的，不是以行 pd.merge(table1,table2,how='right') 5. 使用on=显式指定哪一列为key,当有多个key相同时使用 6. 两张表中不存在相同的列表签，可以使用left_on和right_on来显示制定合并参考列 7. 合并之后两列都会保留，可以使用drop函数删除没用的列 pd.merge(table2,table5,left_on='手机型号',right_on='型号').drop(labels=['型号'],axis=1) # 替换原表 a.drop('',axis=1,inplace=True) # 以表的行索引为合并参考 # 设置left_index或right_index为True pd.merge(table2,table6,left_on='手机型号',right_index=True) 数值型数据，尽量采用级联而不是合并 字符串型数据，可以使用合并 一旦数值出现重复值，就会导致业务逻辑变成1对多或者对对多的关系 5) 内合并与外合并 内合并：只保留两者都有的key（默认模式） 外合并 how='outer'：补NaN 左合并、右合并：how='left'，how='right'， 6）列冲突的解决 当列冲突时，即有多个列名称相同时，需要使用on=来指定哪一个列作为key，配合suffixes指定冲突列名 可以使用suffixes=自己指定后缀 补充 # 去重 series.unique() # 删除 df.drop(inplace=True) # 替换原表 Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"jupyter/jupyter入门之pandas（完）.html":{"url":"jupyter/jupyter入门之pandas（完）.html","title":"jupyter入门之pandas（完）","keywords":"","body":"pandas高级操作 pandas数据处理 1. 删除重复元素 使用duplicated函数检测重复的行，返回元素为布尔类型的Series对象，每个元素对应一行，如果该行不是第一次出现，则元素为True 使用drop_duplicates()函数删除重复的行 使用duplicated()函数查看重复的行 如果使用pd.concat([df1,df2],axis = 1)生成新的DataFrame，新的df中columns相同，使用duplicate()和drop_duplicates()都会出问题 df.duplicated(keep='last') >>> 0 True 1 False 2 True 3 False 4 False dtype: bool df.drop_duplicates(keep='last') # 使用drop函数删除重复元素 df.drop(df[df.duplicated()].index) 2. 映射 映射的含义：创建一个映射关系列表，把values元素和一个特定的标签或者字符串绑定 包含三种操作： replace()函数：替换元素（DataFrame\\Series的函数) 最重要：map()函数：新建一列(Series的函数) rename()函数：替换索引(DataFrame的函数) DateFrame可以存在多种数据类型 Series、numpy只能存在一种类型 DateFrame赋值时如果有None，显示方式取决于该列的数据类型，如果为object，则为None，如果是float,则显示np.nan 1) replace()函数: 替换元素 使用replace()函数，对values进行替换操作 Series替换操作 单值替换 普通替换 字典替换 多值替换 列表替换 字典替换(推荐) s = Series(data=[np.nan,'小七',19,10]) >>> 0 NaN 1 小七 2 19 3 10 dtype: object s.replace(to_replace={'小七':'张学友',19:21}) >>> 0 NaN 1 张学友 2 21 3 10 dtype: object Series参数说明： method：对指定的值使用相邻的值填充 limit：设定填充次数 to_replace: 旧值 DataFrame替换操作 单值替换 普通替换 按列指定单值替换{列标签：替换值} 多值替换 列表替换 单字典替换（推荐） # replace函数中不能接收None对象 df.replace(to_replace=np.nan,value=19) # 接收键值对 df.replace(to_replace={'boy':np.nan},value=True) # 多值替换 df.replace(to_replace=['tom','tomGG'],value=['TOM','TOMGG']) # 推荐使用一个能满足所有表格替换目的的字典来进行替换,便于后期维护 df.replace(to_replace={'dancer':'DANCER','lucy':'王淑芬',True:'是','nosuch':'hahaha'}) 注意：DataFrame中，无法使用method和limit参数 2) map()函数：新建一列 map()可以映射新一列数据 map()中可以使用lambd表达式 map()中可以使用方法，可以是自定义的方法 注意 map()中不能使用sum之类的函数，for循环 map(字典) 字典的键要足以匹配所有的数据，否则出现NaN dic = { 'name':['dancer','lucy','mery','tom'], 'age':[19,20,18,25], 'score':[90.8,109,87.2,99], 'boy':[True,False,False,None], 'oldname':['dancer','lucy','王二狗','tomGG'] } df = DataFrame(data=dic) dic = { 'dancer':'张学友', 'lucy':'张曼玉', 'mery':'刘诗诗', 'tom':'郑凯' } # 普通的值之间的映射,可以直接使用字典 df['oldname'] = df['name'].map(dic) # 自定义方法使用map def trans_func(x): if x>=20: return '成年' else: return '未成年' # 自定义函数可以用来处理一些复杂的逻辑 df['年纪'] = df.age.map(trans_func) 3) transform() transform与map类似 年龄列加1 df.age = df.age.transform(lambda x:x-1) 4) rename()函数:替换索引 使用rename()函数替换行索引 mapper 替换所有索引 index 替换行索引 columns 替换列索引 level 指定多维索引的维度 # 替换列索引 df2.rename(columns={'oldname':'曾用名2'}) # 替换行索引 df2.rename(index={'上学期':'first','下学期':'last'}) # mapper默认替换行索引,level指定替换的索引层级,如果不指定就全部层级都进行替换 df2.rename(mapper={'上学期':'第一学期','dan':'DAN'},level=1) # 同时替换行和列索引 df2.rename(index={'上学期':'first'},columns={'曾用名':'曾用名1'}) 3. 使用聚合操作对数据异常值检测和过滤 使用describe()函数查看每一列的描述性统计量 使用std()函数可以求得DataFrame对象每一列的标准差 data = np.random.randint(0,100,size=(6,6)) df = DataFrame(data=data) df 使用df.describe()之后 4. 排序 使用take()函数排序 take()函数接受一个索引列表，用数字表示 eg:df.take([1,3,4,2,5]) 可以借助np.random.permutation()函数随机排序 df = DataFrame(data=np.random.randint(0,10,size=(5,5)), index = list('ABCDE'), columns = list('甲乙丙丁戊')) df df.take([2,4,3,1],axis=1) 小技巧：当DataFrame规模足够大时，直接使用np.random.randint()函数，就配合take()函数实现随机抽样 5. 数据分类处理（重点） 数据聚合是数据处理的最后一步，通常是要使每一个数组生成一个单一的数值。 数据分类处理： 分组：先把数据分为几组 用函数处理：为不同组的数据应用不同的函数以转换数据 合并：把不同组得到的结果合并起来 数据分类处理的核心： groupby()函数 groups属性查看分组情况 df = DataFrame({'item':['苹果','香蕉','橘子','香蕉','橘子','苹果'], 'price':[4,3,3,2.5,4,2], 'color':['red','yellow','yellow','green','green','green'], 'weight':[12,20,50,30,20,44]}) df # 以颜色分类 df.groupby('color').groups >>> {'green': Int64Index([3, 4, 5], dtype='int64'), 'red': Int64Index([0], dtype='int64'), 'yellow': Int64Index([1, 2], dtype='int64')} # 查看各种颜色水果重量的总和 df.groupby('color')['weight'].sum() >>> color green 94 red 12 yellow 70 Name: weight, dtype: int64 根据item分组 查看结果 # 按item分类取price的平均值 price_mean = DataFrame(df.groupby('item')['price'].mean()) # 平均价格Series price_mean.rename(columns={'price':'mean_prcie'},inplace=True) # 合并对象 pd.merge(df,price_mean,left_on='item',right_index=True,how='outer') # 多个分组条件,得到的是一个多级索引的分组表 df.groupby(['color','item']).sum() 总结 数据类型是离散的可以分组，连续的没有意义 使用列表进行多列分组，得到的结果是多层级索引 6. 高级数据聚合 使用groupby分组后，也可以使用transform和apply提供自定义函数实现更多的运算 df.groupby('item')['price'].sum() df.groupby('item')['price'].apply(sum) transform和apply都会进行运算，在transform或者apply中传入函数即可 transform和apply也可以传入一个lambda表达式 我们来写个例子感受下transform和apply,数据还用到上面的水果 def test_func(items): result = 0 for item in items: result += item return result # 结果可以直接跟原始表合并操作 df.groupby('color')['weight'].apply(test_func) >>>color green 94 red 12 yellow 70 Name: weight, dtype: int64 # 结果可以直接跟原始表级联操作 df.groupby('color')['weight'].transform(test_func) >>> 0 12 1 70 2 70 3 94 4 94 5 94 Name: weight, dtype: int64 注意 transform 会自动匹配列索引返回值，不去重 apply 会根据分组情况返回值，去重 数据加载 将表格数据读取为DataFrame对象 read_csv read_table # sep 确认文件内容的间隔符号 # 如果文件没有列标签，应该设置header为None table = pd.read_csv('./data/type-.txt',sep='-',header=None) 使用read_excel()读取excel表格 table1 = pd.read_csv('./data/type_comma') 写入excel文件 table1.to_excel('dancer.xls') 读取sqlite文件 导包 import sqlite3 as sqlite3 连接数据库 sqlite3.connect('dbpath') 读取table内容 pd.read_sql(\"SQL语句\", con) 写入数据库文件 df对象.to_sql('tablename',connection) 操作数据库 connection.execute(SQL语句) # SQL Server阉割版 轻量级的关系型数据库 支持SQL语句 应用在移动端 # 文件轻量级 import sqlite3 as sqlite3 connection = sqlite3.connect('./data/weather_2012.sqlite') # 读取数据库内容 # select * from weatehr_2012 weather = pd.read_sql('select * from weather_2012',connection,index_col='index') # 写入数据库 table1.to_sql('dancer',connection) # 读取dancer，查看是否写入成功 pd.read_sql('select * from dancer',connection) # 删除一个表 # drop table name connection.execute('drop table dancer') # 设置行索引 weather.set_index('index') 交叉表和透视表 大家之前已经对分组表有了了解，用groupby分组 而透视表也是用来分组，它可以指定分组的类型，行、列、行列 ...三剑客导入 df = DataFrame({'item':['苹果','香蕉','橘子','香蕉','橘子','苹果'], 'price':[4,3,3,2.5,4,2], 'color':['red','yellow','yellow','green','green','green'], 'weight':[12,20,50,30,20,44]}) 透视表 各种电子表格程序和其他数据分析软件中一种常见的数据汇总工具。它根据一个或多个键对数据进行聚合，并根据行和列上的分组键将数据分配到各个矩形区域中 # 行分组透视表 df.pivot_table(index='color',aggfunc='sum')['weight'] # 列分组透视表 df.pivot_table(columns='color',aggfunc='sum').loc['weight'] # 行列分组 # 行列分组的透视表 同时设定index、columns参数 def myfunction(x): mysum = 0 for item in x: mysum += item return mysum aggfunc：设置应用在每个区域的聚合函数，默认值为np.mean fill_value：替换结果中的缺失值 交叉表 是一种用于计算分组频率的特殊透视图,对数据进行汇总 pd.crosstab(index,colums) index:分组数据，交叉表的行索引 columns:交叉表的列索引 pd.crosstab(df.color,df.item) >>> item 橘子 苹果 香蕉 color green 1 1 1 red 0 1 0 yellow 1 0 1 附件 pandas常用函数速查表 导入数据 pd.read_csv(filename)：从CSV文件导入数据 pd.read_table(filename)：从限定分隔符的文本文件导入数据 pd.read_excel(filename)：从Excel文件导入数据 pd.read_sql(query, connection_object)：从SQL表/库导入数据 pd.read_json(json_string)：从JSON格式的字符串导入数据 pd.read_html(url)：解析URL、字符串或者HTML文件，抽取其中的tables表格 pd.read_clipboard()：从你的粘贴板获取内容，并传给read_table() pd.DataFrame(dict)：从字典对象导入数据，Key是列名，Value是数据 导出数据 df.to_csv(filename)：导出数据到CSV文件 df.to_excel(filename)：导出数据到Excel文件 df.to_sql(table_name, connection_object)：导出数据到SQL表 df.to_json(filename)：以Json格式导出数据到文本文件 创建测试对象 pd.DataFrame(np.random.rand(20,5))：创建20行5列的随机数组成的DataFrame对象 pd.Series(my_list)：从可迭代对象my_list创建一个Series对象 df.index = pd.date_range('1900/1/30', periods=df.shape[0])：增加一个日期索引 查看、检查数据 df.head(n)：查看DataFrame对象的前n行 df.tail(n)：查看DataFrame对象的最后n行 df.shape()：查看行数和列数 df.info()：查看索引、数据类型和内存信息 df.describe()：查看数值型列的汇总统计 s.value_counts(dropna=False)：查看Series对象的唯一值和计数 df.apply(pd.Series.value_counts)：查看DataFrame对象中每一列的唯一值和计数 数据选取 df[col]：根据列名，并以Series的形式返回列 df[[col1, col2]]：以DataFrame形式返回多列 s.iloc[0]：按位置选取数据 s.loc['index_one']：按索引选取数据 df.iloc[0,:]：返回第一行 df.iloc[0,0]：返回第一列的第一个元素 数据清理 df.columns = ['a','b','c']：重命名列名 pd.isnull()：检查DataFrame对象中的空值，并返回一个Boolean数组 pd.notnull()：检查DataFrame对象中的非空值，并返回一个Boolean数组 df.dropna()：删除所有包含空值的行 df.dropna(axis=1)：删除所有包含空值的列 df.dropna(axis=1,thresh=n)：删除所有小于n个非空值的行 df.fillna(x)：用x替换DataFrame对象中所有的空值 s.astype(float)：将Series中的数据类型更改为float类型 s.replace(1,'one')：用‘one’代替所有等于1的值 s.replace([1,3],['one','three'])：用'one'代替1，用'three'代替3 df.rename(columns=lambda x: x + 1)：批量更改列名 df.rename(columns={'oldname': 'new name'})：选择性更改列名 df.set_index('column_one')：更改索引列 df.rename(index=lambda x: x + 1)：批量重命名索引 数据处理：Filter、Sort和GroupBy df[df[col] > 0.5]：选择col列的值大于0.5的行 df.sort_values(col1)：按照列col1排序数据，默认升序排列 df.sort_values(col2, ascending=False)：按照列col1降序排列数据 df.sort_values([col1,col2], ascending=[True,False])：先按列col1升序排列，后按col2降序排列数据 df.groupby(col)：返回一个按列col进行分组的Groupby对象 df.groupby([col1,col2])：返回一个按多列进行分组的Groupby对象 df.groupby(col1)[col2]：返回按列col1进行分组后，列col2的均值 df.pivot_table(index=col1, values=[col2,col3], aggfunc=max)：创建一个按列col1进行分组，并计算col2和col3的最大值的数据透视表 df.groupby(col1).agg(np.mean)：返回按列col1分组的所有列的均值 data.apply(np.mean)：对DataFrame中的每一列应用函数np.mean data.apply(np.max,axis=1)：对DataFrame中的每一行应用函数np.max 数据合并 df1.append(df2)：将df2中的行添加到df1的尾部 df.concat([df1, df2],axis=1)：将df2中的列添加到df1的尾部 df1.join(df2,on=col1,how='inner')：对df1的列和df2的列执行SQL形式的join 数据统计 df.describe()：查看数据值列的汇总统计 df.mean()：返回所有列的均值 df.corr()：返回列与列之间的相关系数 df.count()：返回每一列中的非空值的个数 df.max()：返回每一列的最大值 df.min()：返回每一列的最小值 df.median()：返回每一列的中位数 df.std()：返回每一列的标准差 Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"jupyter/jupyter入门之图像处理.html":{"url":"jupyter/jupyter入门之图像处理.html","title":"jupyter入门之图像处理","keywords":"","body":"scipy图片处理 导入库 import numpy as np import pandas as pd from pandas import Series,DataFrame # 图像处理库 import matplotlib.pyplot as plt %matplotlib inline # scipy.fftpack模块用来计算快速傅里叶变换 # 速度比传统傅里叶变换更快，是对之前算法的改进 # 图片是二维数据，注意使用fftpack的二维转变方法 from scipy.fftpack import fft2,ifft2 # 积分库 from scipy.integrate import quad 图片消噪 示例，登月图片消噪 moon = plt.imread('moonlanding.png') # 灰度化处理的图片 plt.imshow(moon,cmap='gray') # 时域-频域 fft_data = fft2(moon) # 滤波，把高频波滤掉 # > 后边的值需要自己调节 condition = np.abs(fft_data) > 2e3 fft_data[np.where(condition)] = 0 # 频域--时域 ifft_data = ifft2(fft_data) plt.imshow(np.real(ifft_data),cmap='gray') # jpg图像 0-255之间的整数 # png图像 0-1之间的小数 灰度处理 # 读取图片 pujing = plt.imread('pujing.jpg') # 查看维度 pujing.shape >>> (354, 500, 3) # 去除色彩 pujing.mean(axis=2).shape >>> (354, 500) # 灰度处理 plt.imshow(pujing.mean(axis=2),cmap='gray') 数值积分，求解圆周率 integrate 对函数(1 - x^2)^0.5进行积分 f = lambda x:(1-x**2)**0.5 # 绘制区间 x = np.linspace(-1,1,100) y = f(x) plt.figure(figsize=(4,4)) plt.plot(x,y) plt.plot(x,-y) 使用scipy.integrate进行积分，调用quad()方法 # 返回值第一个是面积，第二个是误差 area,err = quad(f,-1,1) # area*2是整圆的面积 p = area*2/1**2 print(p) >>> 3.1415926535897967 scipy文件输入/输出 随机生成数组，使用scipy中的io.savemat()保存 文件格式是.mat，标准的二进制文件 n = np.random.randint(0,100,size=10) >>> array([28, 90, 21, 45, 75, 42, 76, 58, 1, 49]) # 导入io库 import scipy.io as io # 存储二进制 文件路径默认是.mat，可以省略 # mdict用于传递要写入本地的数据，字典的键自由定制 io.savemat('dancer',mdict={'data':n}) # 使用io.loadmat()读取数据 io.loadmat('dancer.mat')['data'] >>> array([[28, 90, 21, 45, 75, 42, 76, 58, 1, 49]]) # 读写图片使用scipy中misc.imread()/imsave() # 旧版读取 import scipy.misc as misc misc.imread('pujing.jpg') # 读取 import imageio as imgio pujing = imgio.imread('pujing.jpg') # 存储 pjgray = np.uint8(pujing.mean(axis=2)) imgio.imsave('gray_pujing.jpg',pjgray) misc的rotate、resize、imfilter操作 # 旋转90度 plt.imshow(misc.imrotate(pujing,angle = 90)) import skimage.transform as transform import imageio as misc pujing = misc.imread('pujing.jpg') transform.rotate(pujing,angle=90) # output_shape 指定元组，会对图片进行变形压缩 # mode参数的默认值conctant需要修改，因为即将被remove # 该拜年大小 plt.imshow(transform.resize(pujing,output_shape=(100,100),mode='symmetric')) # 'blur', 'contour', 'detail', 'edge_enhance', 'edge_enhance_more', # 'emboss', 'find_edges', 'smooth', 'smooth_more', 'sharpen' # 类似于滤镜 plt.imshow(misc.imfilter(pujing,'emboss')) scipy.ndimage图片处理 import scipy.ndimage as ndimage # misc库自带的一张图片，可以快速读取一张小浣熊 img = misc.face(gray=True) 使用scipy.misc.face(gray=True)获取图片，使用ndimage移动坐标、旋转图片、切割图片、缩放图片 shift移动坐标 # constant', 'nearest', 'reflect', 'mirror' ,'wrap' plt.imshow(ndimage.shift(img,shift=[100,200],mode='wrap'),cmap='gray') rotate旋转图片 zoom缩放图片 使用切片切割图片 图片进行过滤 添加噪声，对噪声图片使用ndimage中的高斯滤波、中值滤波、signal中维纳滤波进行处理 使图片变清楚 加载图片，使用灰色图片misc.face()添加噪声 gaussian高斯滤波参数sigma：高斯核的标准偏差 median中值滤波参数size：给出在每个元素上从输入数组中取出的形状位置，定义过滤器功能的输入 signal维纳滤波参数mysize：滤镜尺寸的标量 pandas中的绘图函数 线形图 导入库 # 三剑客 import numpy as np import pandas as pd from pandas import Series,DataFrame # 图像处理函数 import matplotlib.pyplot as plt %matplotlib inline 示例 # 连续100天内，某一支股票价格的变化，可以采用线型图来表示 s1 = Series(data=np.random.randint(0,10,size=100)) s1.plot() # 绘制多条股票变化趋势 # 在同一个cell中，绘制多个对象的图 s2 = Series(data=np.random.randint(0,100,size=100)) s2.cumsum().plot() s1.cumsum().plot() 图例的位置可能会随着数据的不同而不同 柱状图 Series柱状图示例,kind = 'bar'/'barh' s1 = Series(data=[100,80,99,80,78,76,22,90,100],index=list('123456789')) s1.plot(kind='bar') DataFrame柱状图示例, df1 = DataFrame(data=np.random.randint(50,100,size=(9,3)), columns=list('ABC'), index=list('123456789')) df1.plot(kind='bar') # 改变方向 df1.plot(kind=\"barh\") 直方图 series示例,kind='hist' s1 = Series(data=np.array([1,1,4,1,3,5,8,8,5,5])) s1.plot(kind='hist',bins=10) # 不同的bins，会导致直方图显示的不一样 rondom生成随机数百分比直方图，调用hist方法 柱高表示数据的频数，柱宽表示各组数据的组距 参数bins可以设置直方图方柱的个数上限，越大柱宽越小，数据分组越细致 设置normed参数为True，可以把频数转换为概率 kde图：核密度估计，用于弥补直方图由于参数bins设置的不合理导致的精度缺失问题 s1.plot(kind='hist',bins=3,normed=True) s1.plot(kind='kde') 示例: 绘制一个由两个不同的标准正态分布组成的双峰分布 n1 = np.random.normal(loc=50,scale=1,size=100) n2 = np.random.normal(loc=70,scale=3,size=50) s = Series(data=np.concatenate((n1,n2))) s.plot(kind='hist',normed=True,bins=30) s.plot(kind='kde') 散点图 散布图 散布图是观察两个一维数据数列之间的关系的有效方法,DataFrame对象可用 使用方法： 设置kind = 'scatter'，给明标签columns 散布图矩阵，当有多个点时，两两点的关系 使用函数：pd.plotting.scatter_matrix(), 参数diagnol：设置对角线的图像类型 x = Series(data=[1,3,5,7,9]) y = Series(data=[2,4,6,8,10]) df = DataFrame(data = np.random.randint(0,10,size=(5,2)),columns=['A','B']) df.A = x df.B = y df.plot(kind='scatter',x='A',y='B') df2 = DataFrame(data=np.random.randn(10000,4),columns=list('ABCD')) # DataFrme多列对应关系 # diagonal='kde' 设置对角线的图形显示模式 r = pd.plotting.scatter_matrix(df2,diagonal='kde') Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"python基础/Dateutil.html":{"url":"python基础/Dateutil.html","title":"Dateutil","keywords":"","body":"Dateutil rrulestr INTERVAL: 每隔一段时间 DTSTART: 规则开始时间 COUNT: 执行次数 rrule freq: 单位 YEARLY, MONTHLY, WEEKLY,DAILY, HOURLY, MINUTELY, SECONDLY interval 每隔一段时间(需配合执行时间单位,DAY/WEEKDAY/MONTH等) dtstart: 开始时间 until: 结束时间 byweekday：执行的星期 wkst： 周开始时间 测试数据 from util.common import * lesson_time(\"2018/09/;2/9:00;3/13:00;5/13:00\",\"2018/08/22 16:26\",20) Lessons.objects(num=str(lesson_num)).first() StudentApplies.objects(lessonid=str(lesson[\"id\"])) TeacherApplies.objects( Q(unionid=data[\"teacher\"][\"unionid\"]) & Q(courseid=course) ).first() Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"python基础/ZIP.html":{"url":"python基础/ZIP.html","title":"ZIP","keywords":"","body":"写个转换类型的函数 def k(i): return list(i) 开始测试 两个同维度参数 >>> a=[1,2,3] >>> b=[4,5,6] >>> k(zip(a,b)) [(1, 4), (2, 5), (3, 6)] 两个不同维度参数 >>> a=[(1,2),(3,4,5),(1,1)] >>> b=[7,8,9] >>> k(zip(a,b)) [((1, 2), 7), ((3, 4, 5), 8), ((1, 1), 9)] >>> a=[1,2,3] >>> b=[1,2] >>> k(zip(a,b)) [(1, 1), (2, 2)] >>> a=[(1,2),(3,4,5),(1,1)] >>> b=[(11,12,13)] >>> k(zip(a,b)) [((1, 2), (11, 12, 13))] 自己和自己 >>> k(zip(b,b)) [((11, 12, 13), (11, 12, 13))] 应用 [\"1.in\", \"1.out\", \"2.in\", \"2.out\"] => [(\"1.in\", \"1.out\"), (\"2.in\", \"2.out\")] >>> a = [\"1.in\", \"1.out\", \"2.in\", \"2.out\"] >>> list(zip(*[a[i::2] for i in range(2)])) [(\"1.in\", \"1.out\"), (\"2.in\", \"2.out\")] Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"前端/vue-cli 3.0版本webpack打包.html":{"url":"前端/vue-cli 3.0版本webpack打包.html","title":"vue-cli 3.0版本webpack打包","keywords":"","body":"vue-cli 3.0版本webpack打包 新项目使用了vue-cli 3版本，发现项目中没有build目录，看了vue-cli文档后，大致清楚了如何修改 首先在package.json同级目录下创建新文件vue.config.js vue.config.js中的内容会覆盖默认的打包配置 function resolve(dir) { return path.join(__dirname, dir) } module.exports = { // 项目部署的基础路径 // 我们默认假设你的应用将会部署在域名的根部， // 比如 https://www.my-app.com/ // 如果你的应用时部署在一个子路径下，那么你需要在这里 // 指定子路径。比如，如果你的应用部署在 // https://www.foobar.com/my-app/ // 那么将这个值改为 `/my-app/` baseUrl: '/', // 将构建好的文件输出到哪里 outputDir: 'dist', // 放置静态资源的地方 (js/css/img/font/...) // assetsDir: '', // 是否在保存的时候使用 `eslint-loader` 进行检查。 // 有效的值：`ture` | `false` | `\"error\"` // 当设置为 `\"error\"` 时，检查出的错误会触发编译失败。 lintOnSave: true, // 使用带有浏览器内编译器的完整构建版本 // 查阅 https://cn.vuejs.org/v2/guide/installation.html#运行时-编译器-vs-只包含运行时 // compiler: false, // babel-loader 默认会跳过 node_modules 依赖。 // 通过这个选项可以显式转译一个依赖。 transpileDependencies: [/* string or regex */], // 是否为生产环境构建生成 source map？ productionSourceMap: false, // 调整内部的 webpack 配置。 // 查阅 https://github.com/vuejs/vue-docs-zh-cn/blob/master/vue-cli/webpack.md chainWebpack: () => { }, configureWebpack: () => { }, // CSS 相关选项 css: { // 将组件内的 CSS 提取到一个单独的 CSS 文件 (只用在生产环境中) // 也可以是一个传递给 `extract-text-webpack-plugin` 的选项对象 extract: true, // 是否开启 CSS source map？ sourceMap: false, // 为预处理器的 loader 传递自定义选项。比如传递给 // sass-loader 时，使用 `{ sass: { ... } }`。 loaderOptions: {}, // 为所有的 CSS 及其预处理文件开启 CSS Modules。 // 这个选项不会影响 `*.vue` 文件。 modules: false }, // 在生产环境下为 Babel 和 TypeScript 使用 `thread-loader` // 在多核机器下会默认开启。 parallel: require('os').cpus().length > 1, // PWA 插件的选项。 // 查阅 https://github.com/vuejs/vue-docs-zh-cn/blob/master/vue-cli-plugin-pwa/README.md pwa: {}, // 配置 webpack-dev-server 行为。 devServer: { open: process.platform === 'darwin', host: 'localhost', port: 8888, https: false, hotOnly: false, open:true, // 查阅 https://github.com/vuejs/vue-docs-zh-cn/blob/master/vue-cli/cli-service.md#配置代理 proxy: 'http://localhost:3001', // string | Object before: app => { } }, configureWebpack: config => { if (process.env.NODE_ENV === 'production') { // 为生产环境修改配置... if(process.env.npm_lifecycle_event === 'analyze'){ config.plugins.push( new BundleAnalyzerPlugin() ); } } else { // 为开发环境修改配置... } }, // 第三方插件的选项 pluginOptions: { } } Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"前端/前端vue优化.html":{"url":"前端/前端vue优化.html","title":"前端vue优化","keywords":"","body":"前端vue加载优化 一. 组件按需引入 二. vue-router懒加载 三. vue打包优化gzip o4qOz0-urH0Rtd7lxt3PmAGpZ9Dc o4qOz0-urH0Rtd7lxt3PmAGpZ9Dc o06oS0hlzVeRu_stBh8bY6HUenew Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"后端/django整理/crm管理系统.html":{"url":"后端/django整理/crm管理系统.html","title":"crm管理系统","keywords":"","body":"CRM客户关系管理系统 客户关系管理（CRM） ​ 客户关系管理（customer relationship management）的定义是：企业为提高核心竞争力，利用相应的信息技术以及互联网技术协调企业与顾客间在销售、营销和服务上的交互，从而提升其管理方式，向客户提供创新式的个性化的客户交互和服务的过程。其最终目标是吸引新客户、保留老客户以及将已有客户转为忠实客户，增加市场。 作用 1.提高市场营销效果 2.为生产研发提供决策支持 3.提供技术支持的重要手段 4.为财务金融策略提供决策支持 5.为适时调整内部管理提供依据 6.使企业的资源得到合理利用 7.优化企业业务流程 8.提高企业的快速响应和应变能力 9.改善企业服务，提高客户满意度 10.提高企业的销售收入 11.推动了企业文化的变革 12.与QQ集成，可以快速与客户沟通 思维导图 用户场景分析 销售 销售A 刚从 百度推广 聊了一个客户，录入了CRM系统，咨询了python全栈开发课程，但是没报名 销售B 从 qq群聊了客户，且报名了python全栈9期课程，给用户发送了报名连接，待用户填写完毕后，把他添加到了python fullstack s9的班级里 销售C 打电话给之前的一个客户，说服他报名linux40期，但是没说服成功，更新了跟踪记录 销售D 聊了一个客户，录入时发现，此客户已存在，不能录入，随后通知相应的客户负责人跟进 销售B 从客户库里过滤出了 所有超过一个月未跟踪的客户，然后进行跟踪（如果成了，这客户就算B的） 销售主管 查看了部门 本月的销售报表， 包括来源分析，成单率分析，班级报名数量分析，销售额同比 学员 客户A 填写了销售发来的报名链接，上传了个人的证件信息，并提交，过了一会儿，发现收到一个邮件，告知他报名python9期课程成功，并帮他开通了学员账号 学员A 登录了学员系统，看到了 自己的合同，报名的班级，以及课程大纲 学员A 提交了python9期的 第1节课的作业 学员A 查看了自己在python9期的学习成绩和排名 学员A 在线搜索一个问题，发现没有答案，于是提交了一个问题 讲师 登录了CRM，查看自己管理的班级列表 进入了python9期，创建了第一节的上课记录，填入了本节内容，作业需求 为python9期的第一节课，进行点名，发现科比迟到了，标记他为迟到状态 批量下载了所有学员的python9期第一节的作业，给每个学生在线 打成绩+批注 管理员 创建了 课程（linux,python） 创建了 校区（北京，上海） 创建了 班级（python fullstacks9和linux40） 创建了 账号（A,B,C,D） 创建了 销售，讲师，学员三个角色，并把ABCD分配到了销售角色里 设置了销售可以操作的权限 表结构设计 重点代码段 登录 from django.shortcuts import render,redirect from django.contrib.auth import authenticate,login def acc_login(request): if request.method == 'POST': username = request.POST.get('username',None) password = request.POST.get('password',None) #user是一个对象 #验证 user = authenticate(username=username,password=password) if user: #登录（已生成session） login(request,user) #如果有next值就获取next值，没有就跳转到首页 return redirect(request.GET.get('next','/')) return render(request,'login.html') 动态菜单生成 首先获取登录的用户（User） 通过User反向查找到UserProfile(用户信息) 然后通过UserProfile找到用户关联的所有角色 最后通过角色循环遍历出用户所有的菜单 OneToOneField和ForeignKey反向获取 OneToOneField反向查，直接request.user.userprofile 后面跟反向的表名（小写）就可以 如果是FK，直接request.user.userprofile_set 后面跟反向的表名（小写）+“_set” 就可以 request.user.userprofile.role.select_related等价于request.user.userprofile.role.all 自定义前端kingadmin全局注册时报错NoneType 是因为我们在注册model的时候，有的写了自定义的model类，有的没写，而我们都统一的赋值，导致那些没写自定义model类（空的）赋值的时候就会报NoneType错误 django自带的自定义admin类的写法继承了ModelAdmin，那注册的时候为什么有的没写自定义admin类没有报错呢？ 是因为继承的admin.ModelAdmin帮我们写了（里面其实都定义为空了），我们模仿django admin的写法，也写个父类。 分页 官方实例 报名页面流程 销售填写客户跟班级，点“下一步”提交 后台获取到客户id和班级id，在数据库中创建记录，并生成一个报名链接，返回到前端 前端显示报名链接，然后销售把报名链接发给用户 学员填写报名信息 添加学员注册url 添加CustomerInfo字段，身份证信息，紧急联络人，性别 有些字段是只读的，填写信息的时候不能修改，因为如果设置了只读(添加属性disabled=true)，提交的时候会报这些字段为空，导致提交错误 所以在前段添加了js代码，BeforeFormSubmit 在提交前去掉disable=true（因为数据库中有默认值，提交的时候就不会报错） 防止用户通过前端改html代码的方式改只读字段的信息，所以在form.py里面添加了一个自定义的验证方法（clean）,如果只读字段提交的时候信息跟数据库中默认的不一样，就报错 #只读字段不让用户通过浏览器改html代码的方式改 def clean(self): # 表单级别的错误 if self.errors: raise forms.ValidationError((\"Please fix errors before re-submit.\")) # means this is a change form ,should check the readonly fields if self.instance.id is not None: #取出只读字段，是一个字符串形式 for field in self.Meta.readonly_fields: #通过反射取出字段的值（数据库里的数据） old_field_val = getattr(self.instance, field) #提交过来的数据 form_val = self.cleaned_data.get(field) #如果两个数据不匹配 if old_field_val != form_val: #就提示只读字段不能修改 #add_error是字段级别的错误 self.add_error(field, \"Readonly Field: field should be '{value}' ,not '{new_value}' \".format(**{'value': old_field_val, 'new_value': form_val})) 学员报名合同及身份信息上传 必须勾选报名合同协议 必须上传个人证件信息 最多只能上传三个文件 文件大小2M以内 列出已上传文件 @csrf_exempt def enrollment_fileupload(request,enrollment_id): '''学员报名文件上传''' enrollment_upload_dir = os.path.join(conf.settings.CRM_FILE_UOLOAD_DIR,enrollment_id) #第一次上传图片就创建目录，学员上传第二章图片的时候，会判断目录是否已经存在 #因为如果目录存在还mkdir就会报错，所以这里要做判断 if not os.path.isdir(enrollment_upload_dir): os.mkdir(enrollment_upload_dir) #获取上传文件的对象 file_obj = request.FILES.get('file') #最多只允许上传3个文件 if len(os.listdir(enrollment_upload_dir)) # 列出学员已上传的文件 upload_files = [] enrollment_upload_dir = os.path.join(conf.settings.CRM_FILE_UOLOAD_DIR, enrollment_id) if os.path.isdir(enrollment_upload_dir): upload_files = os.listdir(enrollment_upload_dir) 合同审核 学员提交报名信息后，应该是等待审核状态 管理员改审核状态为ture， 并保存提交时间 所有权限 perm_dic = { # 'crm_table_index': ['table_index', 'GET', [], {}, ], # 可以查看CRM APP里所有数据库表 # 可以查看每张表里所有的数据 'crm_table_list': ['table_obj_list', 'GET', [], {}], # 'crm_table_list': ['table_obj_list', 'GET', [], {'source':0,'status':0}], # 添加参数：只能访问来源是qq和未报名的客户 # 可以访问表里每条数据的修改页 'crm_table_list_view': ['table_obj_change', 'GET', [], {}], # 可以对表里的每条数据进行修改 'crm_table_list_change': ['table_obj_change', 'POST', [], {}], # 可以访问数据增加页 'crm_table_list_add_view': ['table_obj_add ', 'GET', [], {}], # 可以添加表数据 'crm_table_list_add': ['table_obj_add ', 'POST', [], {}], } *per_name和split用法 > a,*b,c=[1,2,3,4,5,6] >a 1 >c 6 >b [2,3,4,5] >>>a='crm_table_list' >>>a.split('_') ['crm','table','list'] >>> >>>app_name,*per_name=a.split('_') >>> >>>app_name 'crm' >>>per_name # 权限 ['table','list'] Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"后端/django整理/Django Debug Toolbar的安装及使用.html":{"url":"后端/django整理/Django Debug Toolbar的安装及使用.html","title":"Django Debug Toolbar的安装及使用","keywords":"","body":"Django Debug Toolbar的安装及使用 安装 通过pip去安装 pip install django-debug-toolbar 安装完成以后，测试一下是否能够导入 import debug_toolbar 设置INSTALLED_APPS # 在settings文件内的,INSTALLED_APPS内添加一下 INSTALLED_APPS = [ 'django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', 'web', 'debug_toolbar' ] # 如果静态目录改变了，也需要配置一下 # STATIC_URL = '/static/' 配置URL # 在原有的urlpatterns下面添加以下代码 from django.conf import settings if settings.DEBUG: from django.conf.urls import include, url import debug_toolbar urlpatterns = [ url(r'^__debug__/', include(debug_toolbar.urls)), ] + urlpatterns 配置中间件 # 在原有的中间件后面添加下面的中间件 MIDDLEWARE = [ 'django.middleware.security.SecurityMiddleware', 'django.contrib.sessions.middleware.SessionMiddleware', 'django.middleware.common.CommonMiddleware', # 'django.middleware.csrf.CsrfViewMiddleware', 'django.contrib.auth.middleware.AuthenticationMiddleware', 'django.contrib.messages.middleware.MessageMiddleware', 'django.middleware.clickjacking.XFrameOptionsMiddleware', 'web.middlewares.AuthMiddleware', # 添加到这里 'debug_toolbar.middleware.DebugToolbarMiddleware', ] 配置 INTERNAL_IPS # 在settings文件内添加下面的代码 INTERNAL_IPS = ['127.0.0.1'] 配置JQUERY源 由于它默认使用的是google的jquery地址，国内访问不稳定，建议设置成国内的源，比如 # 在settings.py内添加下面的代码 DEBUG_TOOLBAR_CONFIG = { 'JQUERY_URL': 'https://cdn.bootcss.com/jquery/2.2.4/jquery.min.js', } 使用 按照上面的步骤配置完成以后，在debug模式下，启动项目，再打开页面，发现左上角有一个DJDT的小图标，点击图标可以看到界面了。 Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"后端/django整理/Django-redis.html":{"url":"后端/django整理/Django-redis.html","title":"Django Redis","keywords":"","body":"Django-redis 1.安装 pip install django-redis settings.py CACHES = { \"default\":{ \"BACKEND\":\"django_redis.cache.RedisCache\", \"LOCATION\":\"redis://127.0.0.1:6379/1\", \"OPTIONS\":{ \"CLIENT_CLASS\":\"django_redis.client.DefaultClient\" } } } django缓存的使用 2.视图函数中使用缓存 下面这段代码将my_view这个视图函数缓存60*15秒，即15分钟，这个视图中所有的url都会创建一个缓存 from django.views.decorators.cache import cache_page @cache_page(60 * 15) def my_view(request): return render(request, 'index.html') 但是，需要说明的是，给视图添加缓存是有风险的，如果视图所展示的网页中有经常动态变动的信息，那么被添加缓存命不可取。 缓存整个视图最实用的场景应该是这个视图所展示的网页的内容基本上不怎么变动，或者说在很长一段时间内不需要变动，这样使用缓存就非常有效。 3.URLconf中使用缓存 上面说了函数视图使用缓存，但是我们可能还有一种场景，那就是多个 URL 指向同一个函数视图，但是我只想缓存一部分的 URL，这时候就可以采用在 URLconf 中使用缓存，这样就指定了哪些 URL 需要缓存。 下面分别表示了函数视图和类视图的路由中使用缓存的方式，基本一致： from django.views.decorators.cache import cache _page urlpatterns = [ url(r'^foo/([0-9]{1,2})/$',cache_page(60 * 15)(my_view)), url(r'^$', cache_page(60 * 30)(IndexView.as_view()), name='index'), ] URLconf 使用缓存和视图函数使用缓存需要注意的地方是一样的，因为它们都是缓存整个页面，所有都需要考虑是否整个页面都应该缓存。 4.函数中使用缓存 函数中使用缓存是最基本的使用方法，跟在其他非 django 中使用的方式一致，无非就是使用 set() 和 get() 方法。 例如我有一个使用场景：我的博客的文章是使用的 markdown 的格式输入的，所以每次展现到前端之前后端都需要把文章的内容进行一次 markdown 转化，这个渲染的过程难免会有点影响性能，所以我可以使用缓存来存放已经被渲染过的文章内容。具体的代码片段如下： ud = obj.update_date.strftime(\"%Y%m%d%H%M%S\") md_key = '{}_md_{}'.format(obj.id, ud) cache_md = cache.get(md_key) if cache_md: md = cache_md else: md = markdown.Markdown(extensions=[ 'markdown.extensions.extra', 'markdown.extensions.codehilite', TocExtension(slugify=slugify), ]) cache.set(md_key, md, 60 * 60 * 12) 上面的代码中，我选择文章的 ID 和文章更新的日期作为缓存的 key，这样可以保证当文章更改的时候能够丢弃旧的缓存进而使用新的缓存，而当文章没有更新的时候，缓存可以一直被调用，直到缓存按照设置的过期时间过期。 5.模板中使用缓存 模板中使用缓存是我比较推荐的一种缓存方式，因为使用这种方式可以充分的考虑缓存的颗粒度，细分颗粒度，可以保证只缓存那些适合使用缓存的 HTML 片段。 具体的使用方式如下，首先加载 cache 过滤器，然后使用模板标签语法把需要缓存的片段包围起来即可。 {% load cache %} {% cache 500 ‘cache_name’ %} container {% endcache %} 总结 缓存的使用原则 先说一下我在使用缓存的时候遇到的问题，我之前给我的很多视图函数还有URL路由添加了缓存，也就是缓存整个页面，后来发现出问题了，因为我的每个页面都有导航栏，而导航栏上面有登录和登出按钮，这样如果缓存起来的话，就无法让用户显示登录和登出了，并且，有表单的页面也无法提交表单，总之，缓存整个页面是一件有风险的行为。 那么到底哪些时候应该用缓存呢？ 据我目前的理解，下面这些时候可以用缓存： 纯静态页面 读取了数据库信息，但是不经常变动的页面，比如文章热门排行榜，这个调用数据库信息并且还要排序的完全可以使用缓存，因为不需要实时展现最新的 HTML 的片段，比如整个页面都经常变动，但是有个侧边栏不经常变动，就可以缓存侧边栏 需要使用复杂逻辑生成的 HTML 片段，使用缓存可以减少多次重复操作 Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"后端/django整理/django中将重要信息存储并引用.html":{"url":"后端/django整理/django中将重要信息存储并引用.html","title":"django中将重要信息存储并引用","keywords":"","body":"将重要配置信息存放在环境变量中 # 创建环境变量文件 vim ~/.bash_profile # 添加 export 变量名=值 # 例如我将sql密码存入。假设密码为123456abc export sqlpwd=123456abc # 保存退出 source ～/.bash_profile 在django的配置文件中关于数据库这样写: # apps/settings.py DATABASES = { 'default': { 'ENGINE': 'django.db.backends.mysql', 'NAME': 'blog', 'USER': 'root', 'PASSWORD': os.environ.get('sqlpwd'), 'HOST': '127.0.0.1', 'PORT': 3306, 'OPTIONS': {'charset': 'utf8mb4'}, } } 这样就可以安全的去上传啦 Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"后端/django整理/drf技术扩展.html":{"url":"后端/django整理/drf技术扩展.html","title":"drf技术扩展","keywords":"","body":"一.技术点 xadmin管理后台 国际化 可视化api api文档管理 二.安装 pip install djangorestframework pip install markdown # 图片处理 pip install pillow # 过滤器 pip install django-filter # drf的文档支持 pip install coreapi # drf对象级别的权限支持 pip install django-guardian # 后端服务器解决跨域问题 pip install django-cors-headers 三.项目目录 extra_apps (源码包) apps (所有app) media(保存图片) db_tools(数据库相关) # 把extra_apps 和apps标记为sources root # pycharm文件夹右键点击mark diectory as->source root # settings加入 import sys sys.path.insert(0,BASE_DIR) sys.path.insert(0,os.path.join(BASE_DIR, 'apps')) sys.path.insert(0,os.path.join(BASE_DIR, 'extra_apps')) 四.xadmin下载安装 #下载地址: https://pan.baidu.com/s/1NuvdYSX-ENWzX_JfqibucA #打开后将xadmin放入extra_apps中 #将txt文件放入项目根目录下，执行 pip install -r requirements.txt 下载相关依赖 settings.py #需要添加的部分 #app INSTALLED_APPS = [ .... 'xadmin', #替换django后台管理 'crispy_forms', 'testapp', 'rest_framework', 'coreschema', ] #中文支持，django1.8以后支持；1.8以前是zh-cn LANGUAGE_CODE = 'zh-hans' TIME_ZONE = 'Asia/Shanghai' USE_I18N = True USE_L10N = True USE_TZ = False # 静态资源 STATIC_URL = '/static/' # 图像 MEDIA_URL = \"/media/\" STATICFILES_DIRS = ( os.path.join(BASE_DIR, \"static\"), ) MEDIA_ROOT = os.path.join(BASE_DIR, \"media\") urls.py import xadmin urlpatterns = [ url('xadmin/', xadmin.site.urls), ] 创建testapp #testapp/__init__.py default_app_config = 'testapp.apps.TestappConfig' #testapp/adminx.py from xadmin import views import xadmin # 创建xadmin的最基本管理器配置，并与view绑定 class BaseSetting(object): # 开启主题功能 enable_themes = True use_bootswatch = True # 将基本配置管理与view绑定 xadmin.site.register(views.BaseAdminView,BaseSetting) #testapp/apps.py from django.apps import AppConfig class TestappConfig(AppConfig): name = 'testapp' verbose_name = '用户' 终端执行 python manage.py makemigrations python manage.py migrate python manage.py createsuperuser 五.可视化api #testapp/models.py from django.db import models class User(models.Model): uname = models.CharField(max_length=20) upwd = models.CharField(max_length=40) uemil = models.CharField(max_length=30) urelname =models.CharField(max_length=20,default='') uadr = models.CharField(max_length=100,default='') uphone = models.CharField(max_length=11,default='') #testapp/serializer.py from .models import User from rest_framework import serializers # API返回序列化后的接口字段 class UserSerializer(serializers.HyperlinkedModelSerializer): class Meta: model = User fields = '__all__' #testapp/views.py from rest_framework import viewsets from .serializer import UserSerializer from .models import User class UserViewSet(viewsets.ModelViewSet): queryset = User.objects.all() serializer_class = UserSerializer #根路由 from django.conf.urls import url, include from rest_framework import routers from testapp.views import UserViewSet router = routers.DefaultRouter() router.register(r'users', UserViewSet) urlpatterns = [ url('xadmin/', xadmin.site.urls), url(r'^', include(router.urls)), url(r'^api-auth/', include('rest_framework.urls', namespace='rest_framework')), ] 六.api文档管理 #根路由下添加 from rest_framework.documentation import include_docs_urls urlpatterns = [ ..... url(r'docs/',include_docs_urls(title=\"后台管理\")), ] Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"后端/django整理/memcached.html":{"url":"后端/django整理/memcached.html","title":"Memcached","keywords":"","body":"memcached 安装 直接通过系统命令安装 yum install memcached apt install memcached 或者去官网下载源码包编译安装 https://memcached.org/ 启动 # 以后台方式启动Memcached memcached -d 默认端口号是：11211 在Django中配置memcache CACHES = { 'default': { 'BACKEND': 'django.core.cache.backends.memcached.MemcachedCache', 'LOCATION': '127.0.0.1:11211', } } 如果没有安装python的memcache客户端，需要先安装一下 pip install python-memcached 在Django中使用memcache from django.core.cache import cache cache.set('a', 1) cache.get('a') 注意：在django中的memcahe会自动帮我们做对象的序列化和反序列化，我们只管往里设置和往外获取值就行了。 redis和memcahed的区别 redis支持多种类型，memcached只支持字符串 redis支持持久化，memcached不支持 django原生支持memcached，redis需要自己再安装插件 memcached启动速度快，redis当持久化的数据较多时，启动速度慢。 使用memcahed作缓存，需要预热，redis不需要。 Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"后端/django整理/Nginx+Gunicorn部署django.html":{"url":"后端/django整理/Nginx+Gunicorn部署django.html","title":"Nginx+Gunicorn部署django","keywords":"","body":"Django项目部署 在 Linux 服务器上使用 Nginx + Gunicorn 部署 Django 项目的正确姿势 目录 [TOC] 1.项目准备 首先需要把自己本地的项目放到服务器上面来，我使用的是 Github 克隆项目，这种从代码库克隆的方式是比较推荐的，因为可以持续的使用 pull 来让服务器上面的项目保持跟代码仓库中同步。Github 的安装、配置和使用这里省略，如果需要请自行去查阅相关资料完成操作。 1.1 克隆项目 在这里我统一把项目放在/home/project下 我的项目名为write，克隆下来后目录是这样的/home/project/write 1.2 创建虚拟环境 write同级即project下执行 virtualenv write_env Ok,此时目录是这样 /home/project/write /home/project/write_env source write_env/bin/activate 进入虚拟环境 安装依赖， 配置好数据库， 收集静态资源 python manage.py collectstatic 运行项目 127.0.0.1:8000 成功之后 2 开始部署 1.1 安装和配置Gunicorn 虚拟环境中安装Gunicorn pip install gunicorn 创建项目的Gunicorn配置文件（退出虚拟环境） sudo vim /etc/systemd/system/gunicorn_write.service 配置信息如下 [Unit] Description=gunicorn daemon After=network.target [Service] User=ubuntu Group=ubuntu WorkingDirectory=/home/ubuntu/test ExecStart=/home/ubuntu/venv/bin/gunicorn --access-logfile - --workers 2 --bind unix:/home/ubuntu/test/test.sock mongo.wsgi:application [Install] WantedBy=multi-user.target 上面的配置信息中需要根据自己的项目改的有以下几个地方： User 填写自己当前用户名称 WorkingDirectory 填写项目的地址 ExecStart 中第一个地址是虚拟环境中 gunicorn 的目录，所以只需要改前半部分虚拟环境的地址即可 workers 2 这里是表示2个进程，可以自己改 unix 这里的地址是生成一个 sock 文件的地址，直接写在项目的根目录即可 izone.wsgi 表示的是项目中 wsgi.py 的地址，我的项目中就是在 izone 文件夹下的 1.2启动配置文件 文件配置完成之后，使用下面的命令启动服务： ~$ sudo systemctl start gunicorn_write ~$ sudo systemctl enable gunicorn_write 查看服务的状态可以使用命令： ~$ sudo systemctl status gunicorn_write 上面的命令启动没有问题可以看看自己的项目的跟目录下面，应该会多一个 tendcod.sock 文件的。 后续如果对 gunicorn 配置文件做了修改，那么应该先使用这个命令之后重启： ~$ sudo systemctl daemon-reload 然后再使用重启命令： ~$ sudo systemctl restart gunicorn_write 1.3 配置 Nginx 首先创建一个 Nginx 配置文件，不要使用默认的那个： ~$ sudo nano /etc/nginx/sites-available/mynginx 配置信息如下： server { # 端口和域名 listen 80; server_name zskin.xin; # 日志 access_log /home/alex/tendcode/logs/nginx.access.log; error_log /home/alex/tendcode/logs/nginx.error.log; # 不记录访问不到 favicon.ico 的报错日志 location = /favicon.ico { access_log off; log_not_found off; } # static 和 media 的地址 location /static/ { root /home/project/write; } location /media/ { root /home/project/write; } # gunicorn 中生成的文件的地址 location / { include proxy_params; proxy_pass http://unix:/home/ubuntu/test/test.sock; } } server { listen 80; server_name tendcode.com; rewrite ^(.*) http://www.zskin.xin$1 permanent; } 第一个 server 是主要的配置，第二 server 是实现301跳转，即让不带 www 的域名跳转到带有 www 的域名上面。 1.4 连接 Nginx 配置 上面的配置检查好之后，使用下面的命令来将这个配置跟 Nginx 建立连接，使用命令： ~$ sudo ln -s /etc/nginx/sites-available/mynginx /etc/nginx/sites-enabled 运行完毕之后可以查看一下 Nginx 的运营情况，看看会不会报错： ~$ sudo nginx -t 如果上面这句没有报错，那么恭喜你，你的配置文件没有问题，可以继续下一步，如果报错了，需要按照报错的信息去更改配置文件中对应行的代码，好好检查一下吧！ 没报错的话，重启一下 Nginx： ~$ sudo systemctl restart nginx 好了，重启 Nginx 之后可以登录自己配置的域名，看看自己的项目是不是已经成功的运行了呢！ 后续维护 之后的项目维护中，如果更改了 gunicorn 的配置文件，那么需要依次执行下面两条语句去重启服务，如果只是修改了 Django 项目的内容，只需要单独执行第二条重启命令即可： ~$ sudo systemctl daemon-reload ~$ sudo systemctl restart gunicorn_write 如果修改了 Nginx 的配置文件，那么需要依次执行下面两条语句去重启服务： ~$ sudo nginx -t ~$ sudo systemctl restart nginx Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"后端/django整理/七牛云存储资源.html":{"url":"后端/django整理/七牛云存储资源.html","title":"七牛云存储资源","keywords":"","body":"七牛云存储资源 服务器上资源有限，所以如果把所有文件都放到服务器上，肯定有所不便 准备 首先注册七牛云并实名认证，需要上传本人手持身份证正反面照片 注册完成后回到主页 流程如下 对象存储—>新建存储空间—>绑定域名【注1】—>阿里云|Others Dns解析【注2】—>内容管理—>上传文件 配置 这里的主要坑在注1和注2,单独摘出来讲 # 比如我的域名是这样 s001.xin 我绑定的域名就写成 xx.s001.xin 通常可用pic.s001.xin 我使用的是 qiniu.s001.xin 绑定成功后在阿里云（因为我买的是阿里云的）域名DNS解析 添加解析 照这张图第一条cname解析就可以了。 【记录值复制自己的】 开始使用 配置好 差不多10分钟阿里和七牛两边就同时解析成功了 直接在对象存储中的内容管理上传图片 这篇markdown中的图片就是七牛存储的 很方便吧～ Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"后端/django整理/数据库索引相关操作.html":{"url":"后端/django整理/数据库索引相关操作.html","title":"数据库索引相关操作","keywords":"","body":"数据库索引相关操作 分析SQL语句的性能 使用explain去分析SQL语句，比如 explain SELECT `copyrights`.`pcid`, `copyrights`.`pid`, `copyrights`.`cid`, `copyrights`.`roles` FROM `copyrights` WHERE `copyrights`.`pid` = 84160; +----+-------------+------------+------------+------+---------------+-----------+---------+-------+------+----------+-------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+------------+------------+------+---------------+-----------+---------+-------+------+----------+-------+ | 1 | SIMPLE | copyrights | NULL | ref | pid_index | pid_index | 8 | const | 2 | 100.00 | NULL | +----+-------------+------------+------------+------+---------------+-----------+---------+-------+------+----------+-------+ 注意观察上面的输出结果，里面的key, possible_keys（用到的索引），rows(扫描的行数) 创建一个索引 在copyrithgs表的pid字段上，建立一个名为pid_index的索引： create index `pid_index` on `copyrights` (`pid`); 查看索引 desc copyrights; +-------+---------------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +-------+---------------------+------+-----+---------+-------+ | pcid | varchar(32) | NO | PRI | NULL | | | pid | bigint(20) unsigned | NO | MUL | 0 | | | cid | bigint(20) unsigned | NO | | 0 | | | roles | varchar(32) | YES | | NULL | | +-------+---------------------+------+-----+---------+-------+ 观察Key字段，是否有值，比如这里的pid的Key字段是MUL，表明索引建立成功。 也可以用下面的语句 show index from `copyrights`; show create table `copyrights`; 删除索引 drop index `pid_index` on `copyrights`; 索引会在符合一定的查询条件时起到加速查询速度的作用，但是需要在插入和更新操作的时候，去更新索引，这样就导致插入或者更新操作变慢。 索引更适合在读操作比多，写操作比较少的表上。 Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"后端/django整理/用户权限验证.html":{"url":"后端/django整理/用户权限验证.html","title":"用户权限验证","keywords":"","body":"手机号码正则： REGEX_MOBILE = \"^1[358]\\d{9}$|^147\\d{8}$|^176\\d{8}$\" token过期时间 # TOKEN过期时间 import datetime JWT_AUTH = { 'JWT_EXPIRATION_DELTA': datetime.timedelta(days=7), #也可以设置seconds=20 'JWT_AUTH_HEADER_PREFIX': 'JWT', #JWT跟前端保持一致，比如“token”这里设置成JWT } 用户权限验证 url from rest_framework import routers from rest_framework.authtoken import views from rest_framework_jwt.views import obtain_jwt_token from users.views import UserViewset # franmework的router在此注册 router = routers.DefaultRouter() router.register(r'users', UserViewset, base_name=\"users\") urlpatterns = [ url(r'^xadmin/', xadmin.site.urls), url(r'^api-auth', include('rest_framework.urls', namespace='rest_framework')), url(r'^api',include(router.urls)), url('api-token-auth/', views.obtain_auth_token), url('jwt-auth/', obtain_jwt_token), url(r'^$', TemplateView.as_view(template_name=\"index.html\")), ] Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"后端/django整理/简约博客.html":{"url":"后端/django整理/简约博客.html","title":"简约博客","keywords":"","body":"技术点 django.contrib.models中AbstractUser的继承 imagekit上传图片 类视图的使用 https://www.jianshu.com/p/6d6c890f5f72 markdown的用法 https://segmentfault.com/a/1190000009536349 难点 获取特定文章分类下所有文章的数量 # 假设model如下 class Article(models.Model): title = models.CharField('标题', max_length=200) body = models.TextField('正文') created_time = models.DateTimeField('创建时间', auto_now_add=True) author = models.ForeignKey(settings.AUTH_USER_MODEL, verbose_name='作者', on_delete=models.CASCADE) category = models.ForeignKey('Category', verbose_name='分类', on_delete=models.CASCADE) tags = models.ManyToManyField('Tag', verbose_name='标签集合', blank=True) def __str__(self): return self.title class Category(models.Model): name = models.CharField('分类名', max_length=30) def __str__(self): return self.name class Tag(models.Model): name = models.CharField('标签名', max_length=30) def __str__(self): return self.name ​ 因为article与category有外键关联,所以可以在模板中使用 # i是该文章分类 进阶: 若是要获取特定文章分类特定时间段内的文章数量,就需要引用Django中的annote方法 from django.db.models.aggregates import Count category_list = Category.objects.filter(article__created_time_gt=(2015,1,1)).annotate( num_articles=Count('article')).filter(num_articles__gt=0) ​ Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"后端/django整理/项目总结.html":{"url":"后端/django整理/项目总结.html","title":"项目总结","keywords":"","body":"项目总结 项目名: 天天生鲜(电商) 开发人数: 4人 开发周期: 1周 功能实现: 登陆注册 商品分类 商品展示 商品详情 添加购物车 在线下单 用户中心 技术点: Django Rest Framework xadmin后台 Model数据模型 django url实现路由配置 session会话技术 git版本控制工具 pyenv venv vitualenv熟练使用 部署上线 进阶知识点: makemigrations原理理解 爬虫数据导入数据库 数据增删改查熟练掌握 Git高级应用 开发中的问题 刘忠强 张咪 邢田田 负责的模块： 购物车模块 主要是实现商品的购买 遇到的问题及解决方法： 1. 页面中的购物车数量不会随着用户商品数量的增加而变化。 if(hasorder) { oShownum.innerHTML = '0'; } 向网页中写入数据时将其设置为从0开始。 2. 购物车结算购物金额无法和商品数量及单价对应吻合。 num = parseFloat($('.num_show').val()); price = parseFloat($('#gprice').text()); total = num*price; $('#gtotal').text(total.toFixed(2)+'元') 获取商品数量和一个商品的单价后进行商品数量*商品单价=结算总价，并设置显示的数字以两位小数来显示。 自己的不足： 1. 概念迷糊不清。在使用这段时间学习的东西时，概念和概念之间会搞混。 2. 前后所学内容运用不熟练，不能够融会贯通。以至于会处于一种‘我要怎么做，该怎么做的状态’。 3. 理解别人所写的内容会有一定的难度。 通过项目学到的东西： 1. 通过和大家一起学习，让我对自己学习成果有一个清楚的认知。 2. 清楚了session 、cookie 两者之间的区别。Session存用户的信息在浏览器中，清除了浏览器中的记录就可以清除用户在浏览器中的信息，通过reuqest.session.get(‘uid’)就可以获取到用户相应的信息。Cookie存在服务器中。 师仁杰 开发职责: 1.分配小组任务 2.搭建项目环境及git库 3.与组员讨论解决相关模块问题 4.爬虫爬取数据导入数据库(demo实现) 问题及解决： 1.model设计整体欠缺 考虑字段欠缺导致后面项目数据渲染困难,重新设计model 2.git 回滚,fetch,merge解决本地与远程仓库冲突问题,查各种资料csdn,strakflow等 Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"后端/flask整理/Flask+Vue快速打造个人网站（一）.html":{"url":"后端/flask整理/Flask+Vue快速打造个人网站（一）.html","title":"Flask+Vue快速打造个人网站（一）","keywords":"","body":"Flask+Vue快速打造个人网站(一) 记录一下起始日期2018/9/12/21:16 demo地址：宅神的生活 网站实现功能 作为练手项目，该项目主要练习前后端分离项目从开发到部署的流程 图片分类 图片展示 图片、视频后台上传 视频播放 前期准备 文件存储 ：七牛云 用到的库 前端 后端 flask-admin 后台 flask-restful API Flask-login 后台登录 mysql Flask-migrate 数据库迁移 快速开始 对细节不感兴趣或者想直接撸代码的同学这边请：后端 前端 别忘了点赞哦～ 项目搭建 前端主要用了Vue-cli快速搭建项目，使用bootstrap-vue作为UI组件库 # npm安装失败一直是前端的痛，果断选了yarn yarn add vue-cli # 进入vue的GUI页面快速创建项目 vue ui 预设选择默认 右上角搜索插件并安装 axios less qs store2 css-loader less-loader vuex vue-template-compiler Bootstrap-vue vue-router 目录结构 main.js入口 import Vue from 'vue' import App from './App.vue' import router from './router' import store from './store' import BootstrapVue from 'bootstrap-vue' import 'bootstrap/dist/css/bootstrap.css' import 'bootstrap-vue/dist/bootstrap-vue.css' # 引入ui库 Vue.use(BootstrapVue); Vue.config.productionTip = false; new Vue({ router, store, render: h => h(App) }).$mount('#app'); router.js全局路由 import Vue from 'vue' import Router from 'vue-router' # 所用到的组件注册 import Home from './views/Home.vue' import About from './views/About' import ImgCategory from './views/ImgCategory' import ImgDetail from '@/components/ImgDetail.vue' import Video from './views/Video' import VideoDetail from '@/components/VideoDetail' Vue.use(Router); const routes= [ {path: '/', name: 'home', component: Home}, {path: '/about', name: 'about', component: About}, {path: '/img_category', name: 'category', component: ImgCategory}, {path: '/photo', name: 'photo', component: ImgDetail}, {path: '/video', name: 'video', component: Video}, {path: '/iframe', name: 'video_show', component: VideoDetail}, ]; const router = new Router({ mode: 'history', routes: routes }); # 各个模块留一个exprot可供其他模块调用 export default router configUrl.js调用后端接口的地址前缀，方便生产环境与开发环境切换 # type: http/https const type = document.location.protocol; const baseul = type + '//oj.s001.xin/api'; const baseui = type + '//127.0.0.1:5000/api'; export default { # 开发环境改成baseui就可以 baseURL: baseul } requestUrl.js后端接口 # 代替ajax import axios from \"axios\"; # json格式转换 import qs from 'qs'; import baseURL from './configUrl.js'; # 请求头设置 function baseRequest(method, path, params, data, type) { method = method.toUpperCase() || 'GET'; let url = ''; let paramsobj = { params: params }; if (type === 'msg') { url = baseURL.onbaseURL; } else { url = baseURL.baseURL; } axios.defaults.baseURL = url; if (method === 'POST') { axios.defaults.headers.post['Content-Type'] = 'application/x-www-form-urlencoded'; return axios.post(path, qs.stringify(data)); } else if (method === 'GET') { return axios.get(path, paramsobj); } else { return axios.delete(path, qs.stringify(data)); } } # 获取所有分类 export let get_category = function get_category(params){ return baseRequest(\"GET\", '/category/all/', params, ''); }; # 获取所有视频 export let get_all_video = function get_all_video(params){ return baseRequest(\"GET\", '/video/all/', params, ''); }; # 获取所有图片 export let get_all_img = function get_all_img(params){ return baseRequest(\"GET\", '/image/all/', params, ''); }; # 根据分类id获取图片 export let get_img = function get_all_img(params){ return baseRequest(\"GET\", '/image/'+params.id, params, ''); }; vue.config.js设置webpack参数 # 写这个的目的是为webpack生产环境禁用端口检查 module.exports = { configureWebpack: { // other webpack options to merge in ... }, // devServer Options don't belong into `configureWebpack` devServer: { host: '0.0.0.0', hot: true, disableHostCheck: true, }, }; 以上是主要的一些模块化js，请求接口的页面都大致一样，为了对初接触VUE的小伙伴更友好些，我用图片分类和跳转分类下所有图片页面进行讲解 组件库: Bootstrap-Vue 所有页面都在src目录下，assets目录放静态资源, views放一级视图 components目录放复用组件，比如header，footer等 在这里说下vue组件的语法 ... # 导入所需模块 import ... # 可供调用模块 export default{ name: '模块名', props: { # 属性 msg: String }, data(){ # 初始化变量，将动态的数据通过方法、接口、事件赋值给变量渲染到页面 return { msg: '' } }, created(){ # 周期函数 created、页面加载完成调用该方法 this.Init(); }, methods: { # 变量赋值的方法 Init(){ }, Click(i){ this.msg = i; } } } 样式 VUE相关组件、周期函数了解更多 ImgCategory.vue影集一级视图 写在代码前面对照着理解 script部分 首先导入接口， data中定义空数组，拿到图片分类信息后填入数组中 页面加载完之后调用一次GetCate方法 填充数据 Detail方法用于事件点击，分类详情页面 Look import { get_category } from '../../requestUrl' export default { name: 'ImgCategory', data(){ return { cateList: [] } }, created(){ this.GetCate(); }, methods: { GetCate(){ # 调用接口，该不需要传参，直接使用then获取后端数据 get_category({}).then((data)=>{ let all = data.data; if(all.code == 0){ if(all.data){ all.data.forEach((i)=>{ this.cateList.push({ \"title\":i.title, \"id\": i.id }) }) } } }) }, Detail(id){ this.$router.push('/photo?id='+ id); }, } } ImgDetail二级页面分类下所有图片 上个页面通过分类点击进入详情、详情页通过分类id获取id下所有图片 import {get_img} from '../../requestUrl.js'; export default { name: 'ImgDetail', props: { msg: String }, data(){ return { id: '', imgList:[] } }, created(){ this.ImgShow(); }, methods: { ImgShow(){ # 解析url截取id，将id赋值给全局变量id let path = document.location.search; if(path){ this.id = decodeURIComponent(path.split('=')[1]); this.GetAllImg(this.id) } }, GetAllImg(id){ # 通过id获取分类所属图片 get_img({id:id}).then((data)=>{ let all = data.data; if(all.code == 0){ if(all.data){ all.data.forEach((i)=>{ this.imgList.push({ \"desc\":i.desc, \"id\": i.id, \"url\": i.image_url }) }) } } }) } } } Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"后端/flask整理/Flask+Vue快速打造个人网站（三）.html":{"url":"后端/flask整理/Flask+Vue快速打造个人网站（三）.html","title":"Flask+Vue快速打造个人网站（三）","keywords":"","body":"Flask+Vue快速打造个人网站（三） 2018.9.20 19:39 @[TOC] 后端部分 需求三: 虽然大部分视图都在前端展示，但缺少一个可以展示数据，实时增删改查的后台，要实现像django那样的后台模型视图，用到了flask-admin admins目录下的__init__.py from flask_admin import Admin, AdminIndexView admin = Admin( url='/api', index_view=AdminIndexView( url=\"/api/admin/\", name=\"导航栏\", ), name=u\"个人空间\", template_mode='bootstrap3' ) app.py入口文件 from admins import admin, AdminUser, db, ModelView from flask_babelex import Babel from pictures.models import * # model视图注册 admin.add_view(ModelView(Picture, db.session, name=u\"图片\")) admin.add_view(ModelView(Category, db.session, name=\"分类\")) # 初始化app def create_app(config_name): app = Flask(config_name) app.config.from_object(config[config_name]) # 国际化 babel = Babel(app) # 应用注册 db.init_app(app) admin.init_app(app) config[config_name].init_app(app) return app config.py class Config(object): BABEL_DEFAULT_LOCALE = 'zh_CN' @staticmethod def init_app(app): pass 需求四: admin后台有了，但是不能对外放开，flask-login派上了用场 步骤如下 flowchat st=>start: 开始 op=>operation: 创建AdminUser表 op1=>operation: 登录注册接口 op2=>operation: 接口路由注册 e=>end: 结束 st->op->op1->op2->e admins/__init__ \"\"\" 为了防止db循环调用 将db在仅初始化时调用的admin中注册 \"\"\" from flask_admin import Admin, AdminIndexView from flask_sqlalchemy import SQLAlchemy from flask import Flask, url_for, redirect, render_template, request, make_response # 校验密码的方法 from werkzeug.security import generate_password_hash, check_password_hash from wtforms import form, fields, validators from flask_restful import Resource import flask_login as login from flask_admin.contrib.sqla import ModelView db = SQLAlchemy() class AdminUser(db.Model): __tablename__ = \"admin_user\" id = db.Column(db.Integer, primary_key=True) username = db.Column(db.String(50), unique=True) email = db.Column(db.String(50)) password = db.Column(db.String(250)) # Flask-Login integration # NOTE: is_authenticated, is_active, and is_anonymous # are methods in Flask-Login 0: raise validators.ValidationError('Duplicate username') # Create customized model view class class MyModelView(ModelView): column_list = (\"username\", \"email\") def is_accessible(self): return login.current_user.is_authenticated # Create customized index view class class MyAdminIndexView(AdminIndexView): def is_accessible(self): return login.current_user.is_authenticated class Index(Resource): def get(self): return make_response(render_template('home.html', user=login.current_user)) class LoginView(Resource): def __init__(self): self.form = LoginForm(request.form) def get(self): return make_response(render_template('form.html', form=self.form)) def post(self): if self.form.validate(): user = self.form.get_user() if user is None: raise validators.ValidationError('Invalid user') password = request.form.get(\"password\") if AdminUser.check_password(AdminUser, user.password, password): login.login_user(user) return redirect(url_for('api.index')) else: raise validators.ValidationError('Invalid password') return make_response(render_template('form.html', form=self.form)) class RegisterView(Resource): def __init__(self): self.form = RegistrationForm(request.form) def get(self): return make_response(render_template('form.html', form=self.form)) def post(self): if self.form.validate(): user = AdminUser() self.form.populate_obj(user) db.session.add(user) db.session.commit() login.login_user(user) return redirect(url_for('api.index')) return make_response(render_template('form.html', form=self.form)) class LogoutView(Resource): def get(self): login.logout_user() return redirect(url_for('api.index')) admin = Admin( url='/api', index_view=MyAdminIndexView( url=\"/api/admin/\", name=\"导航栏\", ), name=u\"个人空间\", template_mode='bootstrap3' ) routes.py ... docs.add_resource(Index, '/', endpoint=\"index\") docs.add_resource(LogoutView, '/logout/', endpoint=\"logout\") docs.add_resource(LoginView, '/login/', endpoint=\"login\") app.py from admins import admin, login, AdminUser, db, ModelView def create_app(config_name): ... # flask-login初始化 def init_login(): login_manager = login.LoginManager() login_manager.setup_app(app) @login_manager.user_loader def load_user(user_id): return db.session.query(AdminUser).get(user_id) init_login() 需求五: 将错误自定义，并收集起来，方便查看 app.py def create_app(config_name): ... # 日志配置 handler = logging.FileHandler('app.log', encoding='UTF-8') logging_format = logging.Formatter( '%(asctime)s - %(levelname)s - %(filename)s - %(funcName)s - %(lineno)s - %(message)s') handler.setFormatter(logging_format) app.logger.addHandler(handler) ... utils/common.py # 请求成功 def trueReturn(data): return { \"code\": 0, \"data\": data, \"msg\": \"请求成功\" } # 内部错误 def falseReturn(msg): return { \"code\": 1, \"data\": '', \"msg\": msg } # 无权限 def VaildReturn(data): return { \"code\": 4, \"data\": data, \"msg\": \"无效验证\" } # 数据库操作错误 def MongoReturn(): return { \"code\": 2, \"msg\": \"数据库操作错误\" } # 错误判断 def catch_exception(origin_func): def wrapper(self, *args, **kwargs): from flask import current_app from sqlalchemy.exc import ( SQLAlchemyError, NoSuchColumnError, NoSuchModuleError, NoForeignKeysError, NoReferencedColumnError, DisconnectionError ) try: u = origin_func(self, *args, **kwargs) return u except AttributeError as e: current_app.logger.error(e) return falseReturn(str(e)) except ( SQLAlchemyError, NoSuchColumnError, NoSuchModuleError, NoForeignKeysError, NoReferencedColumnError, DisconnectionError ) as e: current_app.logger.error(e) return falseReturn(str(e)) except TypeError as e: current_app.logger.error(e) return falseReturn(str(e)) except Exception as e: current_app.logger.error(e) return falseReturn(str(e)) return wrapper 定义了catch_exception方法，在使用的时候只需在接口添加@catch_exception 例如 class GetAllCategory(Resource): \"\"\"获取所有分类\"\"\" @catch_exception def get(self): result = \"\" return trueReturn(result) 这么写也使代码更加的pythonic， 否则你会看到很多的try,except容错判断 需求七： 解决剩余的bug，全局响应头、跨域 app.py from flask_cors import CORS def create_app(config_name): ... # 全局响应头 @app.after_request def after_request(response): if \"Access-Control-Allow-Origin\" not in response.headers.keys(): response.headers.add('Access-Control-Allow-Origin', '*') if request.method == 'OPTIONS': response.headers['Access-Control-Allow-Methods'] = 'DELETE, GET, POST, PUT' headers = request.headers.get('Access-Control-Request-Headers') if headers: response.headers['Access-Control-Allow-Headers'] = headers return response # 跨域 CORS(app, supports_credentials=True, resources={r\"/api/*\": {\"origins\": \"*\"}}) ... Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"后端/flask整理/Flask+Vue快速打造个人网站（二）.html":{"url":"后端/flask整理/Flask+Vue快速打造个人网站（二）.html","title":"Flask+Vue快速打造个人网站（二）","keywords":"","body":"Flask+Vue快速打造个人网站（二） 2018.9.17 23:09 后端 后端框架使用flask考虑的是前后端分离，可以快速开发API，还有就是以前写的一些代码直接复用 在接口这块其实都差不多，主要来讲项目的模块化划分 目录 模块化项目是为了使代码更加清晰、可复用、低耦合，与django不同的是，前期使用flask时在github撸了很多demo，发现大部分项目结构都不同，各有各的分法。 # 模块化想法 models数据模型、routes全局路由、config全局配置、 一个启动文件、一个app初始化文件、utils外部方法包 这块有个坑就是互相引用的问题，尤其是在models里 A文件importB B文件又import A,就会引起这个问题， 最好的做法是init文件作为中间文件放置公共方法 模块化划分好之后,就是一些外部库的用法 Flask-admin flask-migrate Flask-BabelEx Flask-Cors Flask-login Flask-restful Flask-script Flask-sqlalchemy blueprint 需求一：原始接口都是一个方法上面一个路由，@app.route()这样，多个api多个页面时全局查看路由就很不方便，需要把路由都放在一块与接口分离开来 routes.py路由页面 from flask import Blueprint from flask_restful import Api from admins import Index # 当需要flask_restful写接口时，Blueprint应该怎么引入就成了问题 # 实例化蓝图,路由前缀为/api blue = Blueprint('api', __name__, url_prefix='/api') docs = Api(blue) # 路由注册 docs.add_resource(Index, '/', endpoint=\"index\") # 添加路由时只需 docs.add_resource(API类名, '路由地址', 端点) api页面 from flask_restful import Resource # restfulAPI继承Resource类 class Index(Resource): def __init__(self): # ... def get(self): #... def post(self): # ... models页面 from admins import db class Video(db.Model): __tablename__ = \"video\" id = db.Column(db.Integer, unique=True, primary_key=True) video_url = db.Column(db.String(100)) desc = db.Column(db.String(80)) time_long = db.Column(db.Integer) small_img = db.Column(db.String(100)) watch = db.Column(db.Integer, default=0) def __repr__(self): return \"\" % self.id 这样路由查看就方便多了 需求二：models迁移数据，修改model字段可以随时更新迁移 用到了flask-migrate \"\"\" 系统入口文件， 实例化app 添加shell脚本 \"\"\" from flask_script import Manager, Server from flask_migrate import Migrate, MigrateCommand from app import create_app # 为了避免重复引用，我将db放在了admins.py中 from admins import db, AdminUser # 将所有的model都引进来 from pictures.models import * app = create_app('dev') migrate = Migrate(app, db) manager = Manager(app) manager.add_command(\"runserver\", Server(host='0.0.0.0', port=5000, )) manager.add_command(\"db\", MigrateCommand) if __name__ == '__main__': manager.run() 在admins目录下的__init__.py文件 from flask_sqlalchemy import SQLAlchemy db = SQLAlchemy() Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"后端/flask整理/flask应用.html":{"url":"后端/flask整理/flask应用.html","title":"flask应用","keywords":"","body":"flask hello flask mkdir flask-pro cd flask-pro mkdir hello-flask cd hello-flask/ virtualenv venv --python=python3 source venv/bin/activate pip install flask mkdir app touch app/__init__.py touch app/routers.py touch hello.py app # app/__init__.py from flask import Flask app = Flask(__name__) from app import routers Routers.py from app import app @app.route('/') @app.route('/hello') def index(): return 'HELLO FLASK' export FLASK_APP=hello.py flask run 访问localhost:5000 访问html 新建Hello.html # app/templates/hello.html {{ title }} Hello,{{ user.name }}! Routers.py from app import app from flask import render_template @app.route('/') @app.route('/hello') def hi(): user = {'name':'HOUSE','age':18} return render_template('hello.html',title='hello',user=user) 访问localhost:5000/ debug on 默认flask的debug模式是关闭的 开启方法一：app下新建config文件 DEBUG = true 应用内加上 app.config.from_object('config') 开启方法二：pycharm设置勾选flask_debug 单页面应用 新建项目 hello-flask 下面创建目录templates # hello-flask/app.py from flask import Flask,request # 注册app app = Flask(__name__) # 路由+视图 @app.route('/login',methods=['GET','POST']) def login(): name = 'xiaoming' pwd = 'xiaoming' msg = None if request.method == 'POST': form = request.form username = form.get('username') password = form.get('password') if username == name and password == pwd: return '登陆成功' msg = '用户名或密码错误' from flask import render_template return render_template('login.html',msg=msg) if __name__ == '__main__': app.run() templates下建login.html 前端form表单 #templates/login.html Document {% if msg %} {{msg}} {% endif %} 登陆 访问localhost:5000/login 失败返回 成功返回 flask名词理解 # 蓝图 bluepoint 类似于django中的根路由 # 端点 endpoint 类似于django url中的name 可以用于重定向和模板反向解析 # url_for django中的url Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"后端/flask整理/flask进阶.html":{"url":"后端/flask整理/flask进阶.html","title":"flask进阶","keywords":"","body":"在 Flask 中有两种上下文:程序上下文和请求上下文 请求： before_first_request:注册一个函数，在处理第一个请求之前运行。 before_request:注册一个函数，在每次请求之前运行。 after_request:注册一个函数，如果没有未处理的异常抛出，在每次请求之后运行。 teardown_request:注册一个函数，即使有未处理的异常抛出，也在每次请求之后运行 响应： response.set_cookie('answer', '42')： 设置cookie 密钥: ​ SECRET_KEY: 通用密钥 通常存储于环境变量 Flask扩展 Flask-Script 命令行 Flask-Moment 本地化日期时间 Flask-WTF 处理表单 防止跨站请求伪造 Flask-Migrate 数据库迁移 Flask-Mail 电子邮件支持 Flask-Login 管理已登陆用户的用户会话 Werkzeug 计算密码散列值并进行核对 itsdangerous 生成并核对加密安全令牌 coverage 代码覆盖 数据库 SQL 数据库擅于用高效且紧凑的形式存储结构化数据。这种数据库需要花费大量精力保证 数据的一致性。NoSQL 数据库放宽了对这种一致性的要求，从而获得性能上的优势。 mongo连接判断 from pymongo import MongoClient from pymongo.errors import ConnectionFailure # 原本数据库连接超时是在10s以上，为了防止超时影响开发效率 # 使用serverSelectionTimeoutMS将时间限定为3s # 3次连接不上，错误写入日志，连接失败 def connect(uri): count = 0 while True: client = MongoClient(uri, serverSelectionTimeoutMS=3) try: client.admin.command(\"ping\") except ConnectionFailure as e: log.error(e) count = count + 1 else: break if count == 3: return False return True 大型应用： config.py：程序配置 定义Config基类 配置类可以定义init_app()类方法 Blueprint：蓝图中创建应用程序 在蓝本中就不一样了，Flask 会为蓝本中的全部端点加上一个命名空间，这样就可以在不 同的蓝本中使用相同的端点名定义视图函数，而不会产生冲突。命名空间就是蓝本的名字 (Blueprint 构造函数的第一个参数)，所以视图函数 index() 注册的端点名是 main.index， 其 URL 使用 url_for('main.index') 获取。 Werkzeug： 密码散列 将密码做为值传入 得出散列值存入数据库 generate_password_hash(password, method=pbkdf2:sha1, salt_length=8) 这个函数的参数是从数据库中取回的密码散列 值和用户输入的密码。返回值为 True 表明密码正确 check_password_hash(hash, password) 应用编程接口 最近几年，Web 程序有种趋势，那就是业务逻辑被越来越多地移到了客户端一侧，开创出 了一种称为富互联网应用(Rich Internet Application，RIA)的架构。在RIA中，服务器的 主要功能(有时是唯一功能)是为客户端提供数据存取服务。在这种模式中，服务器变成 了 Web 服务或应用编程接口 RESTFUL C-S 客户端与服务端之间 无状态：客户端发出的请求中必须包含所有必要的信息。服务器不能在两次请求之间保存客户端的任何状态 缓存：为了优化页面响应，可以允许缓存 接口统一：客户端访问服务器资源时使用的协议必须一致，定义良好，且已经标准化。REST Web 服务最常使用的统一接口是 HTTP 协议 系统分层：在客户端和服务器之间可以按需插入代理服务器、缓存或网关，以提高性能、稳定性和 伸缩性。 按需代码：客户端可以选择从服务器上下载代码，在客户端的环境中执行 Flask 会特殊对待末端带有斜线的路由。如果客户端请求的 URL 的末 端没有斜线，而唯一匹配的路由末端有斜线，Flask 会自动响应一个重定向， 转向末端带斜线的 URL。反之则不会重定向。 Post接收参数 参数； { \"name\":\"adasds@163.com\", \"pwd\":\"dasdasdas\" } 接收 request.get_json() 附 flask-restful中级用法 统一修改某个传入字段输出类型 需求： mongo传入id 输出ObjectId类型 传入时间戳 输出datetime类型 输出字段 Flask-RESTful 提供了一个简单的方式来控制在你的响应中实际呈现什么数据。使用 fields 模块，你可以使用在你的资源里的任意对象（ORM 模型、定制的类等等）并且 fields 让你格式化和过滤响应，因此您不必担心暴露内部数据结构。 当查询你的代码的时候，哪些数据会被呈现以及它们如何被格式化是很清楚的。 基本用法 你可以定义一个字典或者 fields 的 OrderedDict 类型，OrderedDict 类型是指键名是要呈现的对象的属性或键的名称，键值是一个类，该类格式化和返回的该字段的值。这个例子有三个字段，两个是字符串（Strings）以及一个是日期时间（DateTime），格式为 RFC 822 日期字符串（同样也支持 ISO 8601） from flask.ext.restful import Resource, fields, marshal_with resource_fields = { 'name': fields.String, 'address': fields.String, 'date_updated': fields.DateTime(dt_format='rfc822'), } class Todo(Resource): @marshal_with(resource_fields, envelope='resource') def get(self, **kwargs): return db_get_todo() # Some function that queries the db 重命名属性 很多时候你面向公众的字段名称是不同于内部的属性名。使用 attribute 可以配置这种映射。 fields = { 'name': fields.String(attribute='private_name'), 'address': fields.String, } lambda 也能在 attribute 中使用 fields = { 'name': fields.String(attribute=lambda x: x._private_name), 'address': fields.String, } Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"后端/flask整理/iSurvey.html":{"url":"后端/flask整理/iSurvey.html","title":"I Survey","keywords":"","body":"iSurvey 问卷调查项目 models设计 User 用户表 tablename = users ''' 某个用户下所有的调查表 建立一个1对n的关系 surveys代表当前用户天蝎的所有问卷调查 surveys = db.relationship('Survey',backref='creator') user.surveys可以获得当前用户所有的问卷调查 ''' Survey 调查表 tablename = surveys ''' questions = db.relationship('Question',backref='survey') # survey.questions 可以获得某个调查表中的所有question creator_id = ...ForeignKey('users.id') survey.creator可以获得当前问卷调查对应的user name #调查表名字 ''' Question 问题表 tablename = questions ''' survey_id = .... ForeignKey('surveys.id') # 标识该问题所属的调查表 content created_at choise # 问题选项 ''' Choise 选项表 _tablename = choises ''' id content is_selected #是否被选中 create # 创建时间 # 标示该选项所属的问题 question_id = db.Column(db.Integer, db.ForeignKey('questions.id')) ''' Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"后端/flask整理/python脚本实现Mp4在Safari播放.html":{"url":"后端/flask整理/python脚本实现Mp4在Safari播放.html","title":"python脚本实现Mp4在Safari播放","keywords":"","body":"MP4视频在Safari播放 前言：在用Flask框架写上传文件接口时，图片可以正常上传显示，但视频无法在Safari中播放，IOS手机也不支持,最终解决了这个问题 来看看问题的关键 简而言之就是，http请求报文信息中，请求头有['range']，它的格式有下面几种(假设总字节为10000)： bytes=0- 0开始，请求全部 bytes=1000-2999 请求字节从1000开始到2999 bytes=8000-9999 请求字节从8000开始到9999 bytes=-500 请求最后500字节 按照上面的介绍，也就是说你的响应头中的['Content-Range']字段，必须对应它的range做出不同响应 bytes 0-9999/10000 0开始，请求全部 bytes 1000-2999/10000 请求字节从1000开始到2999 bytes 8000-9999/10000 请求字节从8000开始到9999 bytes 9499-9999/10000 请求最后500字节 最恶心的是,苹果为了节省用户流量，会先发一个字节的请求，bytes=0-1并判断你是否有Content-range响应。这个请求辉获取到你的视频类型，总长度，成功之后才会发起第二个请求，响应状态为206，通常是断点续传，否则就不再发起请求。所以我们在用ios打开视频的时候，会闪一下出现进度条，然后是图像破裂的样式，显示播放失败 解决 问题就在于获取视频长度、视频类型和针对不同range返回content-range class ServerFile(Resource): \"\"\" 查看作品视频 \"\"\" def get(self,id): # 请求的视频名是md5+.mp4后缀拼接 md = file_set.find_one({'md5': id.split('.')[0]}) if md is None: raise bson.errors.InvalidId() resp = flask.Response(md['content'], mimetype=ALLOWED_EXTENSIONS[md['mime']]) resp.headers['Last-Modified'] = md['time'].ctime() ctype = '*' # 判断请求头是否包含range if request.headers.get(\"Range\"): range_value = request.headers[\"Range\"] # 取出start与end HTTP_RANGE_HEADER = re.compile(r'bytes=([0-9]+)\\-(([0-9]+)?)') m = re.match(HTTP_RANGE_HEADER, range_value) if m: start_str = m.group(1) start = int(start_str) end_str = m.group(2) end = -1 # end存在 if len(end_str) > 0: end = int(end_str) # range存在时，让请求支持断点续传,status_code改为206 resp.status_code = 206 # 图省事这里把content-type写为* resp.headers[\"Content-Type\"] = ctype if end == -1: # 此处的md['size']是文件大小 resp.headers[\"Content-Length\"] = str(md['size'] - start) else: # Content-Length也要改变 resp.headers[\"Content-Length\"] = str(end - start + 1) resp.headers[\"Accept-Ranges\"] = \"bytes\" if end 最终，视频正常播放 特别感谢 Python3 http服务器脚本，支持range请求头部 Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"后端/flask整理/uwsgi部署flask项目.html":{"url":"后端/flask整理/uwsgi部署flask项目.html","title":"uwsgi部署flask项目","keywords":"","body":"uwsgi部署flask项目 nginx 用来做反向代理服务器：通过接收 Internet 上的连接请求，将请求转发给内网中的目标服务器，再将从目标服务器得到的结果返回给 Internet 上请求连接的客户端（比如浏览器）； uwsgi 是一个高效的 Python WSGI Server，我们通常用它来运行 WSGI (Web Server Gateway Interface，Web 服务器网关接口) 应用（比如本项目的 Flask 应用）； supervisor 是一个进程管理工具，可以很方便地启动、关闭和重启进程等； 准备工作: /home下创建目录myproject，在myproject下用ftp或git将项目放入 /home/myproject/flask_blog 进入项目,创建虚拟环境 virtualenv flask_env # 若要指定python版本 virtualenv flask_env --python=python3.6 配置uwsgi 项目根目录下，创建.ini文件. 如/flask_blog/config.ini [uwsgi] # uwsgi 启动时所使用的地址与端口 socket = 127.0.0.1:8001 # 指向网站目录 chdir = /home/myproject/ # python 启动程序文件 wsgi-file = manage.py # python 程序内用以启动的 application 变量名 callable = app # 处理器数 processes = 4 # 线程数 threads = 2 #状态检测地址 stats = 127.0.0.1:9191 【注】：callable=app 这个 app 是 manage.py 程序文件内的一个变量，这个变量的类型是 Flask的 application 类 运行: uwsgi config.ini 配置supervisor 安装 apt-get install supervisor 添加一个新的.conf文件放在*/etc/supervisor/conf.d/下 那么我们就新建立一个用于启动 flask_blog 项目的 uwsgi 的 supervisor 配置 (命名为：my_flask_supervisor.conf)： [program:my_flask] # 启动命令入口 command=/home/myproject/flask_blog/venv/bin/uwsgi /home/myproject/flask_blog/config.ini # 命令程序所在目录 directory=/home/myproject/flask_blog #运行命令的用户名 user=root autostart=true autorestart=true #日志地址(没有就新建一个) stdout_logfile=/home/myproject/flask_blog/logs/uwsgi_supervisor.log 启动服务 service supervisor start 配置Nginx 备份/ext/nginx/sites-available/default中的default文件 然后修改default server { listen 80; server_name XXX.XXX.XXX; #公网地址 location / { include uwsgi_params; uwsgi_pass 127.0.0.1:8001; # 指向uwsgi 所应用的内部地址,所有请求将转发给uwsgi 处理 uwsgi_param UWSGI_PYHOME /home/myproject/flask_blog/venv; # 指向虚拟环境目录 uwsgi_param UWSGI_CHDIR /home/myproject/flask_blog; # 指向网站根目录 uwsgi_param UWSGI_SCRIPT manage:app; # 指定启动程序 } } 启动 sudo service nginx restart 后续维护 修改项目文件后需重启nginx sudo service nginx restart Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"后端/flask整理/上传文件接口.html":{"url":"后端/flask整理/上传文件接口.html","title":"上传文件接口","keywords":"","body":"Flask 图片上传返回地址接口 import hashlib import datetime import flask import pymongo import bson.binary import bson.objectid import bson.errors from io import BytesIO from PIL import Image app = flask.Flask(__name__) app.debug = True # 连接mongo数据库 db = pymongo.MongoClient('localhost', 27017).test # 文件格式过滤 allow_formats = set(['jpeg', 'png', 'gif', 'mp4']) def save_file(f): \"\"\"保存文件并返回哈希处理的值\"\"\" content = BytesIO(f.read()) try: mime = Image.open(content).format.lower() if mime not in allow_formats: raise IOError() except IOError: flask.abort(400) sha1 = hashlib.sha1(content.getvalue()).hexdigest() c = dict( content=bson.binary.Binary(content.getvalue()), mime=mime, time=datetime.datetime.utcnow(), sha1=sha1, ) try: db.files.save(c) except pymongo.errors.DuplicateKeyError: pass return sha1 @app.route('/f/') def serve_file(sha1): \"\"\"api接口返回图片地址\"\"\" try: f = db.files.find_one({'sha1': sha1}) if f is None: raise bson.errors.InvalidId() if flask.request.headers.get('If-Modified-Since') == f['time'].ctime(): return flask.Response(status=304) resp = flask.Response(f['content'], mimetype='image/' + f['mime']) resp.headers['Last-Modified'] = f['time'].ctime() return resp except bson.errors.InvalidId: flask.abort(404) @app.route('/upload', methods=['POST']) def upload(): \"\"\"上传文件\"\"\" f = flask.request.files['uploaded_file'] sha1 = save_file(f) return flask.redirect('/f/' + str(sha1)) @app.route('/') def index(): return ''' ''' if __name__ == '__main__': app.run() Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"后端/flask整理/常见认证机制.html":{"url":"后端/flask整理/常见认证机制.html","title":"常见认证机制","keywords":"","body":"常见认证机制 HTTP Basic Auth HTTP Basic Auth 在HTTP中，基本认证是一种用来允许Web浏览器或其他客户端程序在请求时提供用户名和口令形式的身份凭证的一种登录验证方式，通常用户名和明码会通过HTTP头传递。 在发送之前是以用户名追加一个冒号然后串接上口令，并将得出的结果字符串再用Base64算法编码。例如，提供的用户名是Aladdin、口令是open sesame，则拼接后的结果就是Aladdin:open sesame，然后再将其用Base64编码，得到QWxhZGRpbjpvcGVuIHNlc2FtZQ==。最终将Base64编码的字符串发送出去，由接收者解码得到一个由冒号分隔的用户名和口令的字符串。 优点 基本认证的一个优点是基本上所有流行的网页浏览器都支持基本认证。 缺点 由于用户名和密码都是Base64编码的，而Base64编码是可逆的，所以用户名和密码可以认为是明文。所以只有在客户端和服务器主机之间的连接是安全可信的前提下才可以使用。 接下来我们看一个更加安全也适用范围更大的认证方式 OAuth。 OAuth OAuth 是一个关于授权（authorization）的开放网络标准。允许用户提供一个令牌，而不是用户名和密码来访问他们存放在特定服务提供者的数据。现在的版本是2.0版。 严格来说，OAuth2不是一个标准协议，而是一个安全的授权框架。它详细描述了系统中不同角色、用户、服务前端应用（比如API），以及客户端（比如网站或移动App）之间怎么实现相互认证。 名词定义 Third-party application: 第三方应用程序，又称\"客户端\"（client） HTTP service：HTTP服务提供商 Resource Owner：资源所有者，通常称\"用户\"（user）。 User Agent：用户代理，比如浏览器。 Authorization server：认证服务器，即服务提供商专门用来处理认证的服务器。 Resource server：资源服务器，即服务提供商存放用户生成的资源的服务器。它与认证服务器，可以是同一台服务器，也可以是不同的服务器。 OAuth 2.0 运行流程如图： （A）用户打开客户端以后，客户端要求用户给予授权。 （B）用户同意给予客户端授权。 （C）客户端使用上一步获得的授权，向认证服务器申请令牌。 （D）认证服务器对客户端进行认证以后，确认无误，同意发放令牌。 （E）客户端使用令牌，向资源服务器申请获取资源。 （F）资源服务器确认令牌无误，同意向客户端开放资源 优点 快速开发 实施代码量小 维护工作减少 如果设计的API要被不同的App使用，并且每个App使用的方式也不一样，使用OAuth2是个不错的选择。 缺点 OAuth2是一个安全框架，描述了在各种不同场景下，多个应用之间的授权问题。有海量的资料需要学习，要完全理解需要花费大量时间。OAuth2不是一个严格的标准协议，因此在实施过程中更容易出错。 了解了以上两种方式后，现在终于到了本篇的重点，JWT 认证。 JWT 认证 Json web token (JWT), 根据官网的定义，是为了在网络应用环境间传递声明而执行的一种基于JSON的开放标准（(RFC 7519).该token被设计为紧凑且安全的，特别适用于分布式站点的单点登录（SSO）场景。JWT的声明一般被用来在身份提供者和服务提供者间传递被认证的用户身份信息，以便于从资源服务器获取资源，也可以增加一些额外的其它业务逻辑所必须的声明信息，该token也可直接被用于认证，也可被加密。 JWT 特点 体积小，因而传输速度快 传输方式多样，可以通过URL/POST参数/HTTP头部等方式传输 严格的结构化。它自身（在 payload 中）就包含了所有与用户相关的验证消息，如用户可访问路由、访问有效期等信息，服务器无需再去连接数据库验证信息的有效性，并且 payload 支持为你的应用而定制化。 支持跨域验证，可以应用于单点登录。 JWT原理 JWT是Auth0提出的通过对JSON进行加密签名来实现授权验证的方案，编码之后的JWT看起来是这样的一串字符： eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiYWRtaW4iOnRydWV9.TJVA95OrM7E2cBab30RMHrHDcEfxjoYZgeFONFh7HgQ 由 . 分为三段，通过解码可以得到： 1. 头部（Header） // 包括类别（typ）、加密算法（alg）； { \"alg\": \"HS256\", \"typ\": \"JWT\" } jwt的头部包含两部分信息： 声明类型，这里是jwt 声明加密的算法 通常直接使用 HMAC SHA256 然后将头部进行base64加密（该加密是可以对称解密的)，构成了第一部分。 eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9 2. 载荷（payload） 载荷就是存放有效信息的地方。这些有效信息包含三个部分： 标准中注册声明 公共的声名 私有的声明 公共的声明 ： 公共的声明可以添加任何的信息，一般添加用户的相关信息或其他业务需要的必要信息.但不建议添加敏感信息，因为该部分在客户端可解密。 私有的声明 ： 私有声明是提供者和消费者所共同定义的声明，一般不建议存放敏感信息，因为base64是对称解密的，意味着该部分信息可以归类为明文信息。 下面是一个例子： // 包括需要传递的用户信息； { \"iss\": \"Online JWT Builder\", \"iat\": 1416797419, \"exp\": 1448333419, \"aud\": \"www.gusibi.com\", \"sub\": \"uid\", \"nickname\": \"goodspeed\", \"username\": \"goodspeed\", \"scopes\": [ \"admin\", \"user\" ] } iss: 该JWT的签发者，是否使用是可选的； sub: 该JWT所面向的用户，是否使用是可选的； aud: 接收该JWT的一方，是否使用是可选的； exp(expires): 什么时候过期，这里是一个Unix时间戳，是否使用是可选的； iat(issued at): 在什么时候签发的(UNIX时间)，是否使用是可选的； 其他还有： nbf (Not Before)：如果当前时间在nbf里的时间之前，则Token不被接受；一般都会留一些余地，比如几分钟；，是否使用是可选的； jti: jwt的唯一身份标识，主要用来作为一次性token，从而回避重放攻击。 将上面的JSON对象进行base64编码可以得到下面的字符串。这个字符串我们将它称作JWT的Payload（载荷）。 eyJpc3MiOiJPbmxpbmUgSldUIEJ1aWxkZXIiLCJpYXQiOjE0MTY3OTc0MTksImV4cCI6MTQ0ODMzMzQxOSwiYXVkIjoid3d3Lmd1c2liaS5jb20iLCJzdWIiOiIwMTIzNDU2Nzg5Iiwibmlja25hbWUiOiJnb29kc3BlZWQiLCJ1c2VybmFtZSI6Imdvb2RzcGVlZCIsInNjb3BlcyI6WyJhZG1pbiIsInVzZXIiXX0 信息会暴露：由于这里用的是可逆的base64 编码，所以第二部分的数据实际上是明文的。我们应该避免在这里存放不能公开的隐私信息。 3. 签名（signature） // 根据alg算法与私有秘钥进行加密得到的签名字串； // 这一段是最重要的敏感信息，只能在服务端解密； HMACSHA256( base64UrlEncode(header) + \".\" + base64UrlEncode(payload), SECREATE_KEY ) jwt的第三部分是一个签证信息，这个签证信息由三部分组成： header (base64后的) payload (base64后的) secret 将上面的两个编码后的字符串都用句号.连接在一起（头部在前），就形成了: eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJKb2huIFd1IEpXVCIsImlhdCI6MTQ0MTU5MzUwMiwiZXhwIjoxNDQxNTk0NzIyLCJhdWQiOiJ3d3cuZXhhbXBsZS5jb20iLCJzdWIiOiJqcm9ja2V0QGV4YW1wbGUuY29tIiwiZnJvbV91c2VyIjoiQiIsInRhcmdldF91c2VyIjoiQSJ9 最后，我们将上面拼接完的字符串用HS256算法进行加密。在加密的时候，我们还需要提供一个密钥（secret）。如果我们用 secret 作为密钥的话，那么就可以得到我们加密后的内容: pq5IDv-yaktw6XEa5GEv07SzS9ehe6AcVSdTj0Ini4o 将这三部分用.连接成一个完整的字符串,构成了最终的jwt: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJPbmxpbmUgSldUIEJ1aWxkZXIiLCJpYXQiOjE0MTY3OTc0MTksImV4cCI6MTQ0ODMzMzQxOSwiYXVkIjoid3d3Lmd1c2liaS5jb20iLCJzdWIiOiIwMTIzNDU2Nzg5Iiwibmlja25hbWUiOiJnb29kc3BlZWQiLCJ1c2VybmFtZSI6Imdvb2RzcGVlZCIsInNjb3BlcyI6WyJhZG1pbiIsInVzZXIiXX0.pq5IDv-yaktw6XEa5GEv07SzS9ehe6AcVSdTj0Ini4o 签名的目的：签名实际上是对头部以及载荷内容进行签名。所以，如果有人对头部以及载荷的内容解码之后进行修改，再进行编码的话，那么新的头部和载荷的签名和之前的签名就将是不一样的。而且，如果不知道服务器加密的时候用的密钥的话，得出来的签名也一定会是不一样的。 这样就能保证token不会被篡改。 token 生成好之后，接下来就可以用token来和服务器进行通讯了。 下图是client 使用 JWT 与server 交互过程: 这里在第三步我们得到 JWT 之后，需要将JWT存放在 client，之后的每次需要认证的请求都要把JWT发送过来。（请求时可以放到 header 的 Authorization ） JWT 使用场景 JWT的主要优势在于使用无状态、可扩展的方式处理应用中的用户会话。服务端可以通过内嵌的声明信息，很容易地获取用户的会话信息，而不需要去访问用户或会话的数据库。在一个分布式的面向服务的框架中，这一点非常有用。 但是，如果系统中需要使用黑名单实现长期有效的token刷新机制，这种无状态的优势就不明显了。 优点 快速开发 不需要cookie JSON在移动端的广泛应用 不依赖于社交登录 相对简单的概念理解 缺点 Token有长度限制 Token不能撤销 需要token有失效时间限制(exp) python 使用JWT实践 我基本是使用 python 作为服务端语言，我们可以使用 pyjwt：https://github.com/jpadilla/pyjwt/ 使用比较方便，下边是我在应用中使用的例子： import jwt import time # 使用 sanic 作为restful api 框架 def create_token(request): grant_type = request.json.get('grant_type') username = request.json['username'] password = request.json['password'] if grant_type == 'password': account = verify_password(username, password) elif grant_type == 'wxapp': account = verify_wxapp(username, password) if not account: return {} payload = { \"iss\": \"gusibi.com\", \"iat\": int(time.time()), \"exp\": int(time.time()) + 86400 * 7, \"aud\": \"www.gusibi.com\", \"sub\": account['_id'], \"username\": account['username'], \"scopes\": ['open'] } token = jwt.encode(payload, 'secret', algorithm='HS256') return True, {'access_token': token, 'account_id': account['_id']} def verify_bearer_token(token): # 如果在生成token的时候使用了aud参数，那么校验的时候也需要添加此参数 payload = jwt.decode(token, 'secret', audience='www.gusibi.com', algorithms=['HS256']) if payload: return True, token return False, token 这里，我们可以使用 jwt 直接生成 token，不用手动base64加密和拼接。 详细代码可以参考 gusibi/Metis: 一个测试类小程序（包含前后端代码）。 这个项目中，api 使用 python sanic，文档使用 swagger-py-codegen 生成，提供 swagger ui。 现在可以使用 swagger ui 来测试jwt。 Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"后端/Celery.html":{"url":"后端/Celery.html","title":"Celery","keywords":"","body":"Celery 什么是celery 如何使用 什么是celery Celery 是一个异步任务队列。你可以使用它在你的应用上下文之外执行任务。总的想法就是你的应用程序可能需要执行任何消耗资源的任务都可以交给任务队列，让你的应用程序自由和快速地响应客户端请求。 通过Celery在后台跑任务并不像用线程那么的简单，但是用Celery的话，能够使应用有较好的可扩展性，因为Celery是个分布式架构。下面介绍Celery的三个核心组件。 生产者(Celery client)。生产者(Celery client)发送消息。在Flask上工作时，生产者(Celery client)在Flask应用内运行。 消费者(Celery workers)。消费者用于处理后台任务。消费者(Celery client)可以是本地的也可以是远程的。我们可以在运行Flask的server上运行一个单一的消费者(Celery workers)，当业务量上涨之后再去添加更多消费者(Celery workers)。 消息传递者(message broker)。生产者(Celery client)和消费者(Celery workers)的信息的交互使用的是消息队列(message queue)。Celery支持若干方式的消息队列，其中最常用的是RabbitMQ和Redis. Flask中使用Celery Flask与Celery结合极其简单，无需其他扩展。一个使用Celery的Flask应用的初始化过程如下：通过创建Celery类的对象，完成Celery的初始化。创建Celery对象时，需要传递应用的名称以及消息传递者(message broker)的URL。 教程入口： celery基本介绍 # connect('ultrabear_courses',username='jamie',password='jamie199469',host='127.0.0.1',port=27676,serverSelectionTimeoutMS=3) Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"后端/token机制.html":{"url":"后端/token机制.html","title":"token机制","keywords":"","body":"token机制 考虑相对安全的登录流程 Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"后端/web安全防范.html":{"url":"后端/web安全防范.html","title":"web安全防范","keywords":"","body":"web安全 安全永远是产品的最基础需求 这里总结几种常见攻击与防范 一.XSS XSS，跨站脚本攻击，原理是恶意攻击者往 Web 页面里插入恶意可执行网页脚本代码，当用户浏览该页之时，嵌入其中 Web 里面的脚本代码会被执行，从而可以达到攻击者盗取用户信息或其他侵犯用户安全隐私的目的。 1. 非持久性XSS 非持久型 XSS 漏洞，也叫反射型 XSS 漏洞，一般是通过给别人发送带有恶意脚本代码参数的 URL，当 URL 地址被打开时，特有的恶意代码参数被 HTML 解析、执行。 比如你的页面包含以下代码 document.write('' + '' + location.href.substring(location.href.indexOf('default=') + 8) + '' ); document.write('English'); 攻击者可以直接通过 URL (类似：https://xx.com/xx?default=alert(document.cookie)%3C/script%3E)) 注入可执行的脚本代码。 特征: 非持久型 XSS 漏洞攻击有以下几点特征： 即时性，不经过服务器存储，直接通过 HTTP 的 GET 和 POST 请求就能完成一次攻击，拿到用户隐私数据。 攻击者需要诱骗点击 反馈率低，所以较难发现和响应修复 盗取用户敏感保密信息 防范 Web 页面渲染的所有内容或者渲染的数据都必须来自于服务端。 尽量不要从 URL，document.referrer，document.forms 等这种 DOM API 中获取数据直接渲染。 尽量不要使用 eval, new Function()，document.write()，document.writeln()，window.setInterval()，window.setTimeout()，innerHTML，document.creteElement() 等可执行字符串的方法。 如果做不到以上几点，也必须对涉及 DOM 渲染的方法传入的字符串参数做 escape 转义。 前端渲染的时候对任何的字段都需要做 escape 转义编码。 escape 转义的目的是将一些构成 HTML 标签的元素转义，比如 ，>，空格 等，转义成 &lt;，&gt;，&nbsp; 等显示转义字符。有很多开源的工具可以协助我们做 escape 转义。 2. 持久型XSS 持久型 XSS 漏洞，也被称为存储型 XSS 漏洞，一般存在于 Form 表单提交等交互功能，如发帖留言，提交文本信息等，黑客利用的 XSS 漏洞，将内容经正常功能提交进入数据库持久保存，当前端页面获得后端从数据库中读出的注入代码时，恰好将其渲染执行。 主要注入页面方式和非持久型 XSS 漏洞类似，只不过持久型的不是来源于 URL，refferer，forms 等，而是来源于后端从数据库中读出来的数据。持久型 XSS 攻击不需要诱骗点击，黑客只需要在提交表单的地方完成注入即可，但是这种 XSS 攻击的成本相对还是很高。攻击成功需要同时满足以下几个条件： POST 请求提交表单后端没做转义直接入库。 后端从数据库中取出数据没做转义直接输出给前端。 前端拿到后端数据没做转义直接渲染成 DOM。 特征 持久型 XSS 有以下几个特点： 持久性，植入在数据库中 危害面广，甚至可以让用户机器变成 DDoS 攻击的肉鸡。 盗取用户敏感私密信息 防范 后端在入库前应该选择不相信任何前端数据，将所有的字段统一进行转义处理。 后端在输出给前端数据统一进行转义处理。 前端在渲染页面 DOM 的时候应该选择不相信任何后端数据，任何字段都需要做转义处理。 3. 基于字符集的XSS 其实现在很多的浏览器以及各种开源的库都专门针对了 XSS 进行转义处理，尽量默认抵御绝大多数 XSS 攻击，但是还是有很多方式可以绕过转义规则，让人防不胜防。比如「基于字符集的 XSS 攻击」就是绕过这些转义处理的一种攻击方式，比如有些 Web 页面字符集不固定，用户输入非期望字符集的字符，有时会绕过转义过滤规则。 形成原因 由于浏览器在 meta 没有指定 charset 的时候有自动识别编码的机制，所以这类攻击通常就是发生在没有指定或者没来得及指定 meta 标签的 charset 的情况下 防范 记住指定 XML 中不仅要指定字符集为 utf-8，而且标签要闭合 牛文推荐：http://drops.wooyun.org/papers/1327 （这个讲的很详细） 4. 基于Flash的跨站XSS 形成原因 基于 Flash 的跨站 XSS 也是属于反射型 XSS 的一种，虽然现在开发 ActionScript 的产品线几乎没有了，但还是提一句吧，AS 脚本可以接受用户输入并操作 cookie，攻击者可以配合其他 XSS（持久型或者非持久型）方法将恶意 swf 文件嵌入页面中。主要是因为 AS 有时候需要和 JS 传参交互，攻击者会通过恶意的 XSS 注入篡改参数，窃取并操作cookie。 防范 严格管理 cookie 的读写权限 对 Flash 能接受用户输入的参数进行过滤 escape 转义处理 5.未经验证的跳转XSS 形成原因 有一些场景是后端需要对一个传进来的待跳转的 URL 参数进行一个 302 跳转，可能其中会带有一些用户的敏感（cookie）信息。如果服务器端做302 跳转，跳转的地址来自用户的输入，攻击者可以输入一个恶意的跳转地址来执行脚本。 防范 对待跳转的 URL 参数做白名单或者某种规则过滤 后端注意对敏感信息的保护, 比如 cookie 使用来源验证。 二.CSRF CSRF，跨站请求伪造攻击 攻击者可以盗用你的登陆信息，以你的身份模拟发送各种请求。攻击者只要借助少许的社会工程学的诡计，例如通过 QQ 等聊天软件发送的链接(有些还伪装成短域名，用户无法分辨)，攻击者就能迫使 Web 应用的用户去执行攻击者预设的操作。 例如 当用户登录网络银行去查看其存款余额，在他没有退出时，就点击了一个 QQ 好友发来的链接，那么该用户银行帐户中的资金就有可能被转移到攻击者指定的帐户中。 所以遇到 CSRF 攻击时，将对终端用户的数据和操作指令构成严重的威胁。当受攻击的终端用户具有管理员帐户的时候，CSRF 攻击将危及整个 Web 应用程序。 下图大概描述了 CSRF 攻击的原理，可以理解为有一个小偷在你配钥匙的地方得到了你家的钥匙，然后拿着要是去你家想偷什么偷什么。 完成 CSRF 攻击必须要有三个条件： 用户已经登录了站点 A，并在本地记录了 cookie 在用户没有登出站点 A 的情况下（也就是 cookie 生效的情况下），访问了恶意攻击者提供的引诱危险站点 B (B 站点要求访问站点A)。 站点 A 没有做任何 CSRF 防御 你也许会问：「如果我不满足以上三个条件中的任意一个，就不会受到 CSRF 的攻击」。其实可以这么说的，但你不能保证以下情况不会发生： 你不能保证你登录了一个网站后，不再打开一个 tab 页面并访问另外的网站，特别现在浏览器都是支持多 tab 的。 你不能保证你关闭浏览器了后，你本地的 cookie 立刻过期，你上次的会话已经结束。 上图中所谓的攻击网站 B，可能是一个存在其他漏洞的可信任的经常被人访问的网站。 预防CSRF CSRF 的防御可以从服务端和客户端两方面着手，防御效果是从服务端着手效果比较好，现在一般的 CSRF 防御也都在服务端进行。服务端的预防 CSRF 攻击的方式方法有多种，但思路上都是差不多的，主要从以下两个方面入手： 正确使用 GET，POST 请求和 cookie 在非 GET 请求中增加 token 一般而言，普通的 Web 应用都是以 GET、POST 请求为主，还有一种请求是 cookie 方式。我们一般都是按照如下规则设计应用的请求： GET 请求常用在查看，列举，展示等不需要改变资源属性的时候（数据库 query 查询的时候） POST 请求常用在 From 表单提交，改变一个资源的属性或者做其他一些事情的时候（数据库有 insert、update、delete 的时候） 当正确的使用了 GET 和 POST 请求之后，剩下的就是在非 GET 方式的请求中增加随机数，这个大概有三种方式来进行： 为每个用户生成一个唯一的 cookie token，所有表单都包含同一个伪随机值，这种方案最简单，因为攻击者不能获得第三方的 cookie(理论上)，所以表单中的数据也就构造失败，但是由于用户的 cookie 很容易由于网站的 XSS 漏洞而被盗取，所以这个方案必须要在没有 XSS 的情况下才安全。 每个 POST 请求使用验证码，这个方案算是比较完美的，但是需要用户多次输入验证码，用户体验比较差，所以不适合在业务中大量运用。 渲染表单的时候，为每一个表单包含一个 csrfToken，提交表单的时候，带上 csrfToken，然后在后端做 csrfToken 验证。 CSRF 的防御可以根据应用场景的不同自行选择。CSRF 的防御工作确实会在正常业务逻辑的基础上带来很多额外的开发量，但是这种工作量是值得的。 三.SQL注入 SQL 注入漏洞（SQL Injection）是 Web 开发中最常见的一种安全漏洞。可以用它来从数据库获取敏感信息，或者利用数据库的特性执行添加用户，导出文件等一系列恶意操作，甚至有可能获取数据库乃至系统用户最高权限。 而造成 SQL 注入的原因是因为程序没有有效的转义过滤用户的输入，使攻击者成功的向服务器提交恶意的 SQL 查询代码，程序在接收后错误的将攻击者的输入作为查询语句的一部分执行，导致原始的查询逻辑被改变，额外的执行了攻击者精心构造的恶意代码。 很多 Web 开发者没有意识到 SQL 查询是可以被篡改的，从而把 SQL 查询当作可信任的命令。殊不知，SQL 查询是可以绕开访问控制，从而绕过身份验证和权限检查的。更有甚者，有可能通过 SQL 查询去运行主机系统级的命令。 下面将通过一些真实的例子来详细讲解 SQL 注入的方式的原理。 考虑以下简单的管理员登录表单： Username: Password: 后端的 SQL 语句可能是如下这样的： let querySQL = ` SELECT * FROM user WHERE username='${username}' AND psw='${password}' `; // 接下来就是执行 sql 语句... 目的就是来验证用户名和密码是不是正确，按理说乍一看上面的 SQL 语句也没什么毛病，确实是能够达到我们的目的，可是你只是站在用户会老老实实按照你的设计来输入的角度来看问题，如果有一个恶意攻击者输入的用户名是 zoumiaojiang' OR 1 = 1 --，密码随意输入，就可以直接登入系统了。WFT! 冷静下来思考一下，我们之前预想的真实 SQL 语句是: SELECT * FROM user WHERE username='zoumiaojiang' AND psw='mypassword' 可以恶意攻击者的奇怪用户名将你的 SQL 语句变成了如下形式： SELECT * FROM user WHERE username='zoumiaojiang' OR 1 = 1 --' AND psw='xxxx' 在 SQL 中，-- 是注释后面的内容的意思，所以查询语句就变成了： SELECT * FROM user WHERE username='zoumiaojiang' OR 1 = 1 这条 SQL 语句的查询条件永远为真，所以意思就是恶意攻击者不用我的密码，就可以登录进我的账号，然后可以在里面为所欲为，然而这还只是最简单的注入，牛逼的 SQL 注入高手甚至可以通过 SQL 查询去运行主机系统级的命令，将你主机里的内容一览无余。 防范 防止 SQL 注入主要是不能允许用户输入的内容影响正常的 SQL 语句的逻辑，当用户的输入的信息将要用来拼接 SQL 语句的话，我们应该永远选择不相信，任何内容都必须进行转义过滤，当然做到这个还是不够的，下面列出防御 SQL 注入的几点注意事项： 严格限制Web应用的数据库的操作权限，给此用户提供仅仅能够满足其工作的最低权限，从而最大限度的减少注入攻击对数据库的危害 后端代码检查输入的数据是否符合预期，严格限制变量的类型，例如使用正则表达式进行一些匹配处理。 对进入数据库的特殊字符（'，\"，\\，，&，*，; 等）进行转义处理，或编码转换。 所有的查询语句建议使用数据库提供的参数化查询接口，参数化的语句使用参数而不是将用户输入变量嵌入到 SQL 语句中，即不要直接拼接 SQL 语句。 在应用发布之前建议使用专业的 SQL 注入检测工具进行检测，以及时修补被发现的 SQL 注入漏洞。网上有很多这方面的开源工具，例如 sqlmap、SQLninja 等。 避免网站打印出 SQL 错误信息，比如类型错误、字段不匹配等，把代码里的 SQL 语句暴露出来，以防止攻击者利用这些错误信息进行 SQL 注入。 不要过于细化返回的错误信息，如果目的是方便调试，就去使用后端日志，不要在接口上过多的暴露出错信息，毕竟真正的用户不关心太多的技术细节，只要话术合理就行。 四.命令行注入 命令行注入漏洞，指的是攻击者能够通过 HTTP 请求直接侵入主机，执行攻击者预设的 shell 命令。 假如现在需要实现一个需求：用户提交一些内容到服务器，然后在服务器执行一些系统命令去产出一个结果返回给用户，接口的部分实现如下： // 以 Node.js 为例，假如在接口中需要从 github 下载用户指定的 repo const exec = require('mz/child_process').exec; let params = {/* 用户输入的参数 */}; exec(`git clone ${params.repo} /some/path`); 这段代码确实能够满足业务需求，正常的用户也确实能从指定的 git repo 上下载到想要的代码，可是和 SQL 注入一样，这段代码在恶意攻击者眼中，简直就是香饽饽。 如果 params.repo 传入的是 https://github.com/zoumiaojiang/zoumiaojiang.github.io.git当然没问题了。 可是如果params.repo 传入的是https://github.com/xx/xx.git && rm -rf /* &&恰好你的服务是用 root 权限起的就惨了。 防范 后端对前端提交内容需要完全选择不相信，并且对其进行规则限制（比如正则表达式）。 在调用系统命令前对所有传入参数进行命令行参数转义过滤。 不要直接拼接命令语句，借助一些工具做拼接、转义预处理，例如 Node.js 的 shell-escapenpm 包。 还是前面的例子，我们可以做到如下： const exec = require('mz/child_process').exec; // 借助 shell-escape npm 包解决参数转义过滤问题 const shellescape = require('shell-escape'); let params = {/* 用户输入的参数 */}; // 先过滤一下参数，让参数符合预期 if (!/正确的表达式/.test(params.repo)) { return; } let cmd = shellescape([ 'git', 'clone', params.repo, '/some/path' ]); // cmd 的值: git clone 'https://github.com/xx/xx.git && rm -rf / &&' /some/path // 这样就不会被注入成功了。 exec(cmd); 无论是在何种后端语言环境中，凡是涉及到代码调用系统 shell 命令的时候都一定要谨慎。 五.DDos攻击 利用大量的请求造成资源过载，导致服务不可用， 这个攻击应该不能算是安全问题，这应该算是一个另类的存在，因为这种攻击根本就是耍流氓的存在，「伤敌一千，自损八百」的行为。出于保护 Web App 不受攻击的攻防角度，还是介绍一下 DDoS 攻击吧，毕竟也是挺常见的。 DDoS 攻击可以理解为：「你开了一家店，隔壁家点看不惯，就雇了一大堆黑社会人员进你店里干坐着，也不消费，其他客人也进不来，导致你营业惨淡」。为啥说 DDoS 是个「伤敌一千，自损八百」的行为呢？毕竟隔壁店还是花了不少钱雇黑社会但是啥也没得到不是？DDoS 攻击的目的基本上就以下几个： 深仇大恨，就是要干死你 敲诈你，不给钱就干你 忽悠你，不买我防火墙服务就会有“人”继续干你 DDos 攻击从层次上可分为网络层攻击与应用层攻击，从攻击手法上可分为快型流量攻击与慢型流量攻击，但其原理都是造成资源过载，导致服务不可用。 网络层DDoS 网络层的 DDoS 攻击究其本质其实是无法防御的，我们能做得就是不断优化服务本身部署的网络架构，以及提升网络带宽。当然，还是做好以下几件事也是有助于缓解网络层 DDoS 攻击的冲击： 网络架构上做好优化，采用负载均衡分流。 确保服务器的系统文件是最新的版本，并及时更新系统补丁。 添加抗 DDos 设备，进行流量清洗。 限制同时打开的 SYN 半连接数目，缩短 SYN 半连接的 Timeout 时间。 限制单 IP 请求频率。 防火墙等防护设置禁止 ICMP 包等。 严格限制对外开放的服务器的向外访问。 运行端口映射程序或端口扫描程序，要认真检查特权端口和非特权端口。 关闭不必要的服务。 认真检查网络设备和主机/服务器系统的日志。只要日志出现漏洞或是时间变更,那这台机器就可能遭到了攻击。 限制在防火墙外与网络文件共享。这样会给黑客截取系统文件的机会，主机的信息暴露给黑客，无疑是给了对方入侵的机会。 加钱堆机器。。 报警。。 应用层DDoS 应用层 DDoS 攻击不是发生在网络层，是发生在 TCP 建立握手成功之后，应用程序处理请求的时候，现在很多常见的 DDoS 攻击都是应用层攻击。应用层攻击千变万化，目的就是在网络应用层耗尽你的带宽，下面列出集中典型的攻击类型。 CC 攻击 当时绿盟为了防御 DDoS 攻击研发了一款叫做 Collapasar 的产品，能够有效的防御 SYN Flood 攻击。黑客为了挑衅，研发了一款 Challenge Collapasar 攻击工具（简称 CC）。 CC 攻击的原理，就是针对消耗资源比较大的页面不断发起不正常的请求，导致资源耗尽。因此在发送 CC 攻击前，我们需要寻找加载比较慢，消耗资源比较多的网页，比如需要查询数据库的页面、读写硬盘文件的等。通过 CC 攻击，使用爬虫对某些加载需要消耗大量资源的页面发起 HTTP 请求。 DNS Flood DNS Flood 攻击采用的方法是向被攻击的服务器发送大量的域名解析请求，通常请求解析的域名是随机生成或者是网络世界上根本不存在的域名，被攻击的DNS 服务器在接收到域名解析请求的时候首先会在服务器上查找是否有对应的缓存，如果查找不到并且该域名无法直接由服务器解析的时候，DNS 服务器会向其上层 DNS 服务器递归查询域名信息。域名解析的过程给服务器带来了很大的负载，每秒钟域名解析请求超过一定的数量就会造成 DNS 服务器解析域名超时。 根据微软的统计数据，一台 DNS 服务器所能承受的动态域名查询的上限是每秒钟 9000 个请求。而我们知道，在一台 P3 的 PC 机上可以轻易地构造出每秒钟几万个域名解析请求，足以使一台硬件配置极高的 DNS 服务器瘫痪，由此可见 DNS 服务器的脆弱性。 HTTP 慢速连接攻击 针对 HTTP 协议，先建立起 HTTP 连接，设置一个较大的 Conetnt-Length，每次只发送很少的字节，让服务器一直以为 HTTP 头部没有传输完成，这样连接一多就很快会出现连接耗尽。 防范 判断 User-Agent 字段（不可靠，因为可以随意构造） 针对 IP + cookie，限制访问频率（由于 cookie 可以更改，IP 可以使用代理，或者肉鸡，也不可靠) 关闭服务器最大连接数等，合理配置中间件，缓解 DDoS 攻击。 请求中添加验证码，比如请求中有数据库操作的时候。 编写代码时，尽量实现优化，并合理使用缓存技术，减少数据库的读取操作。 应用层的防御有时比网络层的更难，因为导致应用层被 DDoS 攻击的因素非常多，有时往往是因为程序员的失误，导致某个页面加载需要消耗大量资源，有时是因为中间件配置不当等等。而应用层 DDoS 防御的核心就是区分人与机器（爬虫），因为大量的请求不可能是人为的，肯定是机器构造的。因此如果能有效的区分人与爬虫行为，则可以很好地防御此攻击。 其他DDoS攻击 利用 XSS 举个例子，如果 12306 页面有一个 XSS 持久型漏洞被恶意攻击者发现，只需在春节抢票期间在这个漏洞中执行脚本使得往某一个小站点随便发点什么请求，然后随着用户访问的增多，感染用户增多，被攻击的站点自然就会迅速瘫痪了。这种 DDoS 简直就是无本万利，不用惊讶，现在大站有 XSS 漏洞的不要太多。 来自 P2P 网络攻击 大家都知道，互联网上的 P2P 用户和流量都是一个极为庞大的数字。如果他们都去一个指定的地方下载数据，成千上万的真实 IP 地址连接过来，没有哪个设备能够支撑住。拿 BT 下载来说，伪造一些热门视频的种子，发布到搜索引擎，就足以骗到许多用户和流量了，但是这只是基础攻击。 高级的 P2P 攻击，是直接欺骗资源管理服务器。如迅雷客户端会把自己发现的资源上传到资源管理服务器，然后推送给其它需要下载相同资源的用户，这样，一个链接就发布出去。通过协议逆向，攻击者伪造出大批量的热门资源信息通过资源管理中心分发出去，瞬间就可以传遍整个 P2P 网络。更为恐怖的是，这种攻击是无法停止的，即使是攻击者自身也无法停止，攻击一直持续到 P2P 官方发现问题更新服务器且下载用户重启下载软件为止。 最后总结下，DDoS 不可能防的住，就好比你的店只能容纳 50 人，黑社会有 100 人，你就换一家大店，能容纳 500 人，然后黑社会又找来了 1000 人，这种堆人头的做法就是 DDoS 本质上的攻防之道，「道高一尺，魔高一丈，魔高一尺，道高一丈」，讲真，必要的时候就答应勒索你的人的条件吧，实在不行就报警吧。 六.流量劫持 流量劫持基本分两种：DNS 劫持 和 HTTP 劫持，目的都是一样的，就是当用户访问 zoumiaojiang.com 的时候，给你展示的并不是或者不完全是 zoumiaojiang.com 提供的 “内容”。 DNS劫持 DNS 劫持，也叫做域名劫持，可以这么理解，「你打了一辆车想去商场吃饭，结果你打的车是小作坊派来的，直接给你拉到小作坊去了」，DNS 的作用是把网络地址域名对应到真实的计算机能够识别的 IP 地址，以便计算机能够进一步通信，传递网址和内容等。如果当用户通过某一个域名访问一个站点的时候，被篡改的 DNS 服务器返回的是一个恶意的钓鱼站点的 IP，用户就被劫持到了恶意钓鱼站点，然后继而会被钓鱼输入各种账号密码信息，泄漏隐私。 这类劫持，要不就是网络运营商搞的鬼，一般小的网络运营商与黑产勾结会劫持 DNS，要不就是 电脑中毒，被恶意篡改了路由器的 DNS 配置，基本上做为开发者或站长却是很难察觉的，除非有用户反馈，现在升级版的 DNS 劫持还可以对特定用户、特定区域等使用了用户画像进行筛选用户劫持的办法，另外这类广告显示更加随机更小，一般站长除非用户投诉否则很难觉察到，就算觉察到了取证举报更难。无论如何，如果接到有 DNS 劫持的反馈，一定要做好以下几件事： 取证很重要，时间、地点、IP、拨号账户、截屏、URL 地址等一定要有。 可以跟劫持区域的电信运营商进行投诉反馈。 如果投诉反馈无效，直接去工信部投诉，一般来说会加白你的域名。 HTTP劫持 HTTP 劫持您可以这么理解，「你打了一辆车想去商场吃饭，结果司机跟你一路给你递小作坊的广告」，HTTP 劫持主要是当用户访问某个站点的时候会经过运营商网络，而不法运营商和黑产勾结能够截获 HTTP 请求返回内容，并且能够篡改内容，然后再返回给用户，从而实现劫持页面，轻则插入小广告，重则直接篡改成钓鱼网站页面骗用户隐私。能够实施流量劫持的根本原因，是 HTTP 协议没有办法对通信对方的身份进行校验以及对数据完整性进行校验。如果能解决这个问题，则流量劫持将无法轻易发生。所以防止 HTTP 劫持的方法只有将内容加密，让劫持者无法破解篡改，这样就可以防止 HTTP 劫持了。 HTTPS 协议就是一种基于 SSL 协议的安全加密网络应用层协议，可以很好的防止 HTTP 劫持。这里有篇 文章 讲的不错。HTTPS 在这就不深讲了，后面有机会我会单独好好讲讲 HTTPS。如果不想站点被 HTTP 劫持，赶紧将你的站点全站改造成 HTTPS 吧。 七.服务器漏洞 1. 越权操作漏洞 如果你的系统是有登录控制的，那就要格外小心了，因为很有可能你的系统越权操作漏洞，越权操作漏洞可以简单的总结为 「A 用户能看到或者操作 B 用户的隐私内容」，如果你的系统中还有权限控制就更加需要小心了。所以每一个请求都需要做 userid 的判断 以下是一段有漏洞的后端示意代码： // ctx 为请求的 context 上下文 let msgId = ctx.params.msgId; mysql.query( 'SELECT * FROM msg_table WHERE msg_id = ?', [msgId] ); 以上代码是任何人都可以查询到任何用户的消息，只要有 msg_id 就可以，这就是比较典型的越权漏洞，需要如下这么改进一下： // ctx 为请求的 context 上下文 let msgId = ctx.params.msgId; let userId = ctx.session.userId; // 从会话中取出当前登陆的 userId mysql.query( 'SELECT * FROM msg_table WHERE msg_id = ? AND user_id = ?', [msgId, userId] ); 嗯，大概就是这个意思，如果有更严格的权限控制，那在每个请求中凡是涉及到数据库的操作都需要先进行严格的验证，并且在设计数据库表的时候需要考虑进 userId 的账号关联以及权限关联。 2.目录遍历漏洞 目录遍历漏洞指通过在 URL 或参数中构造 ../，./ 和类似的跨父目录字符串的 ASCII 编码、unicode 编码等，完成目录跳转，读取操作系统各个目录下的敏感文件，也可以称作「任意文件读取漏洞」。 目录遍历漏洞原理：程序没有充分过滤用户输入的 ../ 之类的目录跳转符，导致用户可以通过提交目录跳转来遍历服务器上的任意文件。使用多个.. 符号，不断向上跳转，最终停留在根 /，通过绝对路径去读取任意文件。 目录遍历漏洞几个示例和测试，一般构造 URL 然后使用浏览器直接访问，或者使用 Web 漏洞扫描工具检测，当然也可以自写程序测试。 http://somehost.com/../../../../../../../../../etc/passwd http://somehost.com/some/path?file=../../Windows/system.ini # 借助 %00 空字符截断是一个比较经典的攻击手法 http://somehost.com/some/path?file=../../Windows/system.ini%00.js # 使用了 IIS 的脚本目录来移动目录并执行指令 http://somehost.com/scripts/..%5c../Windows/System32/cmd.exe?/c+dir+c:\\ 防御 方法就是需要对 URL 或者参数进行 ../，./ 等字符的转义过滤。 3.物理路径泄漏 物理路径泄露属于低风险等级缺陷，它的危害一般被描述为「攻击者可以利用此漏洞得到信息，来对系统进一步地攻击」，通常都是系统报错 500 的错误信息直接返回到页面可见导致的漏洞。得到物理路径有些时候它能给攻击者带来一些有用的信息，比如说：可以大致了解系统的文件目录结构；可以看出系统所使用的第三方软件；也说不定会得到一个合法的用户名（因为很多人把自己的用户名作为网站的目录名）。 防止这种泄漏的方法就是做好后端程序的出错处理，定制特殊的 500 报错页面。 4.源码暴露漏洞 和物理路径泄露类似，就是攻击者可以通过请求直接获取到你站点的后端源代码，然后就可以对系统进一步研究攻击。那么导致源代码暴露的原因是什么呢？基本上就是发生在服务器配置上了，服务器可以设置哪些路径的文件才可以被直接访问的，这里给一个 koa 服务起的例子，正常的 koa 服务器可以通过 koa-static 中间件去指定静态资源的目录，好让静态资源可以通过路径的路由访问。比如你的系统源代码目录是这样的： |- project |- src |- static |- ... |- server.js 你想要将 static 的文件夹配成静态资源目录，你应该会在 server.js 做如下配置： const Koa = require('koa'); const serve = require('koa-static'); const app = new Koa(); app.use(serve(__dirname + '/project/static')); 但是如果配错了静态资源的目录，可能就出大事了，比如： // ... app.use(serve(__dirname + '/project')); 这样所有的源代码都可以通过路由访问到了，所有的服务器都提供了静态资源机制，所以在通过服务器配置静态资源目录和路径的时候，一定要注意检验，不然很可能产生漏洞。 最后，希望 Web 开发者们能够管理好自己的代码隐私，注意代码安全问题，比如不要将产品的含有敏感信息的代码放到第三方外部站点或者暴露给外部用户，尤其是前端代码，私钥类似的保密性的东西不要直接输出在代码里或者页面中。也许还有很多值得注意的点，但是归根结底还是绷住安全那根弦，对待每一行代码都要多多推敲。 Django的安全机制 xss保护 django的模板会保护你免受大部分xss攻击， 但不是完全安全的。例如对下面的就不保护： ... 也就是说模版变量中写入操作性代码时就要格外小心 当用到is_safe和自定义标签一起时（using is_safe with custom template tags），格外小心十分重要。 另外，如果你用模板输出非HTML的内容，完全分开的字符和单词需要转义。存储HTML到数据库时你同样应该小心，尤其是HTML被加载并展示的。 csrf保护 当用HTTPS部署时，CsrfViewMiddleware将会检查HTTP referer头被设定到同源域的URL（包括子域和端口）。因为HTTPS提供额外的安全，所以在转发不安全连接请求和可以用支持HSTS的浏览器的地方确保连接用HTTPS是很有必要的。 sql注入保护 用django的queryset，以数据库驱动为基础，合成的(resulting)sql会被合适地转义。然而django同样给开发者权利写原始的查询或自定义的sql。这些能力应该被保守地使用。你应该一直小心去适当转义任何 用户可以控制的参数。另外，当用extra()时你更应该小心谨慎 点击劫持保护 django在X-Frame-Options 中间件的表单里包含有点击劫持保护功能，这种功能在支持的浏览器里可以保护站点不在frame里被渲染。可以按view的基础禁用这个保护功能，或配置发送明确的header。 强烈推荐任何一个不必要或是只允许小部分 使页面包装在第三方frame的网站 使用这个中间件。 ssl/https 尽量使用https而非http 如果一定要用http 需要做到下面几点 设置SECURE_PROXY_SSL_HEADER, 确保你彻底理解了警告。若不这样做会导致CSRF漏洞，若没有正确实施同样会导致危险 建立跳转，基于HTTP的连接会跳转到HTTPS。 用安全的cookies 若浏览器用默认的HTTP连接，cookie会被泄露。所以需要你的 SESSION_COOKIE_SECURE和CSRF_COOKIE_SECURE 设置为True。这会让浏览器只通过HTTPS传送cookies。注意着意味着session在HTTP下不会起作用，CSRF保护机制会阻止一切通过HTTP传送的POST数据。 妥善设置allowed_hosts参数 Django会在HttpRequest.get_host()方法中确认host 头是否与ALLOWED_HOSTS设置的值冲突。 ​ 确认只会由get_host()方法应用。若你的代码通过request.META直接访问host头，你会绕过这个安全防护。 校验安全列表 运行 manage.py check –deploy，确保每一项配置都正确。该操作会检查你可能遗漏的安全设置。 总结 只要做开发，就一定会涉及到web安全问题 不管使用什么开发框架，都需要提前先去理解器内部的安全机制。然后针对这些机制去合理的使用框架， 如果框架内部或者扩展库没有与web攻击相对应的扩展的话，也有必要自己封装。 在django中，一些中间件，settings里的参数设置True或False，不要因为急于功能开发而去注释或删除他们， 你需要先去了解他们的作用，然后再去决定是否需要注释 flask框架中， 也有很多web安全拓展库 比如Flask-Security 总之一句话 把web安全放在第一位 Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"后端/关于主流web框架选择问题.html":{"url":"后端/关于主流web框架选择问题.html","title":"关于主流web框架选择问题","keywords":"","body":"1.使用框架 根据业务来改变 在软件领域，是没有万能框架的，根据业务需求来改变 这也就需要各位不断学习 我们公司用的是django框架。 2.为什么选择这个框架 ​ 这个问题换个说法就是比较各个python框架的优缺点 ​ django框架大而全，提高了开发效率 ​ 并且有很多用于web安全的中间件，结构清晰，部署简单 3.python的后台服务最大能支持多大的pv量会严重影响用户体验性能 一般不会有这个性能问题，像youtube、豆瓣、知乎、拉勾这些大流量后台都是python写的，所以说如果有性能问题，也不是python的，是所有语言都会遇到的问题,大量的pv是可以靠服务器堆起来的。 如果是计算量较大的任务,可以考虑用c/c++写 4.如何兼顾网页前端于移动端开发的后台 目前前后端都是分离的，所以前端只需要掌握一种框架,angular/react/vue，再调用后端给的api接口,完全可以兼顾 5.如何使你的网站反爬虫 对数据加密，编码 判断访问者的useragent、过滤python scrapy等明显的机器 对一个时间段访问不正常的ip封掉 6.python对于其他语言在爬虫方面的优势 Php:多进程和多线程支持不好 java:代码臃肿，代码量大、重构成本高 python:语言简洁，学习成本低、支持俄模块多，有强大的爬虫框架 7.python内存管理 堆、栈、代码、数据 8.网站排名 1.根据pagerank值排名 2.百度竞价 9.常见端口 http:80 Https:443 Mysql:3306 mongoDB:27017 Redis:6379 ftp:21 Ssh:22 Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"后端/前后端分离项目之python后端部署前的操作.html":{"url":"后端/前后端分离项目之python后端部署前的操作.html","title":"前后端分离项目之python后端部署前的操作","keywords":"","body":"前后端分离项目之python后端部署前的操作 前端：VUE+webpack打包+vue-router路由 前端项目结构 前端项目操作目录 前端打包 npm run build 生成打包后目录dist文件夹 直接点进去index.html是空白页,打包后给后端 后端目录 然后正常部署就可以了 Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"后端/后台框架选型.html":{"url":"后端/后台框架选型.html","title":"后台框架选型","keywords":"","body":"后台框架选型 Flask与Django对比 数据库支持 管理系统性能 第三方库支持 代码可读性、是否轻量、可维护性、可扩展性 版本迭代难易程度 与前台交互 目录结构 django项目目录结构 应用结构 flask项目目录 API实现 Django: View.py Serializer.py Url.py flask: 特性\\框架 Django Flask 数据库支持 内置ORM,对关系型数据库支持优于Flask,但非关系型数据库支持略差 对SQL和NoSQL都支持 性能 json序列化速度和flask不相上下,Http响应请求速度也差不多,数据库与模版处理速度上Flask要快很多 第三方库 django自带扩展库，自带admin后台，django-restframework可视化api Flask-scrpit（命令行启动服务）、Flask-Migration（数据库迁移）、Flask-WTF（表单）、Flask-Session 可读性 django项目本身是模块化的，可读性很高 取决于程序员自身 与前台交互 提供接口与必填字段 路由管理 根路由及各应用子路由 flask-restful路由统一管理 版本迭代 修改根路由各应用url 修改蓝图根路由 测试 django自带testCase+client unittest+client Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"后端/后端记录.html":{"url":"后端/后端记录.html","title":"后端记录","keywords":"","body":"目前部署流程 开发结束->push gitee->服务器pull->supervisorctl-> restart api docker映射流程 创建Dockerfile->build-> # 交互模式 本地项目挂载 docker run -it -v 本地目录:挂载目录 镜像 /bin/bash # cp拷贝文件【如果不拷贝、退出后镜像内不保存项目】 -> exit退出交互模式->commit 镜像 -> push P++ Ping++ 没有针对扫码提供客户端 SDK ，所以请不要将服务端拿到的 charge 传给 Ping++ 的 Client-SDK ，会出现报错 no_such_channel 支付完成后，第三方渠道不会给你的客户端任何结果，所以你需要设计客户端主动轮询服务端查询扫码的支付结果。 在 Charge 对象中有 credential 字段，该字段中包含可以生成二维码的 alipay_qr 或 wx_pub_qr 链接。你需要截取出 alipay_qr 或 wx_pub_qr 的链接并自行生成二维码，显示在你的 PC 端或任意你需要展示二维码的平台 若你的服务器未正确返回 2xx，Ping++ 服务器会在 25 小时内向你的服务器不断重发通知，最多 10 次。Webhooks 首次是即时推送，重试通知时间间隔为 5s、10s、2min、5min、10min、30min、1h、2h、6h、15h，直到你正确回复状态 2xx 或者超过最大重发次数，Ping++ 将不再发送 接收到 Webhooks 说明交易成功，交易失败不会发送 Webhooks Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"后端/多人协作项目git免密登录.html":{"url":"后端/多人协作项目git免密登录.html","title":"多人协作项目git免密登录","keywords":"","body":"多人协作项目git免密登录 多人协作时一个服务器免不了放很多项目，在不能git config --global全局设置git用户或者密钥免密时， 可以在你负责的项目目录下执行操作来实现免密登录 在项目根目录下创建.git-credentials文件 https://username:password@git端项目目录(可以直接git clone的那个地址) :wq退出 当前目录下执行git config credential.helper store 执行git pull命令测试是否免密(设置首次可能需要输入密码) ⚠️注意: 保存好之后在.gitignore文件中将.git-credentials文件名添加进去，避免上传到公开项目 Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"工具/gitbook入门.html":{"url":"工具/gitbook入门.html","title":"gitbook入门","keywords":"","body":"gitbook入门 本文从 “是什么”、“为什么”、“怎么办”、“好不好” 四个维度来介绍 GitBook，带你从黑暗之中走出来，get 这种美妙的写作方式。 作者：阿基米东 来源：CSDN 原文：https://blog.csdn.net/lu_embedded/article/details/81100704 是什么？ 　　在我认识 GitBook 之前，我已经在使用 Git 了，毋容置疑，Git 是目前世界上最先进的分布式版本控制系统。 　　我认为 Git 不仅是程序员管理代码的工具，它的分布式协作方式同样适用于很多场合，其中一个就是写作（这会是一个引起社会变革的伟大的工具！）。所以在我发现 GitBook 之前，实际上我已经无数次想象过它的使用场景了。 　　咋一看 GitBook 的名字，你可能会认为它是关于 Git 的一本书。而当你有所了解之后，你也许会认为它是一个使用 Git 构建电子书的工具。其实不然，GitBook 与 Git 的关系，就像雷锋塔和雷锋那样，没有一点关系！ 　　实际上，GitBook 是一个基于 Node.js 的命令行工具，支持 Markdown 和 AsciiDoc 两种语法格式，可以输出 HTML、PDF、eBook 等格式的电子书。所以我更喜欢把 GitBook 定义为文档格式转换工具。 　　 　　所以，GitBook 不是 Markdown 编辑工具，也不是 Git 版本管理工具。市面上我们可以找到很多 Markdown 编辑器，比如 Typora、MacDown、Bear、MarkdownPad、MarkdownX、JetBrains’s IDE（需要安装插件）、Atom、简书、CSDN 以及 GitBook 自家的 GitBook Editor 等等。 　　但 GitBook 又与 Markdown 和 Git 息息相关，因为只有将它们结合起来使用，才能将它们的威力发挥到极致！因此，通常我们会选择合适的 Markdown 编辑工具以获得飞一般的写作体验；使用 GitBook 管理文档，预览、制作电子书；同时通过 Git 管理书籍内容的变更，并将其托管到云端（比如 GitHub、GitLab、码云，或者是自己搭建的 Git 服务器），实现多人协作。 　　实际上，GitBook Editor 对于新手来说是个不错的选择，它集成了 GitBook、Git、Markdown 等功能，还支持将书籍同步到 gitbook.com 网站，使我们可以很方便地编辑和管理书籍。但是不幸的是，GitBook Editor 的注册和登录需要翻墙，即便注册成功了也可能登录不上，似乎是因为网站最近在升级。 　　因此，我推荐，也是我目前使用的搭配是 GitBook + Typora + Git。 为什么？ 　　通常，我们最开始学习和使用的办公软件就是 Word、Excel 和 PowerPoint。这里不是说它们已经过时了，不是说 GitBook 能够替代它们。 　　相反，Microsoft 的办公软件很优秀并且经受了时间的考验，但是正因为它功能丰富，导致稍显臃肿（二八定律：80%的时间里我们只会只用20%的功能），同时因为它存在以二进制格式保存、软件不兼容、格式不兼容、难以进行版本控制、难以实时分享预览、难以多人协作等短板。而这恰恰是 GitBook + Markdown + Git 的长处。 　　简单来说，GitBook + Markdown + Git 带来的好处有： 语法简单 兼容性强 导出方便 专注内容 团队协作 　　所以，如果你和我一样，不满足于传统的写作方式，正在寻找一种令人愉悦的写作方式，那么该尝试使用 GitBook 啦！ 　　当然，GitBook 不是万能的，当我们需要复杂排版时，依然需要依托于 Word 等工具。但不用担心，因为我们可以把 Markdown 格式的文档导出为 Word 格式，再进一步加工。 怎么办？ 怎么安装 　　当你听了我的怂恿，并决定尝试使用 GitBook 的时候，首先面临的问题是 —— 如何搭建 GitBook 环境？ 　　因为 GitBook 是基于 Node.js，所以我们首先需要安装 Node.js（下载地址：https://nodejs.org/en/download/），找到对应平台的版本安装即可。 　　现在安装 Node.js 都会默认安装 npm（node 包管理工具），所以我们不用单独安装 npm，打开命令行，执行以下命令安装 GitBook： npm install -g gitbook-cli 　　安装完之后，就会多了一个 gitbook 命令（如果没有，请确认上面的命令是否加了 -g）。 　　上面我推荐的是 GitBook + Typora + Git，所以你还需要安装 Typora（一个很棒的支持 macOS、Windows、Linux 的 Markdown 编辑工具）和 Git 版本管理工具。戳下面： Typora 下载地址：https://typora.io/ Git 下载地址：https://git-scm.com/downloads ​ Typora 的安装很简单，难点在于需要翻墙才能下载（当然你也可以找我要）。Git 的安装也很简单，但要用好它需要不少时间，这里就不展开了（再讲下去你就要跑啦）。 怎么使用 　　想象一下，现在你准备构建一本书籍，你在硬盘上新建了一个叫 mybook 的文件夹，按照以前的做法，你会新建一个 Word 文档，写上标题，然后开始巴滋巴滋地笔耕。但是现在有了 GitBook，你首先要做的是在 mybook 文件夹下执行以下命令： $ gitbook init 　　执行完后，你会看到多了两个文件 —— README.md 和 SUMMARY.md，它们的作用如下： README.md —— 书籍的介绍写在这个文件里 SUMMARY.md —— 书籍的目录结构在这里配置　　： ​ 这时候，我们启动恭候多时的 Typora 来编辑这两个文件了： 编辑 SUMMARY.md 文件，内容修改为 # 目录 * [前言](README.md) * [第一章](Chapter1/README.md) * [第1节：衣](Chapter1/衣.md) * [第2节：食](Chapter1/食.md) * [第3节：住](Chapter1/住.md) * [第4节：行](Chapter1/行.md) * [第二章](Chapter2/README.md) * [第三章](Chapter3/README.md) * [第四章](Chapter4/README.md) 　　然后我们回到命令行，在 mybook 文件夹中再次执行 gitbook init 命令。GitBook 会查找 SUMMARY.md 文件中描述的目录和文件，如果没有则会将其创建。 　　Typora 是所见即所得（实时渲染）的 Markdown 编辑器，这时候它是这样的： 　　 ​ 接着我们执行 gitbook serve 来预览这本书籍，执行命令后会对 Markdown 格式的文档进行转换，默认转换为 html 格式，最后提示 “Serving book on http://localhost:4000”。嗯，打开浏览器看一下吧： 　　当你写得差不多，你可以执行 gitbook build 命令构建书籍，默认将生成的静态网站输出到 _book 目录。实际上，这一步也包含在 gitbook serve 里面，因为它们是 HTML，所以 GitBook 通过 Node.js 给你提供服务了。 　　当然，build 命令可以指定路径： $ gitbook build 书籍路径 　　serve 命令也可以指定端口： $ gitbook serve --port 2333 　　你还可以生成 PDF 格式的电子书： $ gitbook pdf ./ ./mybook.pdf 　　生成 epub 格式的电子书： $ gitbook epub ./ ./mybook.epub 　　生成 mobi 格式的电子书： $ gitbook mobi ./ ./mybook.mobi 　　如果生成不了，你可能还需要安装一些工具，比如 ebook-convert。或者在 Typora 中安装 Pandoc 进行导出。 　　除此之外，别忘了还可以用 Git 做版本管理呀！在 mybook 目录下执行 git init 初始化仓库，执行 git remote add 添加远程仓库（你得先在远端建好）。接着就可以愉快地 commit，push，pull … 啦！ 好不好？ 　　额…… 你觉得好不好嘛？ 　　反正我觉得挺好的，特别是对我这种懒得排版，又想随时随地写作的宝宝来说。而且能够查看每个版本内容变更的情况，同时又避免了硬盘单一故障带来的风险。 (全剧终) Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"工具/markdown.html":{"url":"工具/markdown.html","title":"Markdown","keywords":"","body":" 前言：markdown在django中的配置与使用 1. 安装 pip install markdown 2. 视图中渲染 例：Article中定义text，想要渲染text blog/views.py import markdown from django.shortcuts import render, get_object_or_404 from .models import Article def detail(request, pk): article = get_object_or_404(Article, pk=pk) # 记得在顶部引入 markdown 模块 article.text = markdown.markdown(article.text, extensions=[ 'markdown.extensions.extra', 'markdown.extensions.codehilite', 'markdown.extensions.toc', ]) return render(request, 'blog/detail.html', context={'article': article}) 说明一下，这里用到了markdown.extensions函数, extra本身包含很多扩展 codehilite是语法高亮 toc是自动生成目录 3. safe标签 我们在发布的文章详情页没有看到预期的效果，而是类似于一堆乱码一样的 HTML 标签，这些标签本应该在浏览器显示它本身的格式，但是 Django 出于安全方面的考虑，任何的 HTML 代码在 Django 的模板中都会被转义（即显示原始的 HTML 代码，而不是经浏览器渲染后的格式）。为了解除转义，只需在模板标签使用 safe 过滤器即可，告诉 Django，这段文本是安全的，你什么也不用做。在模板中找到展示博客文章主体的 {{ article.text }} 部分，为其加上 safe 过滤器，**，大功告成，这下看到预期效果了。 safe 是 Django 模板系统中的过滤器（Filter），可以简单地把它看成是一种函数，其作用是作用于模板变量，将模板变量的值变为经过滤器处理过后的值。例如这里 ，本来 经模板系统渲染后应该显示 body 本身的值，但是在后面加上 safe 过滤器后，渲染的值不再是text 本身的值，而是由 safe 函数处理后返回的值。过滤器的用法是在模板变量后加一个 | 管道符号，再加上过滤器的名称。可以连续使用多个过滤器，例如{{ var|filter1|filter2 }}。 4. 代码高亮   程序员写博客免不了要插入一些代码，Markdown 的语法使我们容易地书写代码块，但是目前来说，显示的代码块里的代码没有任何颜色，很不美观，也难以阅读，要是能够像我们的编辑器里一样让代码高亮就好了。虽然我们在渲染时使用了codehilite 拓展，但这只是实现代码高亮的第一步，还需要简单的几步才能达到我们的最终目的。 安装Pygments 首先我们需要安装 Pygments，激活虚拟环境，运行： pip install Pygments 安装即可。 搞定了，虽然我们除了安装了一下 Pygments 什么也没做，但 Markdown 使用 Pygments 在后台为我们做了很多事。如果你打开博客详情页，找到一段代码段，在浏览器查看这段代码段的 HTML 源代码，可以发现 Pygments 的工作原理是把代码切分成一个个单词，然后为这些单词添加 css 样式，不同的词应用不同的样式，这样就实现了代码颜色的区分，即高亮了语法。为此，还差最后一步，引入一个样式文件来给这些被添加了样式的单词定义颜色。 引入样式文件 pygmentize -f html -a .codehilite -S darcula > darcula.css 该命令会在命令路径下生成.css文件，在html文件可以引用，其中default是默认，也可以换成monokai github等，生成的就是对应主题的.css 文件 templates/base.html ... ... 注意：如果你按照教程中的方法做完后发现代码依然没有高亮，请依次检查以下步骤： 确保在渲染文本时添加了 markdown.extensions.codehilite 拓展，详情见上文。 确保安装了 Pygments。 确保代码块的 Markdown 语法正确，特别是指明该代码块的语言类型，具体请参见上文中 Markdown 的语法示例。 在浏览器端代码块的源代码，看代码是否被 pre 标签包裹，并且代码的每一个单词都被 span 标签包裹，且有一个 class 属性值。如果没有，极有可能是前三步中某个地方出了问题。 确保用于代码高亮的样式文件被正确地引入，具体请参见上文中引入样式文件的讲解。 有些样式文件可能对代码高亮没有作用，首先尝试用 github.css 样式文件做测试。 Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"常见坑/pip安装遇到的坑.html":{"url":"常见坑/pip安装遇到的坑.html","title":"pip安装遇到的坑","keywords":"","body":"pip安装各种坑 1.下载过慢 cd ~/.pip 如果没有，就创建.pip目录,之后创建pip.conf文件 vim ~/.pip/pip.conf 编辑，修改源 [global] index-url = https://pypi.tuna.tsinghua.edu.cn/simple 也可以改为其他源   阿里云 http://mirrors.aliyun.com/pypi/simple/   中国科技大学 https://pypi.mirrors.ustc.edu.cn/simple/   豆瓣(douban) http://pypi.douban.com/simple/   清华大学 https://pypi.tuna.tsinghua.edu.cn/simple/   中国科学技术大学 http://pypi.mirrors.ustc.edu.cn/simple/ 2.下载flask-mongoengine时出错 即使修改了源,在下载flask-mongoengine时，因为还有其他的依赖，所以还是下载不下来，到100%就卡住不动 可以先把依赖装好，之后再试 pip install rednose pip install nose pip install coverage pip install flask-mongoengine 3.python2/3版本支持问题 这块建议使用pyenv来管理python版本 因为不管是用软连接指向或者修改默认配置都没有一个命令修改默认版本来的快 安装配置 #1 安装pyenv，在命令行下键入： $ curl -L https://github.com/pyenv/pyenv-installer/raw/master/bin/pyenv-installer | bash #2 将安装路径写入~/.bashrc,没有就创建 echo 'export PYENV_ROOT=\"$HOME/.pyenv\"' >> ~/.bashrc echo 'export PATH=\"$PYENV_ROOT/bin:$PATH\"' >> ~/.bashrc echo 'eval \"$(pyenv init -)\"' >> ~/.bashrc $ source ~/.bashrc #配置立刻生效 使用 #1.查看pyenv当前支持哪些Python版本 python@ubuntu:~$ pyenv install --list Available versions: 2.1.3 2.2.3 2.3.7 ... #2.列出pyenv中所有可用的python版本 python@ubuntu:~$ pyenv versions system 3.5.4 * 3.6.4 (set by /home/python/.pyenv/version) # *表示当前使用的3.6.4版本 #3.选择指定的python版本 python@ubuntu:~$ pyenv global 3.5.4 #设置指定的版本 python@ubuntu:~$ python Python 3.5.4 (default, Mar 29 2018, 11:02:03) #已经切换到了3.5.4 [GCC 5.4.0 20160609] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> 切换Python版本以后，与版本相关的依赖也会一起切换。因此，我们不用担心不同的版本在系统中是否会相互干扰。 #4. 删除指定python版本 python@ubuntu:~$ pyenv uninstall 3.5.4 pyenv: remove /home/python/.pyenv/versions/3.5.4? y python@ubuntu:~$ pyenv versions system * 3.6.4 (set by /home/python/.pyenv/version) 安装python版本 # 由于直接安装从github上拉太慢，先下载包后安装 $ cd ~/.pyenv $ sudo mkdir cache $ wget http://mirrors.sohu.com/python/3.6.4/Python-3.6.4.tar.xz -P ~/.pyenv/cache/ $ pyenv install 3.6.4 -v $ wget http://mirrors.sohu.com/python/2.7.12/Python-2.7.12.tar.xz -P ~/.pyenv/cache/ $ pyenv install 2.7.12 -v #3.更新pyenv数据库 $ pyenv rehash #4.列出所安装的python版本 $ pyenv versions #5.切换python版本 $ pyenv global 3.6.4 #6.验证版本 $ python Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"常见坑/Reactive-native 运行错误.html":{"url":"常见坑/Reactive-native 运行错误.html","title":"Reactive-native 运行错误","keywords":"","body":"Reactive-native 运行错误 只能启动模拟器，无法安装app 删除node_modules使用yarn 安装app时出错找不到index.js 将index.ios.js改成index.js 安装并初始化成功undefined is not an object（evaluating 'Reactinternals.ReactCurrentOwner'） 执行yarn add react@16.0.0-alpha.12 Evaluating _reactNative.view.propTypes.style Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"常见坑/服务器错误排查.html":{"url":"常见坑/服务器错误排查.html","title":"服务器错误排查","keywords":"","body":"服务器错误排查 1.Server error 500 查看数据库是否正常 查看内存是否占满 及时清理内存 错误日志，success日志等 尽量不要在服务器上存储视频图片 使用第三方平台 web脚本错误，如php语法错误，lua语法错误等。 分析思路： 查看错误日志 配置文件错误 访问量过大 考虑使用负载均衡 Dos攻击 Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"扩展/Item2.html":{"url":"扩展/Item2.html","title":"Item 2","keywords":"","body":"Item2 修改mac上的终端配色 安装 brew cask install iterm2 配置主题 iTerm2 最常用的主题是 Solarized Dark theme，下载地址：http://ethanschoonover.com/solarized 打开 iTerm2，按Command + ,键， 打开 Preferences 配置界面，然后Profiles -> Colors -> Color Presets -> Import`，选择刚才解压的solarized->iterm2-colors-solarized->Solarized Dark.itermcolors文件，导入成功，最后选择 Solarized Dark 主题，就可以了。 配置oh my zsh Oh My Zsh 是对主题的进一步扩展，地址：https://github.com/robbyrussell/oh-my-zsh 一键安装 $ sh -c \"$(curl -fsSL https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh)\" 安装好之后，需要把 Zsh 设置为当前用户的默认 Shell（这样新建标签的时候才会使用 Zsh）： $ chsh -s /bin/zsh # 如果影响正常使用要切回 $ chsh -s /bin/bash # 使用时直接执行 /bin/bash 然后，我们编辑vim ~/.zshrc文件，将主题配置修改为ZSH_THEME=\"agnoster\" 配置字体 由于上面主题需要字体支持 所以我们需要下载字体 字体下载地址：Meslo LG M Regular for Powerline.ttf 下载好之后，直接在 Mac OS 中安装即可。 然后打开 iTerm2，按Command + ,键，打开 Preferences 配置界面，然后Profiles -> Text -> Font -> Chanage Font，选择 Meslo LG M Regular for Powerline 字体 声明高亮 使用brew 安装 $ brew install zsh-syntax-highlighting 编辑vim ~/.zshrc文件 最后一行添加 source /usr/local/share/zsh-syntax-highlighting/zsh-syntax-highlighting.zsh Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"扩展/MAC上比较好用的开发工具.html":{"url":"扩展/MAC上比较好用的开发工具.html","title":"MAC上比较好用的开发工具","keywords":"","body":"MAC上比较好用的开发工具 Typora 轻便简洁的markdown工具, 样式可以自定义，使用很舒服 uTorrent 种子下载器，人越多下载越快 Xmind 脑图工具，记录笔记或者流程图 DBeaver，免费的数据库管理工具 Qipmsg，类似于飞秋，局域网文件传送飞快 Xcode，苹果上app开发工具 圈点 图片编辑，有箭头文字截屏功能 iTerm，mac上非常好用的终端 Dash，管理各个语言框架的api文档 Docker 环境依赖容器，有它运维能省很多事 Charles 网络抓包工具 Anaconda-Navigator 环境集成,有许多python环境，jupyter编程 FileZilla 后端常用 服务器文件传输工具 Postman 接口测试工具，功能强大 Sequel Pro 轻量纯英文数据库管理工具 Navicat Premium收费但功能强大的数据库管理工具,可以很轻松的导出sql文件 shadowSocks 科学上网，从我做起 Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"扩展/RPC,微服务架构.html":{"url":"扩展/RPC,微服务架构.html","title":"RPC,微服务架构","keywords":"","body":"RPC,微服务架构 什么是RPC（ Remote Procedure Call Protocol ） 简单来说就是远程过程调用 本地有一个add方法 想在远程(比如说另一台服务器)调用 ​ 如果用socket中间就会增加很多步骤 ​ 换成代码就更麻烦了，为了更方便的实现这个过程 就出现了RPC RPC框架职责 RPC 框架要向调用方屏蔽各种复杂性，要向服务提供方也屏蔽各类复杂性 ： （ 1 ）调用方感觉就像调用本地函数一样 （ 2 ）服务提供方感觉就像实现一个本地函数一样来实现服务 所以整个 RPC 框架又分为 client 部分与 server 部分，负责把整个非（ 1 ）（ 2 ）的各类复杂性屏蔽，这些复杂性就是 RPC 框架的职责。 结论 （ 1 ） RPC 框架是架构微服务化的首要基础组件 ，它能大大降低架构微服务化的成本，提高调用方与服务提供方的研发效率，屏蔽跨进程调用函数（服务）的各类复杂细节 （ 2 ） RPC 框架的 职责 是： 让调用方感觉就像调用本地函数一样调用远端函数、让服务提供方感觉就像实现一个本地函数一样来实现服务 RPC 分布式 多线程 Docker容器 pyspider Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"扩展/with关键字与上下文管理.html":{"url":"扩展/with关键字与上下文管理.html","title":"with关键字与上下文管理","keywords":"","body":"with关键字与上下文管理 如果你有阅读源码的习惯，可能会看到一些优秀的代码经常出现带有 “with” 关键字的语句，它通常用在什么场景呢？今天就来说说 with 和 上下文管理器。 对于系统资源如文件、数据库连接、socket 而言，应用程序打开这些资源并执行完业务逻辑之后，必须做的一件事就是要关闭（断开）该资源。 比如 Python 程序打开一个文件，往文件中写内容，写完之后，就要关闭该文件，否则会出现什么情况呢？极端情况下会出现 \"Too many open files\" 的错误，因为系统允许你打开的最大文件数量是有限的。 同样，对于数据库，如果连接数过多而没有及时关闭的话，就可能会出现 \"Can not connect to MySQL server Too many connections\"，因为数据库连接是一种非常昂贵的资源，不可能无限制的被创建。 来看看如何正确关闭一个文件。 普通版： def m1(): f = open(\"output.txt\", \"w\") f.write(\"python之禅\") f.close() 这样写有一个潜在的问题，如果在调用 write 的过程中，出现了异常进而导致后续代码无法继续执行，close 方法无法被正常调用，因此资源就会一直被该程序占用而无法被释放。那么该如何改进代码呢？ 进阶版： def m2(): f = open(\"output.txt\", \"w\") try: f.write(\"python之禅\") except IOError: print(\"oops error\") finally: f.close() 改良版本的程序是对可能发生异常的代码处进行 try 捕获，使用 try/finally 语句，该语句表示如果在 try 代码块中程序出现了异常，后续代码就不再执行，而直接跳转到 except 代码块。而无论如何，finally 块的代码最终都会被执行。因此，只要把 close 放在 finally 代码中，文件就一定会关闭。 高级版： def m3(): with open(\"output.txt\", \"w\") as f: f.write(\"Python之禅\") 一种更加简洁、优雅的方式就是用 with 关键字。open 方法的返回值赋值给变量 f，当离开 with 代码块的时候，系统会自动调用 f.close() 方法， with 的作用和使用 try/finally 语句是一样的。那么它的实现原理是什么？在讲 with 的原理前要涉及到另外一个概念，就是上下文管理器（Context Manager）。 上下文管理器 任何实现了 enter() 和 exit() 方法的对象都可称之为上下文管理器，上下文管理器对象可以使用 with 关键字。显然，文件（file）对象也实现了上下文管理器。 那么文件对象是如何实现这两个方法的呢？我们可以模拟实现一个自己的文件类，让该类实现 enter() 和 exit() 方法。 class File(): def __init__(self, filename, mode): self.filename = filename self.mode = mode def __enter__(self): print(\"entering\") self.f = open(self.filename, self.mode) return self.f def __exit__(self, *args): print(\"will exit\") self.f.close() enter() 方法返回资源对象，这里就是你将要打开的那个文件对象，exit() 方法处理一些清除工作。 因为 File 类实现了上下文管理器，现在就可以使用 with 语句了。 with File('out.txt', 'w') as f: print(\"writing\") f.write('hello, python') 这样，你就无需显示地调用 close 方法了，由系统自动去调用，哪怕中间遇到异常 close 方法也会被调用。 contextlib Python 还提供了一个 contextmanager 的装饰器，更进一步简化了上下文管理器的实现方式。通过 yield 将函数分割成两部分，yield 之前的语句在 enter 方法中执行，yield 之后的语句在 exit 方法中执行。紧跟在 yield 后面的值是函数的返回值。 from contextlib import contextmanager @contextmanager def my_open(path, mode): f = open(path, mode) yield f f.close() 调用 with my_open('out.txt', 'w') as f: f.write(\"hello , the simplest context manager\") 总结 Python 提供了 with 语法用于简化资源操作的后续清除操作，是 try/finally 的替代方法，实现原理建立在上下文管理器之上。此外，Python 还提供了一个 contextmanager 装饰器，更进一步简化上下管理器的实现方式。 Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"扩展/前端高级工程师进阶扩展.html":{"url":"扩展/前端高级工程师进阶扩展.html","title":"前端高级工程师进阶扩展","keywords":"","body":"前端高级工程师进阶扩展 国内 腾讯 ISUX 腾讯全端 AlloyTeam 奇舞周刊 阿里巴巴 -U 一点 淘宝前端团队 (FED) 网易 FEG 前端团队 网易用户体验中心 码农周刊 凹凸实验室 ( 京东 ) WEB 前端开发 百度 WEB 前端研发部 搜车大无线团队博客 饿了么大前端 美团点评技术团队 携程设计委员会 前端观察 ( 腾讯 ) 前端外刊评论 Awesomes-Web 前端开发资源库 掘进|发现|前端 Segmentfault|头条|前端 开发者头条|前端 伯乐在线| Web 前端 F2EX.cn web前端开发|web前端资源库 国外 A JS tip per day! CodePen - Front End Developer Playground Reloading – Medium Cybersecurity – Medium Hacker Noon Frontend Focus JavaScript Weekly Css Weekly Web Design Weekly Web Tools Weekly Learn CSS Animation Hack Design Codeburst SitePen Blog JavaScript Weekly ^subscribe Frontend Focus ^subscribe WebOps Weekly ^subscribe Node Weekly ^subscribe Mobile Dev Weekly ^subscribe 前端频道之个人博客 国内 阮一峰的网络日志 张鑫旭 - 鑫空间 - 鑫生活 翁天信的博客 勾三股四 / 赵锦江 Hux Blog( 黄玄 ) CSS 森林 (CSS Forest) 小胡子哥 ( 李靖 ) JerryQu( 屈光宇 ) 十年踪迹的博客 流浪小猫的博客 颜海镜的博客 志文工作室 晚晴幽草轩 Chuck Liu 的博客 Calpa’s Blog 轩枫阁 – 前端开发 HacPai - Vanessa(💃) Cherry’s Blog(💃) 国外 Bitsofcode Css Tricks Blog - Advanced Web Machinery Designmodo: Web Design Blog and Shop Noupe - THE magazine for webworkers and site owners Pragmatists blog Smashing Magazine Web Designer Wall DesignM.ag Just™ Creative Blog Line25 Web Design Blog Find and Fix Node.js Security Flaws John Papa: John Papa Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"数据库/mongo.html":{"url":"数据库/mongo.html","title":"Mongo","keywords":"","body":"mongo ORM字段 BinaryField BooleanField ComplexDateTimeField DateTimeField DecimalField DictField DynamicField EmailField EmbeddedDocumentField FileField FloatField GenericEmbeddedDocumentField GenericReferenceField GeoPointField ImageField IntField ListField MapField ObjectIdField ReferenceField SequenceField SortedListField StringField URLField UUIDField app = create_app() # 让python支持命令行工作 manager = Manager(app) migrate = Migrate(app, db) # 添加迁移脚本的命令到manager中 manager.add_command('db', MigrateCommand) if __name__ == '__main__': manager.run() from flask_script import Manager from flask_migrate import Migrate, MigrateCommand from v1.app import create_app from v1.models import db, StudentInfo app = create_app() # 让python支持命令行工作 manager = Manager(app) @manager.command def create_student(): name = 'student' student = StudentInfo(name=name) student.save() print('创建成功') if __name__ == '__main__': app = create_app() app.debug = True app.run(host='0.0.0.0', port=12341) Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"数据库/MongoEngine.html":{"url":"数据库/MongoEngine.html","title":"Mongo Engine","keywords":"","body":"Mongoengine 连接 connect connect（db = None，alias ='default'，** kwargs ） 多数据库支持 register_connection register_connection( alias=\"default\", db=None, name=None, host=None, port=None, username=None, password=None ) models中切换 from MongoEngine import * class User(Document): name = StringField() meta = { \"db_alias\": \"default\" } 字段 StringField 字符串 URLField Url EmailField 邮箱地址字段 Intfield 32位整数 LongField 64位整数 FloatField 浮点数字段 DecimalField 定点十进制 BooleanField 布尔 DateTimeField 时间 ComplexDateTimeField 精确毫秒级时间 EmbeddedDocumentField 嵌入式文档，有声明的document_type GenericEmbeddedDocumentField 通用嵌入式文档 DynamicField 动态字段类型 ListField 列表字段 EmbeddedDocumentListField 嵌入式有文件的List字段 SortedListField 排序的列表字段，确保始终检索为已排序的列表 DictField 字典 MapField 名称映射到指定字段 ReferenceField 文档引用 # 使用reverse_delete_rule可以处理删除字段引用的文档时应该发生的情况。 DO_NOTHING（0） - 不做任何事情（默认）。 NULLIFY（1） - 更新对null的引用。 CASCADE（2） - 删除与参考相关的文档。 DENY（3） - 防止删除参考对象。 PULL（4） - 从ListField参考文献中拉出参考 初始化参考字段。 参数： dbref - 将引用存储DBRef 为ObjectId.id 或.id。 reverse_delete_rule - 确定删除引用对象时要执行的操作 LazyReferenceField GenericReferenceField BinaryField 二进制数据字段 FileField GirdFS存储字段 ImageField 图像文件存储字段 SequenceField 可以实现id自增 ObjectIdField UUIDField GridFSProxy ImageGridFsProxy ImproperlyConfigured 查询 all() 返回所有文档 all_fields() 包括所有字段 post = BlogPost.objects.exclude('comments').all_fields() as_pymongo() 返回的不是Document实例 而是pymongo值 batch_size() 限制单个批处理中返回的文档数 clone() 创建副本 comment(text) 查询中添加注释 count(with_limit_and_skip=False) 数量 create(**kwargs) 创建新对象，返回保存的对象实例 delete（write_concern = None，_from_doc_delete = False，cascade_refs = None ） 删除查询匹配 的文档 参数： write_concern - 向下传递额外的关键字参数，这些参数将用作结果getLastError命令的选项 。例如， 将等到至少两个服务器已记录写入并将强制主服务器上的fsync。save(..., write_concern={w: 2, fsync: True}, ...)_from_doc_delete - 从文档删除调用时为True，因此信号将被触发，因此不要循环。 返回已删除的文档数量 exec_js(code, fields, *options) 执行js代码 参数： code - 要执行的一串Javascript代码fields - 您将在函数中使用的字段，它们将作为参数传递给您的函数选项 - 您希望函数可用的选项（通过options对象在Javascript中访问） fields（_only_called = False，*\\ kwargs* ） 处理如何加载此文档的字段 仅包含字段的子集： posts = BlogPost.objects().fields（author = 1，title = 1） 排除特定字段： posts = BlogPost.objects().fields（comments = 0） 要检索数组元素的子范围： posts = BlogPost.objects().fields（slice__comments = 5） filter() 过滤, 别名__call__() first() 匹配的第一个对象 from_json(json_data) 将json数据转换为未保存的对象 hint(index=None) 提示， 在对多个字段进行查询时，将索引字段作为提示传递，若索引不存在，提示不会执行任何操作 可以极大提高查询性能 in_bulk(object_ids) 通过其id检索一组文档 参数： object_ids - ObjectIds 的列表或元组 返回类型： ObjectIds的dict作为键，集合特定的Document子类作为值。 insert() 批量插入文档 item_frequencies（field，normalize = False，map_reduce = True ） 返回整个查询文档集中字段中存在的所有项的字典及其对应的频率。这对于生成标记云或搜索文档很有用。 limit(n) 返回的文档数限制为n，也可以用切片,如User.objects[:5] max_time_ms(ms) 服务器上终止查询前等待ms毫秒 modify(upsert = False，full_response = False，remove = False，new = False，*\\ update* ) 更新并返回更新的文档。 基于新 参数返回修改之前或之后的文档。如果没有文档与查询匹配且upsert为false，则返回None。如果 upserting和new为false，则返回None。 如果full_response参数是True，则返回值将是来自服务器的整个响应对象，包括'ok'和'lastErrorObject'字段，而不仅仅是修改后的文档。这很有用，主要是因为'lastErrorObject'文档包含有关命令执行的信息。 参数： upsert - 如果文档不存在则插入（默认False）full_response - 从服务器返回整个响应对象（默认情况下False，不适用于PyMongo 3+）删除 - 删除而不是更新（默认False）new - 返回更新而不是原始文档（默认False）更新 - Django样式的更新关键字参数 next() 将结果包装在document对象中 no_cache() 转换非缓存查询集 no_dereference() 解除对此查询集的结果引用 no_sub_classes() 仅返回此文档实例，不返回继承文档 only() 仅加载此文档的子集 only（）是可链接的并且将执行union ::所以使用以下内容它将同时获取：title和author.name： post = BlogPost.objects.only('title').only('author.name') order_by(*keys) 结果通过key来指定顺序，升降序 scalar(*field) 不返回document实例，按顺序返回特定值或元组值 可以与no_dereference()关闭解除一起使用 search_text(text,language=None) 使用文本索引开始文本搜索 skip(n) 返回结果前跳过n个文档 timeout(enabled) 查询时启用或禁用默认的超时 to_json() 将查询集转换为json update(upsert = False，multi = True，write_concern = None，full_result = False，*\\ update* ) 更新 参数： upsert - 如果文档不存在则插入（默认False）multi - 更新多个文档。write_concern - 向下传递额外的关键字参数，这些参数将用作结果getLastError命令的选项 。例如， 将等到至少两个服务器已记录写入并将强制主服务器上的fsync。save(..., write_concern={w: 2, fsync: True}, ...)full_result - 返回完整的结果字典，而不仅仅是更新的数字，例如return。{'n': 2, 'nModified': 2, 'ok': 1.0, 'updatedExisting': True}更新 - Django样式的更新关键字参数 Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"数据库/NoSQL数据库.html":{"url":"数据库/NoSQL数据库.html","title":"NoSQL数据库","keywords":"","body":"NoSQL数据库 NoSQL数据库包括键值数据库、文档数据库、列式数据库、基于图的数据库 键值数据库：类似于字典 文档数据库：类似于json，是可扩展的，无固定表结构 列式数据库：最快的NoSQL数据库 最流行的键值数据库Redis、Riak、 Amazon DynamoDB Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"数据库/postgresql.html":{"url":"数据库/postgresql.html","title":"Postgresql","keywords":"","body":"redis和postgres安装 redis # 安装 apt-get install redis-server 打开配置文件. /etc/redis/redis.conf，将daemonize属性改为yes（表明需要在后台运行） 后台启动 redis-server /erc/redis/redis.conf 运行redis-cli查看是否配置成功 postgresql apt-get install postgresql 启动 /etc/init.d/postgresql start 修改密码 说明：postgresql安装之后有个默认用户postgres,密码随机发放，所以首先修改密码 # 删除原密码 sudo passwd -d postgres # passwd: password expiry information changed. # 更新密码 sudo -u postgres passwd # passwd: password updated successfully 进入/etc/postgresql/9.5/main/pg_hba.conf修改 # \"local\" is for Unix domain socket connections only local all all trust # IPv4 local connections: host all all 127.0.0.1/32 md5 # IPv6 local connections: host all all ::1/128 md5 # Allow replication connections from localhost, by a user with the # replication privilege. #local replication postgres peer #host replication postgres 127.0.0.1/32 md5 #host replication postgres ::1/128 md5 host all your_database_name your_ip/32 md5 放开访问权限 修改pg_hba.conf文件 修改postgresql.conf文件 进入数据库创建用户和数据库 查看库\\l 切换表 \\c tablename 查看表 \\dt 查看用户 \\du 修改数据库用户密码 ALTER USER postgres WITH PASSWORD 'password'; 退出 \\q docker中的postgresql操作 # 基础命令 # 导入数据库 docker exec -i 9721ad9c8b8e psql -U onlinejudge onlinejudge 清除postgresql表中所有数据 # 清除表中所有数据 CREATE OR REPLACE FUNCTION truncate_tables(username IN VARCHAR) RETURNS void AS {% math %} DECLARE statements CURSOR FOR SELECT tablename FROM pg_tables WHERE tableowner = username AND schemaname = 'public'; BEGIN FOR stmt IN statements LOOP EXECUTE 'TRUNCATE TABLE ' || quote_ident(stmt.tablename) || ' CASCADE;'; END LOOP; END; {% endmath %} LANGUAGE plpgsql; # 执行 SELECT truncate_tables('MYUSER'); Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"数据库/related-name.html":{"url":"数据库/related-name.html","title":"Related Name","keywords":"","body":"Django ORM [TOC] 一. 单表操作 QuerySet数据类型 QuerySet与惰性机制 所谓惰性机制：Publisher.objects.all()或者.filter()等都只是返回了一个QuerySet（查询结果集对象），它并不会马上执行sql，而是当调用QuerySet的时候才执行。 QuerySet特点 可迭代 可切片 惰性计算和缓存 def queryset(request): books=models.Book.objects.all()[:10] #切片 应用分页 books = models.Book.objects.all()[::2] book= models.Book.objects.all()[6] #索引 print(book.title) for obj in books: #可迭代 print(obj.title) books=models.Book.objects.all() #惰性计算--->等于一个生成器，不应用books不会执行任何SQL操作 # query_set缓存机制1次数据库查询结果query_set都会对应一块缓存，再次使用该query_set时，不会发生新的SQL操作； #这样减小了频繁操作数据库给数据库带来的压力; authors=models.Author.objects.all() for author in authors: print(author.name) print('-------------------------------------') models.Author.objects.filter(id=1).update(name='张某') for author in authors: print(author.name) #但是有时候取出来的数据量太大会撑爆缓存，可以使用迭代器优雅得解决这个问题； models.Publish.objects.all().iterator() return HttpResponse('OK') 查询 双下划线应用 # 获取个数 # # models.Tb1.objects.filter(name='seven').count() # 大于，小于 # # models.Tb1.objects.filter(id__gt=1) # 获取id大于1的值 # models.Tb1.objects.filter(id__gte=1) # 获取id大于等于1的值 # models.Tb1.objects.filter(id__lt=10) # 获取id小于10的值 # models.Tb1.objects.filter(id__lte=10) # 获取id小于10的值 # models.Tb1.objects.filter(id__lt=10, id__gt=1) # 获取id大于1 且 小于10的值 # in # # models.Tb1.objects.filter(id__in=[11, 22, 33]) # 获取id等于11、22、33的数据 # models.Tb1.objects.exclude(id__in=[11, 22, 33]) # not in # isnull # Entry.objects.filter(pub_date__isnull=True) # contains # # models.Tb1.objects.filter(name__contains=\"ven\") # models.Tb1.objects.filter(name__icontains=\"ven\") # icontains大小写不敏感 # models.Tb1.objects.exclude(name__icontains=\"ven\") # range # # models.Tb1.objects.filter(id__range=[1, 2]) # 范围bettwen and # 其他类似 # # startswith，istartswith, endswith, iendswith, # order by # # models.Tb1.objects.filter(name='seven').order_by('id') # asc # models.Tb1.objects.filter(name='seven').order_by('-id') # desc # group by # # from django.db.models import Count, Min, Max, Sum # models.Tb1.objects.filter(c1=1).values('id').annotate(c=Count('num')) # SELECT \"app01_tb1\".\"id\", COUNT(\"app01_tb1\".\"num\") AS \"c\" FROM \"app01_tb1\" WHERE \"app01_tb1\".\"c1\" = 1 GROUP BY \"app01_tb1\".\"id\" # limit 、offset # # models.Tb1.objects.all()[10:20] # regex正则匹配，iregex 不区分大小写 # # Entry.objects.get(title__regex=r'^(An?|The) +') # Entry.objects.get(title__iregex=r'^(an?|the) +') # date # # Entry.objects.filter(pub_date__date=datetime.date(2005, 1, 1)) # Entry.objects.filter(pub_date__date__gt=datetime.date(2005, 1, 1)) # year # # Entry.objects.filter(pub_date__year=2005) # Entry.objects.filter(pub_date__year__gte=2005) # month # # Entry.objects.filter(pub_date__month=12) # Entry.objects.filter(pub_date__month__gte=6) # day # # Entry.objects.filter(pub_date__day=3) # Entry.objects.filter(pub_date__day__gte=3) # week_day # # Entry.objects.filter(pub_date__week_day=2) # Entry.objects.filter(pub_date__week_day__gte=2) # hour # # Event.objects.filter(timestamp__hour=23) # Event.objects.filter(time__hour=5) # Event.objects.filter(timestamp__hour__gte=12) # minute # # Event.objects.filter(timestamp__minute=29) # Event.objects.filter(time__minute=46) # Event.objects.filter(timestamp__minute__gte=29) # second # # Event.objects.filter(timestamp__second=31) # Event.objects.filter(time__second=2) # Event.objects.filter(timestamp__second__gte=31) 进阶操作 extra,F,Q # extra 过滤 # # extra(self, select=None, where=None, params=None, tables=None, order_by=None, select_params=None) # Entry.objects.extra(select={'new_id': \"select col from sometable where othercol > %s\"}, select_params=(1,)) # Entry.objects.extra(where=['headline=%s'], params=['Lennon']) # Entry.objects.extra(where=[\"foo='a' OR bar = 'a'\", \"baz = 'a'\"]) # Entry.objects.extra(select={'new_id': \"select id from tb where id > %s\"}, select_params=(1,), order_by=['-nid']) # F 列字段操作 # # from django.db.models import F # models.Tb1.objects.update(num=F('num')+1) # Q 条件查询 # # 方式一： # Q(nid__gt=10) # Q(nid=8) | Q(nid__gt=10) # Q(Q(nid=8) | Q(nid__gt=10)) & Q(caption='root') # 方式二： # con = Q() # q1 = Q() # q1.connector = 'OR' # q1.children.append(('id', 1)) # q1.children.append(('id', 10)) # q1.children.append(('id', 9)) # q2 = Q() # q2.connector = 'OR' # q2.children.append(('c1', 1)) # q2.children.append(('c1', 10)) # q2.children.append(('c1', 9)) # con.add(q1, 'AND') # con.add(q2, 'AND') # # models.Tb1.objects.filter(con) # 执行原生SQL # # from django.db import connection, connections # cursor = connection.cursor() # cursor = connections['default'].cursor() # cursor.execute(\"\"\"SELECT * from auth_user where id = %s\"\"\", [1]) # row = cursor.fetchone() 二. 连表操作 我们在学习django中的orm的时候，我们可以把一对多，多对多，分为正向和反向查找两种方式。 正向查找：ForeignKey在 UserInfo表中，如果从UserInfo表开始向其他的表进行查询，这个就是正向操作，反之如果从UserType表去查询其他的表这个就是反向操作。 一对多：models.ForeignKey(其他表) 多对多：models.ManyToManyField(其他表) 一对一：models.OneToOneField(其他表) 正向连表操作总结： 所谓正、反向连表操作的认定无非是Foreign_Key字段在哪张表决定的， Foreign_Key字段在哪张表就可以哪张表使用Foreign_Key字段连表，反之没有Foreign_Key字段就使用与其关联的 小写表名； 1对多：对象.外键.关联表字段，values(外键字段__关联表字段) 多对多：外键字段.all() 反向连表操作总结： 通过value、valuelist、fifter 方式反向跨表：小写表名_关联表字段 通过对象的形式反向跨表：小写表名_set().all() 应用场景： 一对多：当一张表中创建一行数据时，有一个单选的下拉框（可以被重复选择） 例如：创建用户信息时候，需要选择一个用户类型【普通用户】【金牌用户】【铂金用户】等。 多对多：在某表中创建一行数据是，有一个可以多选的下拉框 例如：创建用户信息，需要为用户指定多个爱好 Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"机器学习/SVM与K-Means.html":{"url":"机器学习/SVM与K-Means.html","title":"SVM与K-Means","keywords":"","body":"SVM 理解： 在二维上，找一条最优分割线将两类分开，分割线满足分类两边尽可能有最大间隙 当直线难以完成分类时，引入超平面使数据分类，分类的契机是kerneling内核 应用场景 SVM主要针对小样本数据进行学习，分类和预测（有时也较回归），有很好的泛化能力 SVM内核： Linear 主要用于线性可分的情形。参数少，速度快，适用于一般数据 rbf 主要用于线性不可分的情形。参数多，分类结果非常依赖于参数。 Poly 参数较多，在另外两种都不适用的时候选择 经验： 就拟合程度来讲，linear在线性可分的情况下和rbf想过差不多，在线性不可分的情况下rbf明显优于linear，poly在前两种情况下效果都不怎么好，但是在变化剧烈的情况下ploy稍微好点。 就速度来讲，linear肯定是最快的，poly的话因为参数很多，测试中最慢。 就参数而言，linear简单易用，rbf, poly参数较多，但是调参好的话可以得到较好的结果。 实战： 鸢尾花分类 导包 from sklearn.svm import SVC from sklearn.svm import LinearSVC from sklearn import datasets 生成数据，训练数据 ```python iris = datasets.load_iris() 只取两个特征（方便画图） X = iris.data[:,:2] y = iris.target 建立模型 svc_linear = SVC(kernel='linear') svc_rbf = SVC(kernel='rbf')#Radial Based Function 基于半径的函数 svc_poly = SVC(kernel='poly') # poly是多项式的意思 linear_svc = LinearSVC() # SVC(kernel = 'linear')相近方法更多，可以处理更多的数据 训练模型 svc_linear.fit(X,y) svc_rbf.fit(X,y) svc_poly.fit(X,y) linear_svc.fit(X,y) - 图片背景云点 ```python # 网格密度 h = 0.02 # 设置x轴y轴的界限 x_min,x_max = X[:,0].min()-1, X[:,0].max()+1 y_min,y_max = X[:,1].min()-1, X[:,1].max()+1 # 得到网格的坐标 xx,yy = np.meshgrid(np.arange(x_min,x_max,h), np.arange(y_min,y_max,h)) 绘制图形 # 设置图片标题 titles = ['svc_linear', 'svc_rbf', 'svc_poly', 'linear_svc'] plt.figure(figsize=(12,8)) # 在2*2子图中画出四种SVC for i,clf in enumerate((svc_linear,svc_rbf,svc_poly,linear_svc)): plt.subplot(2,2,i+1) Z = clf.predict(np.c_[xx.ravel(),yy.ravel()]) Z = Z.reshape(xx.shape) # 等高线以及背景 plt.contourf(xx,yy,Z,alpha=0.2,cmap = 'cool') # 实际点的图 plt.scatter(X[:,0],X[:,1],c=y,cmap='rainbow') plt.title(titles[i]) 多种核函数回归 导包 from sklearn.svm import SVR import numpy as np 随机生成数据，训练数据 #自定义样本点 X = 5*np.random.rand(40,1) X.sort(axis = 0) y = np.sin(X).ravel() #添加噪声 y[::5] += 3*(0.5 - np.random.rand(8)) #建立模型 svr_linear = SVR(kernel='linear') svr_rbf = SVR(kernel = 'rbf') svr_poly = SVR(kernel = 'poly') #训练并预测 p_y_linear = svr_linear.fit(X,y).predict(X) p_y_rbf = svr_rbf.fit(X,y).predict(X) p_y_poly = svr_poly.fit(X,y).predict(X) 绘图 # 画图 plt.figure(figsize=(12,8)) # 画出真实样本点 plt.scatter(X,y,c='k',label='data') # 画出预测曲线 plt.plot(X,p_y_linear,c='navy',label='linear') plt.plot(X,p_y_rbf,c='r',label='rbf') plt.plot(X,p_y_poly,c='g',label='poly') plt.legend() K-Means 应用场景 一种无监督学习，事先不知道结果，自动将相似对象归类 原理 与knn类似，使用欧式距离函数，求各点到各分类中心点的距离，距离近的归为一类 步骤图 缺陷 K值的选定(分多少类) ISODATA算法通过类的自动合并和分裂，得到较为合理的类型数目K 初始随机种子点的选取(分类参考点) K-Means++算法可以用来解决这个问题，其可以有效地选择初始点 步骤 从数据中选择k个对象作为初始聚类中心; 计算每个聚类对象到聚类中心的距离来划分； 再次计算每个聚类中心 计算标准测度函数，直到达到最大迭代次数，则停止，否则，继续操作。 确定最优的聚类中心 实战 Make_blobs随机生成点 效果图 导包 from sklearn.cluster import KMeans import matplotlib.pyplot as plt %matplotlib inline import numpy as np from sklearn.datasets import make_blobs #生成样本点 X_train,y_train = make_blobs(n_samples=300,centers=4,cluster_std= 0.6, random_state = 9) 建模 # 建立模型 kmeans = KMeans(n_clusters=4) kmeans.fit(X_train) y_ = kmeans.predict(X_train) 画图 #画图 plt.figure(figsize = (12,8)) centers = kmeans.cluster_centers_ plt.scatter(X_train[:,0],X_train[:,1],c = y_) plt.scatter(centers[:,0],centers[:,1],c = 'r',s = 100,alpha = 0.4) Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"机器学习/TensorFlow.html":{"url":"机器学习/TensorFlow.html","title":"Tensor Flow","keywords":"","body":"TensorFlow 简介 TensorFlow™ 是一个开放源代码软件库，用于进行高性能数值计算。借助其灵活的架构，用户可以轻松地将计算工作部署到多种平台（CPU、GPU、TPU）和设备（桌面设备、服务器集群、移动设备、边缘设备等 Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"机器学习/决策树与贝叶斯.html":{"url":"机器学习/决策树与贝叶斯.html","title":"决策树与贝叶斯","keywords":"","body":"决策树 上图完整表达了这个女孩决定是否见一个约会对象的策略，其中绿色节点表示判断条件，橙色节点表示决策结果，箭头表示在一个判断条件在不同情况下的决策路径，图中红色箭头表示了上面例子中女孩的决策过程 原理： 决策树（decision tree）是一个树结构（可以是二叉树或非二叉树）。其每个非叶节点表示一个特征属性上的测试，每个分支代表这个特征属性在某个值域上的输出，而每个叶节点存放一个类别。使用决策树进行决策的过程就是从根节点开始，测试待分类项中相应的特征属性，并按照其值选择输出分支，直到到达叶子节点，将叶子节点存放的类别作为决策结果。 应用： 决策树算法能够读取数据集合，构建类似于上面的决策树。 决策树可以使用不熟悉的数据集合，并从中提取出一系列规则，机器学习算法最终将使用这些机器从数据集中创造的规则。 优缺点： 优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。既能用于分类，也能用于回归 缺点：可能会产生过度匹配问题 构造： 构造决策树的关键步骤是分裂属性。即在某个节点处按某一特征属性的不同划分构造不同的分支 分裂属性的三种情况： 1、属性是离散值且不要求生成二叉决策树。此时用属性的每一个划分作为一个分支。 2、属性是离散值且要求生成二叉决策树。此时使用属性划分的一个子集进行测试，按照“属于此子集”和“不属于此子集”分成两个分支。 3、属性是连续值。此时确定一个值作为分裂点split_point，按照>split_point和 构造决策树的关键内容是属性选择度量,属性选择度量算法有很多，这里介绍常用的ID3算法 ID3算法: 划分原则：将无序数据变得有序 熵：在信息学中，是对不确定性的度量 信息增益 计算熵 在决策树当中，设D为用类别对训练元组进行的划分，则D的熵表示为： ID3算法就是在每次需要分裂时，计算每个属性的增益率，然后选择增益率最大的属性进行分裂 使用决策树进行分类 导包 from sklearn import datasets from sklearn.tree import DecisionTreeClassifier 导入数据 iris = datasets.load_iris() X = iris.data y = iris.target 使用决策树 #max_depth 设置决策树的最大深度 reg1 = DecisionTreeClassifier(max_depth=5) reg1.fit(X,y).score(X,y) 使用回归预测一个椭圆 效果图 导包 import numpy as np import matplotlib.pyplot as plt from sklearn.tree import DecisionTreeRegressor 创造数据 # 创建X与y rng = np.random.RandomState(1) #伪随机数 == np.random.seed(1) #随机生成-100 到100的数字,这些数字就是角度 X = np.sort(200 * rng.rand(100,1) - 100,axis = 0) #根据角度生成正弦值和余弦值，这些值就是圆上面的点 y = np.array([np.pi * np.sin(X).ravel(),np.pi * np.cos(X).ravel()]).transpose() y[::5,:] += (0.5 -rng.rand(20,2))#添加噪声 训练数据 参数max_depth越大，越容易过拟合 # 第1步：训练 regr1 = DecisionTreeRegressor(max_depth=2) regr2 = DecisionTreeRegressor(max_depth=5) regr3 = DecisionTreeRegressor(max_depth=8) regr1.fit(X,y) regr2.fit(X,y) regr3.fit(X,y) 训练数据 # 第2步：预测 X_test = np.arange(-100.0,100.0,0.01)[:,np.newaxis] y_1 = regr1.predict(X_test) y_2 = regr2.predict(X_test) y_3 = regr3.predict(X_test) 绘制 # 显示图像 plt.figure(figsize=(12,8)) s = 50 plt.subplot(221) plt.scatter(y[:,0],y[:,1],c='navy',s=s,label='data') plt.legend() plt.subplot(222) plt.scatter(y_1[:,0],y_1[:,1],c='b',s=s,label='data') plt.legend() plt.subplot(223) plt.scatter(y_2[:,0],y_2[:,1],c='r',s=s,label='data') plt.legend() plt.subplot(224) plt.scatter(y_3[:,0],y_3[:,1],c='g',s=s,label='data') plt.legend() plt.show() 贝叶斯 思想： 对于给出的待分类样本特征x，求解在此样本出现的条件下各个类别出现的概率，哪个最大，就认为此待分类样本属于哪个类别。 原理： 朴素贝叶斯： 朴素的概念：独立性假设，假设各个特征之间是独立不相关的 贝叶斯模型 高斯分布朴素贝叶斯 高斯分布就是正态分布 【用途】用于一般分类问题 使用自带的鸢尾花数据 from sklearn import datasets iris = datasets.load_iris() X = iris.data y = iris.target from sklearn.naive_bayes import GaussianNB gnb = GaussianNB() gnb.fit(X,y).score(X,y) 多项式分布朴素贝叶斯 多项式分布： 【用途】适用于文本数据（特征表示的是次数，例如某个词语的出现次数 from sklearn.naive_bayes import MultinomialNB mnb = MultinomialNB() mnb.fit(X,y).score(X,y) 伯努利分布朴素贝叶斯 【用途】适用于伯努利分布，也适用于文本数据（此时特征表示的是是否出现，例如某个词语的出现为1，不出现为0） 绝大多数情况下表现不如多项式分布，但有的时候伯努利分布表现得要比多项式分布要好，尤其是对于小数量级的文本数据 from sklearn.naive_bayes import BernoulliNB bnb = BernoulliNB() bnb.fit(X,y).score(X,y) ### 得分要比多项式朴素贝叶斯差 文本短信分类 使用多项式分布朴素贝叶斯 导包 import pandas as pd from sklearn.naive_bayes import MultinomialNB #转换数据，String串分析起来不方便，所以需要将字符串转进行转换 from sklearn.feature_extraction.text import TfidfVectorizer data = pd.read_table('../data/SMSSpamCollection',header = None) 处理数据 tf = TfidfVectorizer() display(data[1].shape,type(data[1])) X = tf.fit_transform(data[1]) y = data[0] 训练数据 mnb = MultinomialNB() #训练数据 mnb.fit(X,y) 预测数据 # 使用tf.transform把文本变成能处理的数字，生成一个测试数据 X_test = tf.transform(['07732584351 - Rodger Burns - MSG = We tried to call you re your reply to our sms for a free nokia mobile + free camcorder. Please call now 08000930705 for delivery tomorrow']) #预测数据 mnb.predict(X_test) 输出结果显示这是一条垃圾短信 Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"机器学习/特征工程.html":{"url":"机器学习/特征工程.html","title":"特征工程","keywords":"","body":"特征工程 思考 机器学习的算法最终预测结果很大程度与特征的筛选，清洗等有很大的关系，如何使特征的选取有章可循？ 这就是本文特征工程的重点 特征的使用方案 原则上根据业务，尽可能找出对因变量有影响的所有自变量 可用性评估:获取难度、覆盖率、准确率 特征处理 特征清洗：包括清洗异常样本、采样（数据不均衡，样本权重） 特征预处理（重点） 特征监控 特征有效性分析 监控重要特征，防止特征质量下降，影响模型效果 我们将特征处理中的预处理与特征如何选择作为重点来分析 数据预处理 from sklearn.preporcessing 无量纲化: 区间缩放法 区间缩放法利用了边界值信息，将特征的取值区间缩放到某个特点的范围，例如[0, 1]等 返回值为缩放到[0, 1]区间的数据 from sklearn.preprocessing import MinMaxScaler MinMaxScaler().fit_transform(iris.data) 标准化 标准化的前提是特征值服从正态分布，标准化后，其转换成标准正态分布。 标准化需要计算特征的均值和标准差 其中S为标准差 from sklearn.preprocessing import StandardScaler StandardScaler().fit_transform(iris.data) 归一化 标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，将样本的特征值转换到同一量纲下。归一化是依照特征矩阵的行处理数据，其目的在于样本向量在点乘运算或其他核函数计算相似性时，拥有统一的标准，也就是说都转化为“单位向量”。 from sklearn.preprocessing import Normalizer Normalizer().fit_transform(iris.data) 对定量特征二值化 定量特征二值化的核心在于设定一个阈值，大于阈值的赋值为1，小于等于阈值的赋值为0 from sklearn.preprocessing import Binarizer 对定性特征哑编码 (独热编码) from sklearn.preprocessing import OneHotEncoder # 对文本数据进行数字编码 from sklearn.preprocessing import LabelEncoder 缺失值计算 缺失值计算，返回值为计算缺失值后的数据 参数missing_value为缺失值的表示形式，默认为NaN 参数strategy为缺失值填充方式，默认为mean（均值） from numpy import vstack, array, nan from sklearn.preprocessing import Imputer # 例 Imputer().fit_transform(vstack((array([nan, nan, nan, nan]), iris.data))) 特征选择 按照特征是否发散选择：类似于方差，数据越集中越稳定 按照特征与目标的相关性强弱选择 依据经验选择 实现方式 from sklearn import feature_selection Filter（过滤法） # 按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征 from sklearn.feature_selection import VarianceThreshold # 方差选择法，返回值为特征选择后的数据 # 参数threshold为方差的阈值 VarianceThreshold(threshold=0.5).fit_transform(iris.data) Wrapper(包装法) 根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。 from sklearn.feature_selection import RFE from sklearn.linear_model import LogisticRegression #递归特征消除法，返回特征选择后的数据 #参数estimator为基模型 #参数n_features_to_select为选择的特征个数 RFE(estimator=LogisticRegression(), n_features_to_select=2).fit_transform(iris.data, iris.target) Embedded(集成法) 先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。 基于惩罚项的特征选择法 使用带惩罚项的基模型，除了筛选出特征外，同时也进行了降维。 from sklearn.feature_selection import SelectFromModel from sklearn.linear_model import LogisticRegression #带L1惩罚项的逻辑回归作为基模型的特征选择 # 最终选择的特征个数由C决定 SelectFromModel(LogisticRegression(penalty=\"l1\", C=0.1)).fit_transform(iris.data, iris.target) 基于树模型的特征选择法 树模型中GBDT也可用来作为基模型进行特征选择，使用feature_selection库的SelectFromModel类结合GBDT模型 from sklearn.feature_selection import SelectFromModel from sklearn.ensemble import GradientBoostingClassifier #GBDT作为基模型的特征选择 SelectFromModel(GradientBoostingClassifier()).fit_transform(iris.data, iris.target) 算法思想 GBDT的思想可以用一个通俗的例子解释，假如有个人30岁，我们首先用20岁去拟合，发现损失有10岁，这时我们用6岁去拟合剩下的损失，发现差距还有4岁，第三轮我们用3岁拟合剩下的差距，差距就只有一岁了。如果我们的迭代轮数还没有完，可以继续迭代下面，每一轮迭代，拟合的岁数误差都会减小。 数据挖掘流程 Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"机器学习/用机器学习的方法来学习机器学习.html":{"url":"机器学习/用机器学习的方法来学习机器学习.html","title":"用机器学习的方法来学习机器学习","keywords":"","body":"用机器学习的方法来学习机器学习 ​ 机器学习中的算法有很多，涉猎知识面也很广，有概率、线性代数、统计学等等。这篇文章不在于去理解这些算法的核心，而在于如何使用，之前几篇分别阐述了几种机器学习算法的原理，应用场景，小的示例。这篇文章就来总结下这些算法的一些特征。 ​ 大家都知道机器学习的算法实现包括下面步骤 获取数据 导入算法训练数据 得出结论 预测新数据 那么正好，我们现在也有了数据（各类算法的基础知识），通过算法（我们了解的简单结论）来训练数据，得出最终结论（各类算法的应用场景及其优缺点），这就是本文的初衷。 Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"机器学习/题海.html":{"url":"机器学习/题海.html","title":"题海","keywords":"","body":"1.美国各州人口数据分析 import numpy as np import pandas as pd from pandas import Series,DataFrame # 导入数据 abb = pd.read_csv('../data/state-abbrevs.csv') areas = pd.read_csv('../data/state-areas.csv') pop = pd.read_csv('../data/state-population.csv') # 合并pop与abbrevs两个DataFrame，分别依据state/region列和abbreviation列来合并。 # 为了保留所有信息，使用外合并。 pop_abb = pd.merge(pop,abb,left_on='state/region',right_on='abbreviation',how='outer') # 去除abbreviation的那一列（axis=1） # inplace 修改原始数据 pop_abb.drop('abbreviation',axis=1,inplace=True) # 查看存在缺失数据的列。 # 使用.isnull().any()，只有某一列存在一个缺失数据，就会显示True。 pop_abb.isnull().any() # 查看缺失数据 null_data = pop_abb[pop_abb.isnull().any(axis=1)] null_data # 根据数据是否缺失情况显示数据，如果缺失为True，那么显示 # 找到有哪些state/region使得state的值为NaN，使用unique()查看非重复值 pop_abb[pop_abb['state'].isnull()]['state/region'].unique() # 先查看state/region == PR的行 condition1 = (pop_abb['state/region'] == 'PR') # 赋值操作 pop_abb.loc[condition1,'state'] = 'PUERTO' # 为找到的这些state/region的state项补上正确的值，从而去除掉state这一列的所有NaN！ condition2 = (pop_abb['state/region'] == 'USA') pop_abb.loc[condition2,'state'] = 'America' # 查看结果 pop_abb[pop_abb['state/region'] == 'USA'] pop_abb[pop_abb['state/region'] == 'PR'] # 处理后的结果 pop_abb.isnull().any() # 合并各州面积数据areas，使用左合并。 # 因为pop_abb是一个相对完整的表，如果只有面积没有人口是没有意义的，所以以left为准 total = pd.merge(pop_abb,areas,how='left') # 继续寻找存在缺失数据的列 total.isnull().any() # 我们会发现area(sq.mi)这一列有缺失数据，为了找出是哪一行，我们需要找出是哪个state没有数据 total[total['area (sq. mi)'].isnull()]['state'].unique() drop_indexs = total[total['area (sq. mi)'].isnull()]['state'].index # 去除含有缺失数据的行 total.drop(drop_indexs,inplace=True) # 查看数据是否缺失 total.isnull().any() # 找出2010年的全民人口数据,df.query(查询语句) pop_2010 = total.query(\"ages == 'total' & year == 2010\") # 对查询结果进行处理，以state列作为新的行索引:set_index pop_2010.set_index('state',inplace=True) # 计算人口密度。注意是Series/Series，其结果还是一个Series。 pop_2010 #排序，并找出人口密度最高的五个州sort_values() (pop_2010['population']/pop_2010['area (sq. mi)']).sort_values() 2.美国2012政治献金 months = {'JAN' : 1, 'FEB' : 2, 'MAR' : 3, 'APR' : 4, 'MAY' : 5, 'JUN' : 6, 'JUL' : 7, 'AUG' : 8, 'SEP' : 9, 'OCT': 10, 'NOV': 11, 'DEC' : 12} of_interest = ['Obama, Barack', 'Romney, Mitt', 'Santorum, Rick', 'Paul, Ron', 'Gingrich, Newt'] parties = { 'Bachmann, Michelle': 'Republican', 'Romney, Mitt': 'Republican', 'Obama, Barack': 'Democrat', \"Roemer, Charles E. 'Buddy' III\": 'Reform', 'Pawlenty, Timothy': 'Republican', 'Johnson, Gary Earl': 'Libertarian', 'Paul, Ron': 'Republican', 'Santorum, Rick': 'Republican', 'Cain, Herman': 'Republican', 'Gingrich, Newt': 'Republican', 'McCotter, Thaddeus G': 'Republican', 'Huntsman, Jon': 'Republican', 'Perry, Rick': 'Republican' } 政治献金 3.城市气候与海洋的关系 import numpy as np import pandas as pd from pandas import Series,DataFrame import matplotlib.pyplot as plt %matplotlib inline # 读数据 ferrara1 = pd.read_csv('./ferrara_150715.csv') ferrara2 = pd.read_csv('./ferrara_250715.csv') ferrara3 = pd.read_csv('./ferrara_270615.csv') ferrara = pd.concat([ferrara1,ferrara2,ferrara3],ignore_index=True) torino1 = pd.read_csv('./torino_150715.csv') torino2 = pd.read_csv('./torino_250715.csv') torino3 = pd.read_csv('./torino_270615.csv') torino = pd.concat([torino1,torino2,torino3],ignore_index=True) mantova1 = pd.read_csv('./mantova_150715.csv') mantova2 = pd.read_csv('./mantova_250715.csv') mantova3 = pd.read_csv('./mantova_270615.csv') mantova = pd.concat([mantova1,mantova2,mantova3],ignore_index=True) milano1 = pd.read_csv('./milano_150715.csv') milano2 = pd.read_csv('./milano_250715.csv') milano3 = pd.read_csv('./milano_270615.csv') milano = pd.concat([milano1,milano2,milano3],ignore_index=True) ravenna1 = pd.read_csv('./ravenna_150715.csv') ravenna2 = pd.read_csv('./ravenna_250715.csv') ravenna3 = pd.read_csv('./ravenna_270615.csv') ravenna = pd.concat([ravenna1,ravenna2,ravenna3],ignore_index=True) asti1 = pd.read_csv('./asti_150715.csv') asti2 = pd.read_csv('./asti_250715.csv') asti3 = pd.read_csv('./asti_270615.csv') asti = pd.concat([asti1,asti2,asti3],ignore_index=True) bologna1 = pd.read_csv('./bologna_150715.csv') bologna2 = pd.read_csv('./bologna_250715.csv') bologna3 = pd.read_csv('./bologna_270615.csv') bologna = pd.concat([bologna1,bologna2,bologna3],ignore_index=True) piacenza1 = pd.read_csv('./piacenza_150715.csv') piacenza2 = pd.read_csv('./piacenza_250715.csv') piacenza3 = pd.read_csv('./piacenza_270615.csv') piacenza = pd.concat([piacenza1,piacenza2,piacenza3],ignore_index=True) cesena1 = pd.read_csv('./cesena_150715.csv') cesena2 = pd.read_csv('./cesena_250715.csv') cesena3 = pd.read_csv('./cesena_270615.csv') cesena = pd.concat([cesena1,cesena2,cesena3],ignore_index=True) faenza1 = pd.read_csv('./faenza_150715.csv') faenza2 = pd.read_csv('./faenza_250715.csv') faenza3 = pd.read_csv('./faenza_270615.csv') faenza = pd.concat([faenza1,faenza2,faenza3],ignore_index=True) # 查看列数 faenza.columns # 去除没用的列 citys = [ferrara,torino,mantova,milano,ravenna,asti,bologna,piacenza,cesena,faenza] for city in citys: city.drop('Unnamed: 0',axis=1,inplace=True) # 显示最高温度与离海远近的关系 max_temps = [] distances = [] for city in citys: max_temps.append(city['temp'].max()) distances.append(city['dist'].mean()) plt.scatter(distances,max_temps) # 把距离按照从小到大进行排序，使用线性图来查看结果 df = DataFrame(data = { 'temp':max_temps, 'distance':distances }) sort_df = df.sort_values('distance') plt.plot(sort_df.distance,sort_df.temp) #观察发现，离海近的可以形成一条直线，离海远的也能形成一条直线。 #首先使用numpy：把列表转换为numpy数组，用于后续计算。 #分别以100公里和50公里为分界点，划分为离海近和离海远的两组数据 temps_np = np.array(max_temps) dist_np = np.array(distances) # 近海城市 near_dists = dist_np[dist_np50] far_temps = temps_np[dist_np>50] display(near_dists,near_temps) display(far_dists,far_temps) 。。。歇一会 继续 # 使用支持向量机计算回归参数 # 使用线性回归模型 from sklearn.linear_model import LinearRegression # 构建算法模型 near = LinearRegression() # 训练机器学习模型 # 训练【样本集】（自变量）必须是列向量 near.fit(near_dists.reshape(-1,1),near_temps) # 获取预测数据 near_test = np.linspace(near_dists.min()-10,near_dists.max()+10,100).reshape(-1,1) near_temps = near.predict(near_test) far = LinearRegression() far.fit(far_dists.reshape(-1,1),far_temps) far_test = np.linspace(far_dists.min()-10,far_dists.max()+10,100).reshape(-1,1) far_temps = far.predict(far_test) plt.plot(near_test,near_temps,color='blue',label='near_predict') plt.plot(far_test,far_temps,color='green',label='far_predict') plt.scatter(distances,max_temps,color='red',label='true_data') plt.legend() 查看最低温度与海洋距离的关系 最低湿度与海洋距离的关系 最高湿度与海洋距离的关系 平均湿度与海洋距离的关系 思考：模仿最高温度，得到平均湿度与海洋距离的回归曲线 风向与风速的关系 在子图中，同时比较风向与湿度和风力的关系 # 查看某一个城市的风向跟风力的关系 plt.figure(figsize=(6,6)) axes1 = plt.subplot(2,1,1) axes1.scatter(milano['wind_deg'],milano['wind_speed']) axes1.set_title('deg-speed') axes2 = plt.subplot(2,1,2) axes2.scatter(milano['wind_deg'],milano['humidity']) axes2.set_title('deg-humidity') # 玫瑰图 plt.axes(polar=True) plt.bar(milano['wind_deg'],milano['wind_speed']) # 直方图 plt.bar(milano['wind_deg'],milano['wind_speed']) deg = milano['wind_deg'] speed = milano['wind_speed'] mean_speeds = [] for d in np.arange(0,360,45): mean_speeds.append(speed[(deg=d)].mean()) index = np.linspace(0,2*np.pi,8,endpoint=False) plt.figure(figsize=(6,6)) plt.axes(polar=True) colors = np.random.random(size=(8,3)) plt.bar(x=index,height=mean_speeds,color=colors,align='edge') plt.title('milano') # 湿度跟风向的关系 mean_hum = [] humidities = milano['humidity'] for d in np.arange(0,360,45): mean_hum.append(humidities[(deg=d)].mean()) plt.figure(figsize=(6,6)) plt.axes(polar=True) colors = np.random.random(size=(8,3)) plt.bar(x=index,height=mean_hum,color=colors,align='edge') plt.title('milano') 由于风向是360度，我们可以考虑使用玫瑰图（极坐标条形图） 首先自定义一个画图函数 用numpy创建一个直方图，将360度划分为8个面元，将数据分类到这8个面元中 计算米兰各个方向上的风速 将各个方向的风速保存在列表中 画出各个方向的风速 将上面步骤写成函数 def showbyrose(degs,others,title): # 求平均值（others） mean_others = [] for d in np.arange(0,360,45): mean_others.append(others[(degs=d)].mean()) plt.figure(figsize=(6,6)) plt.axes(polar=True) colors = np.random.random(size=(8,3)) plt.bar(x=index,height=mean_others,color=colors,align='edge') plt.title(title) showbyrose(ferrara['wind_deg'],ferrara['wind_speed'],'deg-speed') 4.人脸识别 import numpy as np import pandas as pd from pandas import Series,DataFrame from sklearn.datasets import fetch_lfw_people # 降维 pca from sklearn.decomposition import PCA # 集成学习 from sklearn.ensemble import VotingClassifier from sklearn.neighbors import KNeighborsClassifier from sklearn.linear_model import LogisticRegression from sklearn.tree import DecisionTreeClassifier from sklearn.svm import SVC from sklearn.model_selection import KFold from sklearn.model_selection import cross_val_score from sklearn.model_selection import GridSearchCV import matplotlib.pyplot as plt %matplotlib inline # 读取数据 faces = fetch_lfw_people(resize=0.4,min_faces_per_person=70) # 拆分数据 data = faces.data target = faces.target names = faces.target_names # 不涉及分类，使用pca pca = PCA(n_components=0.9) train = pca.fit_transform(data) # 集成学习 estimators = [] model1 = LogisticRegression() model2 = KNeighborsClassifier() model3 = DecisionTreeClassifier() model4 = SVC() estimators.append((\"LogisticRegression\",model1)) estimators.append((\"KNeighborsClassifier\",model2)) estimators.append((\"DecisionTreeClassifier\",model3)) estimators.append((\"SVC\",model4)) # 生成算法集成对象 enstimator = VotingClassifier(estimators) # 交叉验证(选择一个更加稳定的算法、其次看算法评分) kfold = KFold(n_splits=10) results = cross_val_score(enstimator,train,target,cv=kfold) # 使用svc测量 result1 = cross_val_score(SVC(kernel='linear'),train,target,cv=kfold) # 使用logistic测量 result2 = cross_val_score(LogisticRegression(),train,target,cv=kfold) # 使用knn测量 cross_val_score(KNeighborsClassifier(),train,target,cv=kfold) # 使用决策树测量 cross_val_score(DecisionTreeClassifier(),train,target,cv=kfold) # 通过比较可以选择logistic或者svc进行测量 # 对比logistci和svc哪一个更稳定 print(\"方差比--SVC:{} Logistic:{}\".format(result1.std(),result2.std())) print(\"平均分--SVC:{} Logistic:{}\".format(result1.mean(),result2.mean())) # 最终选择Logistic回归 # 网格搜索找到最优参数 param_dic = { 'C':[0.00001,0.001,0.1,10,100,1000,10000], 'penalty':['l1','l2'] } logistic = LogisticRegression() result = GridSearchCV(logistic,param_grid=param_dic) # 拆分数据集 from sklearn.model_selection import train_test_split # X_train,X_test,y_train,y_test = train_test_split(train,target,random_state=1) # 考虑到要展示预测结果，所以样本集的拆分，采用有序的方式进行 X_train = train[:1100] y_train = target[:1100] X_test = train[1100:] y_test = target[1100:] result.fit(X_train,y_train) best_logistic = result.best_estimator_ # 使用最好的模型来进行预测，查看评分 best_logistic.score(X_test,y_test) # 查看人脸图片 plt.imshow(data[0].reshape((50,37)),cmap='gray') name = names[target[0]] plt.title(name) # 展示预测结果（把前100张预测数据的真实值和预测值一起展示） true_test = data[1100:] true_target = target[1100:] predict_target = best_logistic.predict(X_test) for i,name in enumerate(names): names[i] = name.split(' ')[-1] # 最终展示 plt.figure(figsize=(16,24)) for i in range(100): axes = plt.subplot(10,10,i+1) data= true_test[i].reshape((50,37)) plt.imshow(data,cmap='gray') pre_name = names[predict_target[i]] true_name = names[true_target[i]] title = 'T:'+true_name + '\\nP:'+pre_name axes.set_title(title) axes.axis('off') # 处理网络图片，查看预测结果 bush = plt.imread('bush.jpg') bush = bush.max(axis=2) small_bush = bush[65:165,240:314] plt.imshow(small_bush,cmap='gray') from skimage.transform import resize # import scipy.misc as misc resize_bush = resize(small_bush,(50,37)) plt.imshow(resize_bush) bush_data = pca.transform(resize_bush.reshape(1,-1)) best_logistic.predict(bush_data) names[1] 5.病马死亡 import numpy as np import pandas as pd from pandas import Series,DataFrame import matplotlib.pyplot as plt %matplotlib inline # 读取数据 part1 = pd.read_table('./data/horseColicTraining.txt',header=None) part2 = pd.read_table('./data/horseColicTest.txt',header=None) # 先把所有数据级联到一起 samples = pd.concat((part1,part2)) # 拆分 train = samples.values[:,:-1] target = samples.values[:,-1] # 训练数据 from sklearn.preprocessing import StandardScaler,MinMaxScaler,Normalizer train1 = StandardScaler().fit_transform(train) train2 = MinMaxScaler().fit_transform(train) train3 = Normalizer().fit_transform(train) trains = [train1,train2,train3] feature_project_names = ['StandardScaler','MinMaxScaler','Normalizer'] from sklearn.model_selection import train_test_split # 预测不同的特征处理，对算法的影响 # 使用此函数找到一个好的特征处理方案 def score_with_model(model,trains,target,feature_project_names): for i,train in enumerate(trains): X_train,X_test,y_train,y_test = train_test_split(train,target,random_state=1) score = model.fit(X_train,y_train).score(X_test,y_test) print(\"{} 特征处理{} 得分:{}\".format(model.__class__.__name__,feature_project_names[i],score)) from sklearn.neighbors import KNeighborsClassifier from sklearn.linear_model import LogisticRegression from sklearn.svm import SVC from sklearn.tree import DecisionTreeClassifier score_with_model(KNeighborsClassifier(),trains,target,feature_project_names) score_with_model(LogisticRegression(),trains,target,feature_project_names) score_with_model(DecisionTreeClassifier(),trains,target,feature_project_names) score_with_model(SVC(),trains,target,feature_project_names) # 交叉验证，获得好的算法 # 先使用MinMaxScaler处理 from sklearn.model_selection import KFold from sklearn.model_selection import cross_val_score def select_best_model(model,train,target): kfold = KFold(n_splits=10) results = cross_val_score(model,train,target,cv=kfold) print(\"{}算法 平均值是{}，方差是{}，最大值{},最小值{}\".format(model.__class__.__name__,results.mean(),results.std(),results.max(),results.min())) print(results) select_best_model(KNeighborsClassifier(),train2,target) select_best_model(LogisticRegression(),train2,target) select_best_model(DecisionTreeClassifier(),train2,target) select_best_model(SVC(),train2,target) # 拆分样本集 X_train,X_test,y_train,y_test = train_test_split(train2,target,test_size=0.1) # 综合考虑，选择SVC算法，使用标准化对数据特征进行预处理 # 算法调参，使用GridSearchCV from sklearn.model_selection import GridSearchCV svc = SVC() param_dic = { 'kernel':['linear','rbf','poly'], 'C':[0.001,0.01,0.1,1,10,100], 'gamma':np.arange(0,100,10) } gridCV = GridSearchCV(svc,param_grid=param_dic) gridCV.fit(X_train,y_train) best_svc = gridCV.best_estimator_ best_svc.score(X_test,y_test) X_train,X_test,y_train,y_test = train_test_split(train,target,test_size=0.1) SVC(C=0.001,kernel='poly',gamma=10).fit(X_train,y_train).score(X_test,y_test) 后续 # 特征没选好，原始数据比特征处理过的数据表现要好 # 特征选择 # 降维 from sklearn.discriminant_analysis import LinearDiscriminantAnalysis from sklearn.model_selection import train_test_split from sklearn.decomposition import PCA # 因为要使用lda进行有监督的降维处理，所以，先使用pca查看大概有多少个特征起主导作用 train4 = PCA(n_components=0.97).fit_transform(train) X_train,X_test,y_train,y_test = train_test_split(train4,target,test_size=0.1) SVC(C=0.001,kernel='rbf',gamma=10).fit(X_train,y_train).score(X_test,y_test) X_train,X_test,y_train,y_test = train_test_split(train4,target,test_size=0.1) SVC().fit(X_train,y_train).score(X_test,y_test) Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"爬虫/10 分布式爬虫.html":{"url":"爬虫/10 分布式爬虫.html","title":"10 分布式爬虫","keywords":"","body":"10 分布式爬虫 10.1 分布式爬虫的优点 可以充分地利用多个电脑的资源。 10.2 如何实现分布式爬虫 scrapy本身支持分布式吗？ 不支持的！！！ 为什么不支持呢？ scrapy的url队列存在哪里？ （单机内存） 如何实现分布式呢？ 替换url队列的存储介质 （redis支持分布式的内存数据库） 为scrapy做一个新的调度器(redis)，替换scapy的默认调度器, 从而实现分布式功能。 10.3 scrapy-redis scrapy-redis是scrapy的一个组件（插件），和 scrapy 、redis配合。从而实现支持分布式爬虫 start_urls= ['http://www.dushu.com' ] # 以前 redis-cli lpush myspider:start_urls 'http://www.dushu.com' # 使用分布式 使用Scrapy框架+Scrapy-redis组件实现分布式爬虫 scrapy-redis是什么？ scrapy-redis是大神们写的一个scrapy的组件，主要用来负责分布式爬虫的调度任务它依赖于Scrapy和redis。 scrapy-redis提供了几个新的组件（新类）用来补充scrapy不能实现分布式的问题，它们分别是Scheduler、Dupefilter、Pipeline和Spider。 scrapy用类似Python中collection.deque的对象来保存待爬取的urls， scrapy-redis用redis数据库来保存待爬取的urls（redis支持分布式，支持队列共享） redis在爬虫系统中的作用： 存储链接 和scrapy一起工作，redis用来调度spiders（多个spider共用一个redis队列，即分布式处理） 充分利用redis结构的作用： set：set中没有重复元素，利用这个特性，可以用来过滤重复元素； list：实现队列和栈的功能； sorted set：元素需要排序的时候用； hash：存储的信息字段比较多时，可以用hash； 使用scrapy-redis实现分布式处理的步骤 创建项目 scrapy startproject example-project cd example-project scrapy genspider dmoz dmoz.org scrapy genspider myspider_redis dmoz.org scrapy genspider mycrawler_redis dmoz.org 编写代码 （这里我们直接使用官方网站的演示代码，演示分布式的配置、运行等） 使用scrapy redis，需要注意以下几个区别： 传统的spiders中，每个spider类继承的是scrapy.Spider类或Scrapy.spiders.CrawlSpider类，而分布式写法每个spider继承的是scrapy_redis.spiders.RedisSpider或scrapy_redis.spiders.RedisCrawlSpider类。 from scrapy_redis.spiders import RedisCrawlSpider class MyCrawler(RedisCrawlSpider): ...... 在分布式写法中，start_urls=[]换成了 redis_key = 'myspider:start_urls' class MyCrawler(RedisCrawlSpider): \"\"\"Spider that reads urls from redis queue (myspider:start_urls).\"\"\" name = 'mycrawler_redis' redis_key = 'mycrawler:start_urls' 在分布式写法中， allowed_domains = ['dmoz.org'] 换成了 domain = kwargs.pop('domain', '') self.allowed_domains = filter(None, domain.split(',')) 搭建分布式爬虫环境 环境准备：4台服务器，一个做master，另外3个做slave。 scrapy、scrapy-redis、redis master服务器的配置： 安装scrapy、scrapy-redis、redis。 修改master的redis配置文件redis.conf： 1）将 bind 127.0.0.1 修改为bind 0.0.0.0。（注意防火墙设置） 2）重启redis-server。 在爬虫项目中的setting.py文件中添加配置信息： REDIS_HOST = 'localhost' REDIS_PORT = 6379 slave端的配置： 在爬虫项目中的setting.py文件中添加配置信息： REDIS_URL = 'redis://your ip:6379' master和slave端中共同的配置 在setting.py中启用redis存储 ITEM_PIPELINES = { 'scrapy_redis.pipelines.RedisPipeline': 400, } 运行分布式爬虫 scrapy runspider myspider_redis.py redis-cli> lpush myspider_redis:start_urls [the url] 详细参考官方文档：https://github.com/rmax/scrapy-redis Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"爬虫/boss直聘.html":{"url":"爬虫/boss直聘.html","title":"boss直聘","keywords":"","body":"boss直聘 list页 字段 选择器 职位（job） .job-title 薪资(money) .red 公司名(company) .company-text>h3>a 工作年限(workyear) .info-primary>p 学历(xueli) .info-primary>p 公司规模(people) .company-text>p 公司背景(rongzi) .company-text>p 发布时间（time） .info-publis>p 详情(detail) div.info-primary > h3 > a 分布式爬虫 settings.py # settings.py SCHEDULER = \"scrapy_redis.scheduler.Scheduler\" DUPEFILTER_CLASS = \"scrapy_redis.dupefilter.RFPDupeFilter\" DOWNLOAD_DELAY = 5 CONCURRENT_REQUESTS = 1 DOWNLOADER_MIDDLEWARES = { 'boss.middlewares.BossDownloaderMiddleware': 543, 'boss.middlewares.RandomUserAgentMiddleware': 125, 'boss.middlewares.RandomProxyMiddleware': 345 } ITEM_PIPELINES = { 'scrapy_redis.pipelines.RedisPipeline': 400 } REDIS_URL = 'redis://127.0.0.1:6379' DUPEFILTER_CLASS = \"scrapy_redis.dupefilter.RFPDupeFilter\" SCHEDULER = \"scrapy_redis.scheduler.Scheduler\" SCHEDULER_PERSIST = True RANDOM_UA_TYPE= 'random' middleware.py class RandomUserAgentMiddleware(object): ''' 随机更换User-Agent ''' def __init__(self, crawler): super(RandomUserAgentMiddleware, self).__init__() self.ua = UserAgent() self.ua_type = crawler.settings.get('RANDOM_UA_TYPE', 'random') @classmethod def from_crawler(cls, crawler): return cls(crawler) def process_request(self, request, spider): def get_ua(): return getattr(self.ua, self.ua_type) request.headers.setdefault('User-Agent', get_ua()) class RandomProxyMiddleware(object): def process_request(self,request,spider): proxy = 'http://' + requests.get('http://zskin.xin:5010/get/').text request.meta['proxy'] = proxy Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"爬虫/csdn爬虫.html":{"url":"爬虫/csdn爬虫.html","title":"csdn爬虫","keywords":"","body":" 123 Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"爬虫/get.post请求及cookie.html":{"url":"爬虫/get.post请求及cookie.html","title":"get.post请求及cookie","keywords":"","body":"get请求 url = 'https://www.lagou.com' headers = { \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36\" } # 发起请求 req = urllib.request.Request(url, headers=headers) # with上下文管理读取内容 with urllib.request.urlopen(req) as response: html_doc = response.read() post请求 from urllib import request import urllib.parse url = 'http://10.31.161.89:8000/users/' values = { \"username\":\"shire\", \"password\":\"qianfeng\", \"mobile\":\"15129087058\" } # 对数据进行编码 data = urllib.parse.urlencode(values) data = data.encode('utf-8') # 获取返回的数据 req = request.Request(url, data=data) request.urlopen(req) 获取cookie 需要从http模块导入cookiejar from http import cookiejar from urllib import request def get_cookie(): # 创建实例 cookie = cookiejar.CookieJar() # 创建一个cookie处理器 cookie_processor = request.HTTPCookieProcessor(cookie) \"\"\" 创建一个opener 打开链接 获取保存在cookie中的数据 \"\"\" opener = request.build_opener(cookie_processor) response = opener.open('http://10.31.161.89:8000/xadmin/') print(cookie) if __name__ == '__main__': get_cookie() 结果： ]> 保存cookie def save_cookie(): cookie = cookiejar.MozillaCookieJar('cookie.txt') cookie_processor = request.HTTPCookieProcessor(cookie) opener = request.build_opener(cookie_processor) response = opener.open('http://www.baidu.com') cookie.save() 通过比对可以发现 保存cookie修改了创建实例 使用了 cookiejar.MozillaCookieJar('cookie.txt') 最后 cookie.save()来保存数据 加载cookie # 创建实例 cookie = cookiejar.MozillaCookieJar() # load cookie.load('cookie.txt') for item in cookie: print(item) 附件 快速上手request Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"爬虫/pyspider使用.html":{"url":"爬虫/pyspider使用.html","title":"pyspider使用","keywords":"","body":"pyspider使用 pyspider简介 PySpider 是一个我个人认为非常方便并且功能强大的爬虫框架，支持多线程爬取、JS动态解析，提供了可操作界面、出错重试、定时爬取等等的功能，使用非常人性化 安装 pip install pyspider 报错信息： 25555端口被使用 这是因为我之前先测试了phantomjs的使用 所以phantomjs已启动占用了25555端口 Mac 用户可以使用 lsof -i :25555查看pid 然后 kill -s 9 [查看的pid] 来杀死进程 ImportError: pycurl: libcurl link-time ssl backend (openssl) is different from compile-time ssl backend (none/other) pip uninstall pycurl export PYCURL_SSL_LIBRARY=openssl export LDFLAGS=-L/usr/local/opt/openssl/lib;export CPPFLAGS=-I/usr/local/opt/openssl/include;pip install pycurl --compile --no-cache-dir 正常使用 启动5000端口 create 这里我们用医院的接口来做示范 #!/usr/bin/env python # -*- encoding: utf-8 -*- # Created on 2017-03-15 16:22:50 # Project: 120_ask from pyspider.libs.base_handler import * class Handler(BaseHandler): crawl_config = { } @every(minutes=24 * 60) # 初始启动方法 def on_start(self): self.crawl('http://tag.120ask.com/jibing/pinyin/a', callback=self.index_page) @config(age=10 * 24 * 60 * 60) # 主页 def index_page(self, response): for each in response.doc('div.s_m1.m980 a').items(): self.crawl(each.attr.href, callback=self.list_page) # 循环遍历出列表页 def list_page(self, response): for each in response.doc('div.s_m2.m980 a').items(): self.crawl(each.attr.href, callback=self.detail_page) # 详情页 def detail_page(self, response): return { \"url\": response.url, \"disease_name\" : response.doc(\"div.LogoNav.m980.clears p a:nth-child(2)\").text(), \"guake\" : [x.text() for x in response.doc(\"div.p_rbox3 div.p_sibox3b span\").items()] } Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"爬虫/scrapy入门.html":{"url":"爬虫/scrapy入门.html","title":"scrapy入门","keywords":"","body":"scrapy入门 scrapy框架的组成 引擎 爬虫所有行为都由引擎来支配，类似于人的行为都由大脑支配一样 自动运行，无需关注，会自动组织所有的请求对象，分发给下载器 下载器 从引擎处获取到请求对象后，请求数据 spiders Spider类定义了如何爬取某个(或某些)网站。包括了爬取的动作(例如:是否跟进链接)以及如何从网页的内容中提取结构化数据(爬取item)。 换句话说，Spider就是您定义爬取的动作及分析某个网页(或者是有些网页)的地方。 调度器 有自己的调度规则，无需关注 管道 最终处理数据的管道，会预留接口供我们处理数据 当Item在Spider中被收集之后，它将会被传递到Item Pipeline，一些组件会按照一定的顺序执行对Item的处理。 每个item pipeline组件(有时称之为“Item Pipeline”)是实现了简单方法的Python类。他们接收到Item并通过它执行一些行为，同时也决定此Item是否继续通过pipeline，或是被丢弃而不再进行处理。 以下是item pipeline的一些典型应用： 清理HTML数据 验证爬取的数据(检查item包含某些字段) 查重(并丢弃) 将爬取结果保存到数据库中 安装 pip install scrapy 快速上手 scrapy startproject 项目名 cd 项目名 scrapy genspider app名 '爬取的网址' 以项目名firstPorject，网址www.baidu.com为例 # 例 scrapy startproject scrapyapp cd scrapyapp scrapy genspider baidu 'www.baidu.com' 此时的项目目录 目录结构 spiders 由我们自己创建，是实现爬虫核心功能的文件 注意，创建文件的路径不要搞错，应当在spiders文件夹内创建自定义的爬虫文件 items.py 定义数据结构的地方，是一个继承自scrapy.Item得类 Middleware,py 中间件 pipelines.py 管道文件 里面只有一个类，用于处理下载数据的后续处理 Settings.py 配置文件 比如： 是否遵守robots协议 User-Agent定义等 进入baidu.py文件 # -*- coding: utf-8 -*- # baidu.py import scrapy class BaiduSpider(scrapy.Spider): # 用于运行爬虫的爬虫名称 name = 'baidu' # 允许域名,只能爬取这个域名下的数据 allowed_domains = ['www.baidu.com'] # spider文件首次提交给引擎的url # 可以有多个url 可手动配置 start_urls = ['http://www.baidu.com/'] # 回调配置, 当下载器下载完一个请求对象的数据 def parse(self, response): pass 修改回调配置 # baidu.py def parse(self, response): items=[] # 解析当前的response数据 # print(response.text) # 返回一个Selector对象列表 title = response.xpath('//input[@id=\"su\"]/@value') # extract、extract_first()方法可以直接对列表提取内容 # extract_first() 如果被提取的selector列表为空，也不会报错 name = title.extract() name1 = title.extract_first() print(name,name1) # 如果不写返回值不会报错, 但无法使用管道 return items 修改settings.py配置 # ROBOTSTXT_OBEY改为False,表示不遵守网页robots.txt协议 ROBOTSTXT_OBEY = False # UA肯定要配置，如果不配置，爬虫文件无法执行 USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36' # 反爬速度限制,开启 DOWNLOAD_DELAY = 3 # 默认请求头。开启 DEFAULT_REQUEST_HEADERS = { 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8', 'Accept-Language': 'en', } 运行项目： 进入baidu.py所在目录 执行 $ scrapy crawl baidu 查看控制台输出 除了所需信息外，还有一些日志信息 如果只输出print信息, 执行 $ scrapy crawl baidu --nolog 日志 日志级别（从低到高） debug info warning error Critical 如果要执行 $ scrapy crawl baidu 不想输出日志 在settings.py文件中 添加 LOG_LEVEL = \"DEBUG\" # 将日志信息存储到log.log中 LOG_FILE = \"log.log\" Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"爬虫/scrapy实战.html":{"url":"爬虫/scrapy实战.html","title":"scrapy实战","keywords":"","body":"scrapy实战 糗事百科 创建项目 scrapy startproject qiubaiproject cd qiubai scrapy genspider qsbk Item.py # -*- coding: utf-8 -*- import scrapy class QiubaiprojectItem(scrapy.Item): # 统一了spider和管道之间数据格式的问题 # 只关注业务逻辑,要保存什么数据，就要创建什么属性，属性的创建方法是统一的 name = scrapy.Field() age = scrapy.Field() img_url = scrapy.Field() Qiubai.py # -*- coding: utf-8 -*- import scrapy class Qiubai2Spider(scrapy.Spider): name = 'qiubai' allowed_domains = ['www.qiushibaike.com'] start_urls = ['http://www.qiushibaike.com/'] # 应该返回一个值，解析的所有数据，必须返回一个可以迭代的对象 def parse(self, response): author_list = response.xpath('//div[starts-with(@class,\"author\")]') # print(author_list) items = [] for author_node in author_list: # Selector对象可以继续使用xpath进行解析操作 item = { 'name' : author_node.xpath('.//h2/text()').extract_first().strip('\\n'), 'age' : author_node.xpath('./div/text()').extract_first(), 'img_url' : 'http:' + author_node.xpath('.//img/@src').extract_first(), } # 字典的键必须跟items里面声明的键一致 items.append(item) # 不写不会报错，意味着你没有把数据提交给引擎，引擎也不会提交给管道，数据就不会被管道保存 return items Pipelines.py # -*- coding: utf-8 -*- # Define your item pipelines here # # Don't forget to add your pipeline to the ITEM_PIPELINES setting # See: https://doc.scrapy.org/en/latest/topics/item-pipeline.html import json # 管道就是一个对象，由piplines里面的类进行声明 # piplines里面可以声明多个管道类，每一个管道类可以负责处理一个业务逻辑 # 使用管道之前，必须进行settings里面的设置ITEM_PIPLINS class QiubaiprojectPipeline(object): # 蜘蛛开始爬取的时候自动调用 def open_spider(self, spider): self.fp = open('qiubai.json','w',encoding='utf-8') def close_spider(self, spider): self.fp.close() # 这是一个回调方法，是由引擎自动调用，引擎会把要存储的数据一个一个的通过item参数传递回来 # spider就是回传数据给引擎的那个蜘蛛对象 def process_item(self, item, spider): json_str = json.dumps(item,ensure_ascii=False) self.fp.write(json_str+'\\n') return item Settings.py(修改部分) # 添加 # DEBUG INFO WARNING ERROR CRITICAL # 高于设置级别的日志信息都会打印出来 # 日志级别 LOG_LEVEL = 'DEBUG' # 收集日志的日志文件，以.log结尾 LOG_FILE = 'qiubai.log' # 修改 USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36' ROBOTSTXT_OBEY = False DEFAULT_REQUEST_HEADERS = { 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8', # 'Accept-Language': 'en', } # 数值越大，优先级越低 ITEM_PIPELINES = { 'qiubaiproject.pipelines.QiubaiprojectPipeline': 300, } 修改完毕后，运行 $ scrapy crawl qiubai --nolog 效果图 Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"爬虫/selenium使用.html":{"url":"爬虫/selenium使用.html","title":"selenium使用","keywords":"","body":"selenium使用 下载 pip install selenium 快速开始 from selenium import webdriver browser = webdriver.Chrome() browser.get('http://www.baidu.com/') 运行之后就自动打开浏览器到百度页面 如果报错: selenium.common.exceptions.WebDriverException: Message: 'chromedriver' executable needs to be in PATH. 下载webdriver https://blog.csdn.net/cz9025/article/details/70160273 版本映射表 chromedriver版本 支持的Chrome版本 v2.39 v66-68 v2.38 v65-67 v2.37 v64-66 v2.36 v63-65 v2.35 v62-64 v2.34 v61-63 v2.33 v60-62 v2.32 v59-61 v2.31 v58-60 v2.30 v58-60 v2.29 v56-58 v2.28 v55-57 v2.27 v54-56 v2.26 v53-55 v2.25 v53-55 v2.24 v52-54 v2.23 v51-53 v2.22 v49-52 v2.21 v46-50 v2.20 v43-48 v2.19 v43-47 v2.18 v43-46 v2.17 v42-43 v2.13 v42-45 v2.15 v40-43 v2.14 v39-42 v2.13 v38-41 v2.12 v36-40 v2.11 v36-40 v2.10 v33-36 v2.9 v31-34 v2.8 v30-33 v2.7 v30-33 v2.6 v29-32 v2.5 v29-32 v2.4 v29-32 Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"爬虫/spider项目.html":{"url":"爬虫/spider项目.html","title":"spider项目","keywords":"","body":"spider 为练习方便 暂未对项目进行分类，后边如果做的多的话会进行资源整合 安装相关依赖模块 ```pip install -r requirements.txt** pip install -r requirements.txt ### 有道翻译`fanyi.py` 原文链接见有道翻译sign破解 目前只是中英文翻译 给出实例 返回结果 ### 煎蛋网妹子图 `jiandan.py` 原文链接见煎蛋网爬虫 原代码在输出的时候出现编码错误，经过笔者(就是我)的不懈努力 终于改好了 具体改动可对比代码 ### qq音乐 `qqmusic.py` 这个重点讲一下，因为自己做的(傲娇) 翻了很多博客，就不贴地址了 - 创建了一个qqMusic类（方便管理，增强可读性） - 定义两个方法*get_json*获取排行榜信息, *get_link*获取歌曲url - 整个爬取过程实际上就是模拟浏览器get请求获取json文件 大家在爬的过程中也会发现源码ul中没有内容 一般人到这就换下一家了 ， #### 但是 这才是重点，越难爬越有挑战才越有动力 #### 通过审查元素/F12的network直接点js抓包。 #### 可以发现所需要的内容都是在js返回的json文件中 那么你所要做的就是 #### 模拟request请求获取json文件 上图中查看headers你可以看到一些信息 1. headers请求头 2. params 请求数据（最下边） 3. url地址 有用的请求头包括`user-agent` `referer` params一些参数是固定不变的 ### 但是 在获取音乐链接的时候通过换音乐对比请求 发现有两个参数是变化的 就是`songmid` `filename` 仔细对比songmid, filename发现 ####filename的值也就比songmid多的前面的 C400 所以要解决的就只是 songmid songmid在第一个函数获取的内容中就有。 **完美!** 然后模拟请求获取 `vkey值` 这里再说一下。 vkey值是针对于你获取的音乐的专属key值 没有这个key值取不到音乐 ```Html http://dl.stream.qqmusic.qq.com/C400004RXylR0adkY1.m4a?vkey=FFED438561F460E854B12428B698E9966EFF9EF576D20BA50E85F59AA204E5CE10338EEF9011DEC7B477E3B4BE2F4902A7B9A3659C8C5193 &guid=9384442260 &uin=0 &fromtag=66 最终拼接url 到这里markdown就结束了 Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"爬虫/tinyproxy代理服务器的搭建.html":{"url":"爬虫/tinyproxy代理服务器的搭建.html","title":"tinyproxy代理服务器的搭建","keywords":"","body":"安装tinyproxy 安装 # centos sudo yum install tinyproxy # ubuntu sudo apt install tinyproxy 打开配置文件 vim /etc/tinyproxy/tinyproxy.conf 搜索并修改以下配置 # 注释掉这一行 # Allow 127.0.0.1 # 修改端口号 Port 1801 修改完了保存退出 重启服务： systemctl restart tinyproxy 日志文件的路径： /var/log/tinyproxy/tinyproxy.log Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"爬虫/tinyproxy定时任务.html":{"url":"爬虫/tinyproxy定时任务.html","title":"tinyproxy定时任务","keywords":"","body":"crontab -e # 初次使用选择vim # 退出 # 确认没有问题后再打开 systemctl restart tinyproxy crontab -e 在最后一行添加 */5 * * * * systemctl restart tinyproxy 保存退出 # 查看刚才添加的代码 crontab -l Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"爬虫/使用CrawlSpider做通用爬虫.html":{"url":"爬虫/使用CrawlSpider做通用爬虫.html","title":"使用CrawlSpider做通用爬虫","keywords":"","body":"使用CrawlSpider做通用爬虫 from scrapy.linkextractors import LinkExtractor LinkExtractor( # 允许 正则, 提取符合正则的链接 allow=\"\" # 排除 正则, 不提取符合正则的链接 deny=\"\" # 允许的域名 allow_domains=\"\" # 不允许的域名 deny_domains=\"\" # 提取符合xpath规则的链接 restrict_xpaths=(\"\") # 提取符合css规则的链接 restrict_css=(\"\") ) 示例: # -*- coding: utf-8 -*- import scrapy from scrapy.spiders import CrawlSpider,Rule # 提取链接 from scrapy.linkextractors import LinkExtractor class DsSpider(CrawlSpider): name = 'ds' allowed_domains = ['dushu.com'] start_urls = ['http://www.dushu.com/book/1158.html'] # 提取链接 可写正则 rules = ( Rule(LinkExtractor(allow=r'book/1158_\\d+\\.html', ), callback='parse_item', follow=False), ) def parse_item(self, response): # 提取页面所有的书籍信息 book_list = response.xpath('//h3').xpath('string(.)') for book in book_list: print(book.extract()) Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"爬虫/如何使你的爬虫更健壮.html":{"url":"爬虫/如何使你的爬虫更健壮.html","title":"如何使你的爬虫更健壮","keywords":"","body":"如何使你的爬虫更健壮 1. 防止url被过滤 # dont_filter: 本次请求不执行过滤重复url request = Request(url, dont_filter=True) 2.防止自定义的cookie被修改 当爬虫需要分页爬取时，携带cookie爬取，所携带的cookie可能会被scrapy中自带的httpCookie.py中的方法所修改，在保证不修改源码的情况下只需修改spider文件中的start_requests方法 # 禁止合并cookie request.meta['dont_merge_cookies'] = True spiders/xx.py def start_requests(self): \"\"\"重载scrapy.Spider类的start_requests函数，以设置meta信息\"\"\" for url in self.start_urls: # dont_filter：本次请求不执行过滤重复url的逻辑 request = Request(url, dont_filter=True) request.meta['dont_merge_cookies'] = True yield request 3. 分页 分页有多种实现方式，这里使用的是抓取当前页面显示的分页链接，然后进去爬取内容并继续返回新的链接，直到没有新的页面 # 当前页的链接列表 other_pages = response.xpath('//div[@class=\"page\"]/a/@href').extract() for page in other_pages: request = Request(page, callback=self.parse) # 不要合并cookies，这样可以使用settings里设置的cookies request.meta['dont_merge_cookies'] = True yield request # 也可以简写 yield from [response.follow(page) for page in other_pages] 4.将字符串转成int def conver_int(s): \"\"\"将字符串转换成int >>>conver_int(' 123') >>>123 >>>conver_int('') >>>0 >>>conver_int('123,456') >>>123456 \"\"\" if not s: return 0 return int(s.replace(',', '')) ci = conver_int # 使用时 ci(response.xpath('//span/text()').get()) 5.连接数据库存储数据 我们的需求是希望爬到的数据有重复就更新，没有就添加 而且尽可能的使代码优雅，表中字段过多时应想办法简化，不重复造轮子 当爬虫数据插入数据库时，实际上是在执行Insert Into语句 # 示例 insert into table (player_id,award_type,num) values(20001,0,1) on DUPLICATE key update player_id=20001,award_type=0,num=1; 显然，比较麻烦的是 1.字段名都得输出 2.键值一一对应 3.每个表都得重复此操作 所以可以把表名、键、值提取出来 # pipelines.py class MysqlPipeline(object): def __init__(self): # 连接 self.conn = None # 游标 self.cur = None def open_spider(self, spider): self.conn = pymysql.connect( host='127.0.0.1', port=3306, user='root', password='', db='数据库名', charset='utf8mb4', ) self.cur = self.conn.cursor() def process_item(self, item, spider): # 判断是否有table_name,这里的table_name是表名 if not hasattr(item, 'table_name'): return item # 键 cols = item.keys() # 值 values = list(item.values()) # 四个括号分别对应 表名、键的占位符、值的占位符%s、解析键 # 最后一个虽然解析了键 但又生成新的值的占位符%s sql = \"INSERT INTO `{}` ({})\"\\ \"VALUES ({}) ON DUPLICATE KEY UPDATE {}\".format( item.table_name, ','.join(['`%s`' % col for col in cols]), ','.join(['%s'] * len(cols)), ','.join('`{}`=%s'.format(col) for col in cols) ) # 解析值,有两个地方占位,所以values*2 self.cur.execute(sql, values * 2) self.conn.commit() print(self.cur._last_executed) return item def close_spider(self, spider): self.cur.close() self.conn.close() Items.py class PostItem(scrapy.Item): # 将每个class对应的表赋值给table_name,在pipelines中调用 table_name = 'posts' pid = Field() title = Field() 6.ip代理 ip代理最大的问题是检测ip的可用性 代码实现 middlewares.py # -*- coding: utf-8 -*- # Define here the models for your spider middleware # # See documentation in: # http://doc.scrapy.org/en/latest/topics/spider-middleware.html import random from scrapy.exceptions import NotConfigured class RandomProxyMiddleware(object): def __init__(self, settings): # 获取settings中的ip,每个ip初始状态为0 最大失败次数为3 self.proxies = settings.getlist('PROXIES') self.stats = {}.fromkeys(self.proxies, 0) self.max_failed = 3 @classmethod def from_crawler(cls, crawler): if not crawler.settings.getbool('HTTPPROXY_ENABLED'): raise NotConfigured return cls(crawler.settings) def process_request(self, request, spider): # ip池中随机选择 request.meta['proxy'] = random.choice(self.proxies) print('use proxy: %s' % request.meta['proxy']) def process_response(self, request, response, spider): # 用response.status的值来判断请求是否成功 # cur_proxy是请求时的ip cur_proxy = request.meta['proxy'] if response.status >= 400: self.stats[cur_proxy] += 1 print('get http status %s when use proxy: %s' % \\ (response.status, cur_proxy)) # 满足失败次数 从代理池中删除,remove_proxy方法在最下边定义 if self.stats[cur_proxy] >= self.max_failed: self.remove_proxy(cur_proxy) return response # 因为scrapy是异步并发,所以存在一个失败ip多次请求的情况,失败次数大于3次删除ip之后,其余请求还会返回来删除ip,此时就会报错 # 简单来说就是 两个人同时取一张卡中的钱,其中一个人取完了钱，另一边没有及时刷新就会报错 # 所以在此处将此ip的请求一并删除 def process_exception(self, request, exception, spider): cur_proxy = request.meta['proxy'] print('raise exption: %s when use %s' % (exception, cur_proxy)) self.remove_proxy(cur_proxy) del request.meta['proxy'] return request # 删除ip def remove_proxy(self, proxy): if proxy in self.proxies: self.proxies.remove(proxy) print('proxy %s removed from proxies list' % proxy) Settings.py 7.使用tinyproxy在自己的服务器开启ip代理 安装 # centos sudo yum install tinyproxy # ubuntu sudo apt install tinyproxy 打开配置文件 vim /etc/tinyproxy/tinyproxy.conf 搜索并修改以下配置 # 注释下面这行，大概在第210行 Allow 127.0.0.1 # 修改端口号 Port 1888 修改完保存退出 重启服务: systemctl restart tinyproxy 查看日志文件的路径 tail -f /var/log/tinyproxy/tinyproxy.log 8. 爬虫状态的持久化（断点续爬） scrapy crawl [spider] -s JOBDIR=crawls/somespider-1 python3.6 -m pip install requirements.txt Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"爬虫/常用工具、多线程及分布式.html":{"url":"爬虫/常用工具、多线程及分布式.html","title":"常用工具、多线程及分布式","keywords":"","body":"一. csrf模拟跨站请求 {% csrf %} 其中这个csrf其实就说csrf中间件做的登录验证中重要的一环 {%csrf%} == (input不会显示传输) 将登录时server端生成的token与csrf传的value值做比对 如果相等则登录 不相等则报403 所以为了安全 尽量不要注释/删除 csrfmidlleware哦 二. web常用工具 发送request urllib requests Aiohttp（异步io） 解析responese lxml bs4 re Cookie的处理 header信息的伪装 Scrapy 三. 模拟浏览器 Selenuim Webdriver 四. 多线程爬虫 from queue import Queue import threading import requests import re import redis start_url = \"http://www.geyanw.com\" # 队列,存放要爬取的url # urls_queue = Queue() # 注意:如果脚本在slaver端运行,host应指向Master的IP # 如果脚本在Master端运行,host直接写localhost即可. urls_queue = redis.Redis(host='要修改的', port=6379, db=0) # 并发下载线程数 DOWNLOADER_NUM = 2 # 线程池 thread_pool = [] def fetch(url): \"\"\" 根据url获取url对应的内容,并从网页中提取要爬取的url。 把提取的url put 到队列。 Args： url：要下载的页面的地址 Returns: text:网页的内容 \"\"\" try: r = requests.get(url) html = r.content text = html.decode('gb2312') # text = html.decode('ISO-8859-1') return text except Exception as e: print(e) else: # 检测http返回的状态码是否正常 r.raise_for_staus() def parse(html): \"\"\" 对一级页面进行解析，解析html源码中的内容。 Args： html：网页的源码, html的类型是str。 \"\"\" pattern = re.compile(r'href=\"(/[a-z0-9-/]+(.html)?)\"') urls = pattern.findall(html) for url in urls[:5]: print(url[0]) urls_queue.sadd('links', start_url + url[0]) # urls_queue.put(start_url + url[0]) def parse_detail(html): \"\"\" 解析详情页中的内容，从详情页中抽取数据。 Args: html:详情页的源代码。 \"\"\" from bs4 import BeautifulSoup soup = BeautifulSoup(html, \"lxml\") print(soup.text) def downloader(): \"\"\" 从url队列中提取url，下载url对应的页面。 每个url都是一个详情页的地址。 Returns: None \"\"\" # 不停地从url队列中取url，如果url不是None，下载url页面，并进行解析。 while True: # url = urls_queue.get() url = urls_queue.spop('links') if url is not None: print(url) html = fetch(url) parse_detail(html) def main(): \"\"\" 在主线程中初始化url队列。 根据DOWNLOAD_NUM的设置，启动多个线程，多个线程并发地从 url队列中获取url，执行url页面的下载。 \"\"\" # 主线程中初始化队列 # 初始化的操作只在Master端运行即可. # html = fetch(start_url) # parse(html) # 启动多个子线程 for i in range(DOWNLOADER_NUM): t = threading.Thread(target=downloader) t.start() thread_pool.append(t) # 阻塞队列，直到队列中没有任何url # urls_queue.join() # 阻塞线程，直到所有线程结束 for t in thread_pool: t.join() if __name__ == '__main__': main() 五. 分布式 什么是分布式 为什么使用？ 可以实现并行 如何搭建 一台电脑作为控制端（master）其他作为普通任务执行者(Slaver) 一台服务器开启redis 其余服务器去连接 除了host之外代码都相同 如何实现分布式爬虫 需要一个分布式队列 用来存放urls队列 六 附件 redis在windows ubuntu上安装配置测试 都配置好了多台服务器开启就行了 需要说明的是 mac下redis安装 brew install redis 注意master防火墙是否开启6379端口 Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"爬虫/异步IO.html":{"url":"爬虫/异步IO.html","title":"异步IO","keywords":"","body":"异步IO 背景: 同步io操作时 一个线程被挂起 其他任务皆等待，所以引出多线程多进程处理并发 线程不可能无止境增加 线程越多 cpu用在切换线程的时间越多 效率越低 所以为保证cpu执行能力和解决IO龟速不匹配 除了多线程多进程外 还有一种解决办法就是异步IO 消息模型是如何解决同步IO必须等待IO操作这一问题的？ 当遇到IO操作时，代码只负责发出IO请求，不等待IO结果，然后直接结束本轮消息处理，进入下一轮消息处理过程。当IO操作完成后，将收到一条“IO完成”的消息，处理该消息时就可以直接获取IO操作结果。 在“发出IO请求”到收到“IO完成”的这段时间里，同步IO模型下，主线程只能挂起，但异步IO模型下，主线程并没有休息，而是在消息循环中继续处理其他消息。这样，在异步IO模型下，一个线程就可以同时处理多个IO请求，并且没有切换线程的操作。对于大多数IO密集型的应用程序，使用异步IO将大大提升系统的多任务处理能力 协程的优势 最大的优势就是协程极高的执行效率。因为子程序切换不是线程切换，而是由程序自身控制，因此，没有线程切换的开销，和多线程比，线程数量越多，协程的性能优势就越明显。 第二大优势就是不需要多线程的锁机制，因为只有一个线程，也不存在同时写变量冲突，在协程中控制共享资源不加锁，只需要判断状态就好了，所以执行效率比多线程高很多。 因为协程是一个线程执行，那怎么利用多核CPU呢？最简单的方法是多进程+协程，既充分利用多核，又充分发挥协程的高效率，可获得极高的性能。 传统的生产者-消费者模型是一个线程写消息，一个线程取消息，通过锁机制控制队列和等待，但一不小心就可能死锁。 如果改用协程，生产者生产消息后，直接通过yield跳转到消费者开始执行，待消费者执行完毕后，切换回生产者继续生产，效率极高： def consumer(): r = '' while True: n = yield r if not n: return print('[CONSUMER] Consuming %s...' % n) r = '200 OK' def produce(c): c.send(None) n = 0 while n 执行结果： [PRODUCER] Producing 1... [CONSUMER] Consuming 1... [PRODUCER] Consumer return: 200 OK [PRODUCER] Producing 2... [CONSUMER] Consuming 2... [PRODUCER] Consumer return: 200 OK [PRODUCER] Producing 3... [CONSUMER] Consuming 3... [PRODUCER] Consumer return: 200 OK [PRODUCER] Producing 4... [CONSUMER] Consuming 4... [PRODUCER] Consumer return: 200 OK [PRODUCER] Producing 5... [CONSUMER] Consuming 5... [PRODUCER] Consumer return: 200 OK 注意到consumer函数是一个generator，把一个consumer传入produce后： 首先调用c.send(None)启动生成器； 然后，一旦生产了东西，通过c.send(n)切换到consumer执行； consumer通过yield拿到消息，处理，又通过yield把结果传回； produce拿到consumer处理的结果，继续生产下一条消息； produce决定不生产了，通过c.close()关闭consumer，整个过程结束。 整个流程无锁，由一个线程执行，produce和consumer协作完成任务，所以称为“协程”，而非线程的抢占式多任务。 最后套用Donald Knuth的一句话总结协程的特点： “子程序就是协程的一种特例。” Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"爬虫/爬虫常见问题.html":{"url":"爬虫/爬虫常见问题.html","title":"爬虫常见问题","keywords":"","body":"爬虫常见问题 Request回调没有反应 因为domain不匹配默认被过滤掉， 可在domain中加入url或在request请求中加入dont_filter=True 403响应 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023 [scrapy.core.engine] DEBUG: Crawled (403) (referer: None) ['partial'] [scrapy.core.engine] DEBUG: Crawled (403) (referer: None) ['partial'] [scrapy.spidermiddlewares.httperror] INFO: Ignoring response : HTTP status code is not handled or not allowed [scrapy.core.engine] INFO: Closing spider (finished) 如果手动访问该链接正常 首先检查UserAgent参数 可以将settings.py文件中的 # Crawl responsibly by identifying yourself (and your website) on the user-agent #USER_AGENT = 'zipru_scraper (+http://www.yourdomain.com)' 修改为 USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.95 Safari/537.36' 为了使我们的爬虫访问表现得更像人类的操作，让我们降低请求速率(原理上借助AutoThrottle 拓展)，在settings.py中继续添加 CONCURRENT_REQUESTS = 1 DOWNLOAD_DELAY = 5 302重定向 因为请求次数过多 所以有时会被重定向到其他页面 甚至会需要填验证码 对于爬虫代码当然不会填写验证 所以爬虫就此中断 我们可以定制redirect middleware（重定向中间件）,在遇到302时我们希望它绕过这些防范措施 为会话添加访问Cookie，最终请求到原始网页 # middlewares.py import os, tempfile, time, sys, logging logger = logging.getLogger(__name__) import dryscrape import pytesseract from PIL import Image from scrapy.downloadermiddlewares.redirect import RedirectMiddleware class ThreatDefenceRedirectMiddleware(RedirectMiddleware): def _redirect(self, redirected, request, spider, reason): # 如果没有特殊的防范性重定向那就正常工作 if not self.is_threat_defense_url(redirected.url): return super()._redirect(redirected, request, spider, reason) logger.debug(f'Zipru threat defense triggered for {request.url}') request.cookies = self.bypass_threat_defense(redirected.url) request.dont_filter = True # 防止原始链接被标记为重复链接 return request def is_threat_defense_url(self, url): return '://zipru.to/threat_defense.php' in url 我们继承了RedirectMiddleware中间件，使得我们可以复用内置的重定向处理操作，而只需要将我们的代码插入_redirect(redirected, request, spider, reason)方法，一旦构建了重定向请求，它会被process_response(request, response, spider)方法调用。对于非防范性重定向，我们调用父类的标准方法处理。这里，我们还未实现bypass_threat_defense(url)方法，但是它任务很明确，就是返回访问cookies，使得我们可以刷新原始请求的cookies，而重新处理原始请求。 为了启用我们的新中间件，要在zipru_scraper/settings.py中添加如下内容： DOWNLOADER_MIDDLEWARES = { 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware': None, 'zipru_scraper.middlewares.ThreatDefenceRedirectMiddleware': 600, } 它禁用了默认的重定向中间件，并将我们自己的中间件插入到相同的位置。此外，我们还需要安装一些需求包： pip install dryscrape # headless webkit 无头webkit pip install Pillow # image processing 图像处理 pip install pytesseract # OCR 字符识别 要注意，这些包都具有pip不能处理的外部依赖。如果安装出错，你需要访问dryscrape，Pillow以及 pytesseract来获取安装指导。 接下来，我们只需要实现bypass_thread_defense(url)。我们可以解析JavaScript来获取需要的变量，并在python中重建逻辑，不过那显得琐碎而麻烦。让我们选择笨拙而简单的做法，使用无头webkit实例。这有不少选择，不过我独爱dryscrape（之前安装的）。 首先，在我们的中间件构造器里初始化一个dryscrape会话。 def __init__(self, settings): super().__init__(settings) # start xvfb to support headless scraping if 'linux' in sys.platform: dryscrape.start_xvfb() self.dryscrape_session = dryscrape.Session(base_url='http://zipru.to') 你可以把这个会话当作一个浏览器标签，它会做所有浏览器通常所做的事（如获取外部资源，获取脚本）。我们可以在选项卡中导航到新的URL，点击按钮，输入文本以及做其它各类事务。Scrapy支持请求和项目处理的并发，但响应的处理是单线程的。这意味着我们可以使用这个单独的dryscrape会话，而不用担心线程安全。 现在我们来看一下绕过服务器防御的基本逻辑。 def bypass_threat_defense(self, url=None): # 有确实的url则访问 if url: self.dryscrape_session.visit(url) # 如果有验证码则处理 captcha_images = self.dryscrape_session.css('img[src *= captcha]') if len(captcha_images) > 0: return self.solve_captcha(captcha_images[0]) # 点击可能存在的重试链接 retry_links = self.dryscrape_session.css('a[href *= threat_defense]') if len(retry_links) > 0: return self.bypass_threat_defense(retry_links[0].get_attr('href')) # 否则的话，我们是在一个重定向页面上，等待重定向后再次尝试 self.wait_for_redirect() return self.bypass_threat_defense() def wait_for_redirect(self, url = None, wait = 0.1, timeout=10): url = url or self.dryscrape_session.url() for i in range(int(timeout//wait)): time.sleep(wait) # 如果url发生变化则返回 if self.dryscrape_session.url() != url: return self.dryscrape_session.url() logger.error(f'Maybe {self.dryscrape_session.url()} isn\\'t a redirect URL?') raise Exception('Timed out on the zipru redirect page.') 这里我们处理了在浏览器访问时可能遇到的各种情况，并且做了一个正常人类会做出的操作。任何时刻采取的行动取决于当前的页面，代码以一种优雅的方式顺序处理变化的情况。 验证码问题 最后一个谜题是解决验证码。有不少解决验证码服务的API供你在紧要关头使用，但是这里的验证码足够简单我们可以用OCR来解决它。使用pytesseract做字符识别，我们最终可以添加solve_captcha(img)方法来完善我们的bypass_threat_defense()。 def solve_captcha(self, img, width=1280, height=800): # 对当前页面截图 self.dryscrape_session.set_viewport_size(width, height) filename = tempfile.mktemp('.png') self.dryscrape_session.render(filename, width, height) # 注入javascript代码来找到验证码图片的边界 js = 'document.querySelector(\"img[src *= captcha]\").getBoundingClientRect()' rect = self.dryscrape_session.eval_script(js) box = (int(rect['left']), int(rect['top']), int(rect['right']), int(rect['bottom'])) # 解决截图中的验证码 image = Image.open(filename) os.unlink(filename) captcha_image = image.crop(box) captcha = pytesseract.image_to_string(captcha_image) logger.debug(f'Solved the Zipru captcha: \"{captcha}\"') # 提交验证码结果 input = self.dryscrape_session.xpath('//input[@id = \"solve_string\"]')[0] input.set(captcha) button = self.dryscrape_session.xpath('//button[@id = \"button_submit\"]')[0] url = self.dryscrape_session.url() button.click() # 如果我们被重定向到一个防御的URL，重试 if self.is_threat_defense_url(self.wait_for_redirect(url)): return self.bypass_threat_defense() # 否则就可以返回当前的cookies构成的字典 cookies = {} for cookie_string in self.dryscrape_session.cookies(): if 'domain=zipru.to' in cookie_string: key, value = cookie_string.split(';')[0].split('=') cookies[key] = value return cookies 可以看到，如果验证码解析失败，我们会回到bypass_threat_defense()。这样我们拥有多次尝试的机会，直到成功一次。 看起来我们的爬虫应该成功了，可是它陷入了无限循环中： [scrapy.core.engine] DEBUG: Crawled (200) (referer: None) [zipru_scraper.middlewares] DEBUG: Zipru threat defense triggered for http://zipru.to/torrents.php?category=TV [zipru_scraper.middlewares] DEBUG: Solved the Zipru captcha: \"UJM39\" [zipru_scraper.middlewares] DEBUG: Zipru threat defense triggered for http://zipru.to/torrents.php?category=TV [zipru_scraper.middlewares] DEBUG: Solved the Zipru captcha: \"TQ9OG\" [zipru_scraper.middlewares] DEBUG: Zipru threat defense triggered for http://zipru.to/torrents.php?category=TV [zipru_scraper.middlewares] DEBUG: Solved the Zipru captcha: \"KH9A8\" ... 看起来我们的中间件至少成功解决了验证码，然后重新发起请求。问题在于新的请求重又触发了防御机制。我一开以为bug在解析与添加cookies，可再三检查无果。这是另一个“唯一可能不同的东西是请求头”的情况。 Scrapy和dryscrape的请求头显然都绕过了触发403的第一层过滤器，因为我们没有得到任何403响应。但这肯定是因为某种请求头的差异造成的问题。我的猜测是，其中一个加密的访问Cookie包含了完整的原始访问请求头的哈希值，如果两次请求头不匹配，将触发威胁防御机制。这里的意图可能是防止某人直接将浏览器的cookies复制到爬虫中，但也只是增加了点小麻烦。 所以让我们在zipru_scraper/settings.py明确指定我们的请求头： DEFAULT_REQUEST_HEADERS = { 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8', 'User-Agent': USER_AGENT, 'Connection': 'Keep-Alive', 'Accept-Encoding': 'gzip, deflate', 'Accept-Language': 'en-US,*', } 注意这里，我们显式地使用之前定义的USER_AGENT赋值给User-Agent，虽然它已经被用户代理中间件自动添加，但是这样做会便于我们复制请求头到dryscrape中。下面修改我们的ThreatDefenceRedirectMiddleware的初始化函数为： def __init__(self, settings): super().__init__(settings) # start xvfb to support headless scraping if 'linux' in sys.platform: dryscrape.start_xvfb() self.dryscrape_session = dryscrape.Session(base_url='http://zipru.to') for key, value in settings['DEFAULT_REQUEST_HEADERS'].items(): # seems to be a bug with how webkit-server handles accept-encoding if key.lower() != 'accept-encoding': self.dryscrape_session.set_header(key, value) 现在scrapy crawl zipru -o torrents.jl命令行运行，成功了！数据流不断涌出！并且都记录到了我们的torrents.jl文件里。 Ip被封 小编之前在爬boss直聘时 ip就被封了 显示页面403，24小时后自动解封 这里可以用爬虫ip代理池 总结 解决了 Useragent过滤 模糊的js重定向 验证码 请求头一致性检查 {'company': '第伍区科技', 'job': 'Python 数字货币量化交易研发工程师', 'money': '15k-30k', 'people': [, , , , , , , , , , , , , , , , , , , , , , , , , , , , , ], 'rongzi': [, , , , , , , , , , , , , , , , , , , , , , , , , , , , , ], 'time': '发布于昨天', 'url': 'https://www.zhipin.com/job_detail/564a190c33d4a05f1XV509-4FlY~.html?ka=search_list_2_blank&lid=U596cSZTDJ.search', 'workyear': [, , , , , , , , , , , , , , , , , , , , , , , , , , , , , ], 'xueli': [, , , , , , , , , , , , , , , , , , , , , , , , , , , , , ]} Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"爬虫/爬虫进阶（入门）.html":{"url":"爬虫/爬虫进阶（入门）.html","title":"爬虫进阶（入门）","keywords":"","body":"爬虫进阶（入门） 爬虫的目的当然不仅仅是能下载图片网页视频等，大部分情况还是需要 获取数据,获取数据的话就需要对html，xml，json等文件进行处理 xpath选择器，BeautifulSoup来选取网页节点，进一步获取数据 requests库，代替urllib.request，用来请求、代理 他们的用法这里不做记录。 1. 代理 代理分类 透明（表面上是代理ip 实际上用的还是真实ip） 匿名（不会用真实的ip,知道是代理ip 但无法识别ip地址） 高匿（模拟浏览器，使用的是代理ip 以假乱真） 使用场景 爬取网站反爬机制会对ip进行限制（封ip） 使用 import urllib.request import urllib.parse # 配置代理对象，把协议作为键，主机和端口号为值 handler = urllib.request.ProxyHandler(proxies={'http':'183.33.192.247:9797'}) url = 'https://www.baidu.com/s?wd=ip' headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36' } request = urllib.request.Request(url=url,headers=headers) opener = urllib.request.build_opener(handler) response = opener.open(request) content = response.read().decode('utf-8') with open('proxy.html','w',encoding='utf-8') as fp: fp.write(content) 2. 工具 抓包工具 fiddler 直接去官网下载fiddler mac安装还需要下载mono 安装教程 mac启动 mono --arch=32 Fiddler.exe 注意：使用了一下发现在mac上很不友好，windows下好评还是很多的 mac下使用青花瓷 Charles 下载 安装配置 xpath helper Xpath插件的使用 打开Chrome浏览器的扩展程序（设置-更多工具-扩展程序） 把Xpath.crx拖拽到里面 把浏览器退出，重启 Ctrl+Shift+X打开xpath帮助工具（浏览器上面的黑色输入框） 按住shift,鼠标移动到网页内的任意元素位置上，可以自动定位该元素的xpath路径 正则替换 Sublime， alt+command+f打开替换，windows是ctrl+h 上边输入(.*?):\\s(.*) 下边输入\"\\1\":\"\\2\"就可以替换为json正常格式> 3.实战 站长素材(path) import urllib.request import urllib.parse from lxml import etree import os # 作业：实现站长素材不同类型的多个分页的图片抓取 def get_url(page): if page == 1: url = 'http://sc.chinaz.com/tupian/shuaigetupian.html' else: url = 'http://sc.chinaz.com/tupian/shuaigetupian_{}.html'.format(page) return url def request_data(url): handler = urllib.request.HTTPHandler() opener = urllib.request.build_opener(handler) headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36' } request = urllib.request.Request(url=url,headers=headers) response = opener.open(request) return response def parse_data(response): html_str = response.read().decode('utf-8') # etree.parse() tree = etree.HTML(html_str) img_xpath = '//div[@id=\"container\"]/div//img' img_nodes = tree.xpath(img_xpath) items = [] for img_node in img_nodes: src = img_node.xpath('./@src2')[0] alt = img_node.xpath('./@alt')[0] item = { 'src':src, 'alt':alt } items.append(item) return items def download_data(items): for item in items: src = item['src'] alt = item['alt'] suffix = src.split('.')[-1] file_name = alt+'.'+suffix file_path = os.path.join('shuaige',file_name) urllib.request.urlretrieve(src,file_path) print(file_name+'下载完成') print('全部下载完成') if __name__ == '__main__': page = int(input('请输入要查询的页码：')) url = get_url(page) response = request_data(url) items = parse_data(response) download_data(items) 股票（BeautifulSoup） import urllib.request import urllib.parse from lxml import etree from bs4 import BeautifulSoup url = 'http://quote.stockstar.com/' headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36' } request = urllib.request.Request(url=url,headers=headers) response = urllib.request.urlopen(request) content = response.read().decode('gb2312') # tree = etree.HTML(content) soup = BeautifulSoup(content,'lxml') # 使用BeatifullSoup定位tr_list tr_list = soup.select('#datalist tr') # 使用BeatifullSoup for tr in tr_list: td_list = tr.find_all('td') for td in td_list: # 如果string不行，还get_text()一定行 print(td.string) 智联招聘(BeautifulSoup) import urllib.request import urllib.parse from bs4 import BeautifulSoup from Item import GSXX import json def request_data(url): headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36' } data = { 'jl':'北京', 'kw':'python', 'p':1, } data = urllib.parse.urlencode(data) url = url + data request = urllib.request.Request(url=url,headers=headers) response = urllib.request.urlopen(request) return response def parse_data(response): content = response.read().decode('utf-8') soup = BeautifulSoup(content,'lxml') table_list = soup.select('#newlist_list_content_table table')[1:] items = [] for table in table_list: zwmc = table.select('.zwmc')[0].get_text().strip('\\n') gsmc = table.select('.gsmc')[0].get_text().strip('\\n') zwyx = table.select('.zwyx')[0].get_text().strip('\\n') gzdd = table.select('.gzdd')[0].get_text().strip('\\n') item = GSXX(zwmc,gsmc,zwyx,gzdd) items.append(item) return items def transform_data(item): return {name:item.name} def save_data(items): for item in items: json_obj = item.__dict__ json.dump(json_obj,open('zhilian.json','a',encoding='utf-8'),ensure_ascii=False) def study_json(): # 关于json文件操作的几个方法 # pickle模块 序列化操作 # json.load() 从json文件中读取json对象 # json.loads() 把一个字符串对象读取成一个json对象 # json.dump() 把一个json对象写入到文件中 # json.dumps() 把一个json对象读取成字符串对象 dic = { 'name':'dancer', 'age':10 } json_str = json.dumps(dic) print(type(json_str)) json.dump(dic,open('test.json','w',encoding='utf-8')) json_obj = json.load(open('test.json','r',encoding='utf-8')) print(json_obj,type(json_obj)) json_obj2 = json.loads(json_str,encoding='utf-8') print(type(json_obj2),json_obj2) if __name__ == '__main__': # 带参数的GET请求 url = 'http://sou.zhaopin.com/jobs/searchresult.ashx?' response = request_data(url) items = parse_data(response) save_data(items) Selenuim使用 Selemuim 模拟浏览器行为 pantormJS 阳光宽频网视频下载 import requests from lxml import etree import json from selenium import webdriver import time def handle_video(name, url): headers = { 'User-Agent': 'Mozilla/5.0(Macintosh;IntelMacOSX10.6;rv:2.0.1)Gecko/20100101Firefox/4.0.1' } # r = requests.get(url=url, headers=headers) # 在这又不对了，因为video标签是通过js动态加载进来的 # html_tree = etree.HTML(r.text) # video_src = html_tree.xpath('//video[@class=\"vjs-tech\"]') # print(video_src) # 发现数据也是动态添加进来的，所以这个时候使用phantomjs和selenium请求页面，执行其中的js代码，将真正的数据展示给你即可，然后再去解析它 # 创建浏览器对象 driver = webdriver.PhantomJS('phantomjs.exe') # 向指定地址发送请求 driver.get(url) # 让程序稍微休眠一下，去执行里面的js代码 time.sleep(5) # 【注】得到网页源代码,此时page_source是执行js之后的网页代码 html_tree = etree.HTML(driver.page_source) video_src = html_tree.xpath('//video[@class=\"vjs-tech\"]/source/@src')[0] # print(video_src) # with open('1.html', 'w', encoding='utf-8') as fp: # fp.write(driver.page_source) r = requests.get(url=video_src, headers=headers) filename = './video/' + name + '.mp4' with open(filename, 'wb') as fp: fp.write(r.content) print(filename + '下载完毕') def handle_index(url, json_url): headers = { 'User-Agent': 'Mozilla/5.0(Macintosh;IntelMacOSX10.6;rv:2.0.1)Gecko/20100101Firefox/4.0.1' } r = requests.get(url=json_url, headers=headers) # 是否会将json格式的字符串转化为python对象 # print(r.text) obj = json.loads(r.text) # print(len(obj['data'])) print('开始下载。。。。。。。') for video_obj in obj['data']: # 获取视频名字 name = video_obj['video_id'] # 获取视频连接 video_url = 'https://365yg.com' + video_obj['source_url'] # print(name, video_url) # 获取视频连接，并且下载视频 handle_video(name, video_url) print('结束下载') # 如下方式是不对的，因为内容是通过js动态添加的，所以我们需要抓包，捕获数据接口 # 通过xpath获取所有符合要求的a连接 # html_tree = etree.HTML(r.text) # a_list = html_tree.xpath('//div[@class=\"title-box\"]') # print(a_list) def main(): # 首页的url url = 'https://365yg.com/' json_url = 'https://365yg.com/api/pc/feed/?category=video&utm_source=toutiao&widen=3&max_behot_time=0&max_behot_time_tmp=0&tadrequire=true&as=A1253AF48A0FCE5&cp=5A4A4F4CAEE59E1&_signature=' # 得到首页的内容，获取里面所有的符合条件的a标签的href handle_index(url, json_url) if __name__ == '__main__': main() 阳光视频下载 from selenium import webdriver from handless import share_browser import time from lxml import etree import os import requests shouye_url = \"http://www.365yg.com/\" driver = share_browser() driver.get(shouye_url) time.sleep(3) driver.execute_script(\"document.documentElement.scrollTop=1000000\") time.sleep(3) detail_xpath = '//a[@ga_event=\"video_comment_click\"]/@href' tree = etree.HTML(driver.page_source) detail_list = tree.xpath(detail_xpath) for detail in detail_list: detail_url = 'http://www.365yg.com' + detail driver.get(detail_url) time.sleep(1) tree1 = etree.HTML(driver.page_source) src_xpath = '//video/@src' src = tree1.xpath(src_xpath)[0] name = tree1.xpath('//h2/text()')[0] if not src.startswith(\"http:\"): src = 'http:' + src file_path = 'ygvideo' file_name = name + '.mp4' path = os.path.join(file_path,file_name) r = requests.get(src) with open(path,'wb') as fp: fp.write(r.content) 验证码识别 利用第三方平台集成环境 云打码 注册开发者账号 登录并创建软件 记住软件id,key 下载python调试文档 将两个dll文件及py文件放到项目目录下 YDpython3.x.py # -*- coding: cp936 -*- import sys import os from ctypes import * class Recgonizier(): def __init__(self): # print('>>>正在初始化...') self.YDMApi = windll.LoadLibrary('yundamaAPI') self.appId = '软件id' self.appKey = b'软件key' # print('软件ＩＤ：%d\\r\\n软件密钥：%s' % (self.appId, self.appKey)) self.username = b'普通账号' self.password = b'密码' # 第一步：初始化云打码，只需调用一次即可 self.YDMApi.YDM_SetAppInfo(self.appId, self.appKey) # 第二步：登陆云打码账号，只需调用一次即可 self.uid = self.YDMApi.YDM_Login(self.username, self.password) if self.username == b'test': exit('\\r\\n>>>请先设置用户名密码') ####################### 一键识别函数 YDM_EasyDecodeByPath ####################### def esay_recognition(self, file_path): # 例：1004表示4位字母数字，不同类型收费不同。请准确填写，否则影响识别率。在此查询所有类型 http://www.yundama.com/price.html codetype = 1004 # 分配30个字节存放识别结果 result = c_char_p(b\" \") # 识别超时时间 单位：秒 timeout = 60 # 验证码文件路径 file_path = file_path.encode('utf-8') filename = file_path # 一键识别函数，无需调用 YDM_SetAppInfo 和 YDM_Login，适合脚本调用 captchaId = self.YDMApi.YDM_EasyDecodeByPath(self.username, self.password, self.appId, self.appKey, filename, codetype, timeout, result) # print(\"一键识别：验证码ID：%d，识别结果：%s\" % (captchaId, result.value)) return result.value ########################## 普通识别函数 YDM_DecodeByPath ######################### def normal_recognition(self, file_path): print('\\r\\n>>>正在登陆...') if uid > 0: balance = self.YDMApi.YDM_GetBalance(self.username, self.password) print('\\r\\n>>>正在普通识别...') codetype = 1004 result = c_char_p(b\" \") file_path = file_path.encode('utf-8') filename = file_path # 普通识别函数，需先调用 YDM_SetAppInfo 和 YDM_Login 初始化 captchaId = YDMApi.YDM_DecodeByPath(filename, codetype, result) return result.value 古诗文网站 import requests from bs4 import BeautifulSoup from RecognizerFile import Recgonizier # 请求这个页面，获取验证码图片 login_url = 'https://so.gushiwen.org/user/login.aspx?from=http://so.gushiwen.org/user/collect.aspx' # 处理登录 post_url = 'https://so.gushiwen.org/user/login.aspx?from=http%3a%2f%2fso.gushiwen.org%2fuser%2fcollect.aspx' headers = { 'user-agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36' } s = requests.Session() response = s.get(login_url,headers=headers) response.encoding='utf-8' soup = BeautifulSoup(response.text,'lxml') img_url = 'https://so.gushiwen.org' + soup.select('#imgCode')[0].attrs.get('src') # 注意要使用绘画请求图片 img_response = s.get(img_url) with open('getCode.jpg','wb') as fp: fp.write(img_response.content) r = Recgonizier() code = r.esay_recognition('getCode.jpg') print('验证码识别结果'+str(code)) a = soup.select('#__VIEWSTATE')[0].attrs.get('value') b = soup.select('#__VIEWSTATEGENERATOR')[0].attrs.get('value') print(a,b) data = { '__VIEWSTATE': a, '__VIEWSTATEGENERATOR': b, 'from': 'http://so.gushiwen.org/user/collect.aspx', 'email': '邮箱@qq.com', 'pwd': '密码', 'code': code, 'denglu': '登录' } response = s.post(url=post_url,data=data,headers=headers) response.encoding='utf-8' with open('gushi.html','w',encoding='utf-8') as fp: fp.write(response.text) 4. 模拟登录 QQmail from selenium import webdriver from selenium.webdriver.common.keys import Keys import time # 模拟登陆qq邮箱 driver = webdriver.Chrome() driver.get(\"https://mail.qq.com/\") time.sleep(5) # 切换iframe driver.switch_to.frame(\"login_frame\") # 用户名 密码 driver.find_element_by_name(\"u\").send_keys(\"******@qq.com\") driver.find_element_by_name(\"p\").send_keys(\"your password\") time.sleep(5) driver.find_element_by_id(\"login_button\").click() # driver.find_element_by_id(\"login_button\")send_keys(Keys.RETURN) time.sleep(5) print(driver.get_cookies()) driver.save_screenshot('qq.png') driver.quit() 知乎 from selenium import webdriver from selenium.webdriver.chrome.options import Options from selenium.webdriver.common.keys import Keys import time #模拟登录知乎 driver=webdriver.Chrome() url='https://www.zhihu.com/signin' driver.get(url) driver.find_element_by_name('username').send_keys('189********') time.sleep(3) driver.find_element_by_name('password').send_keys('your password') time.sleep(3) driver.find_element_by_xpath('//form/button').click() driver.save_screenshot('zhihu.png') print(driver.get_cookies()) driver.quit() Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"爬虫/简单爬虫.html":{"url":"爬虫/简单爬虫.html","title":"简单爬虫","keywords":"","body":"1. 简单爬虫（小白） 针对一些反爬虫机制不是很成熟甚至没有反爬的网站 可以用来练手 用到的库 urllib json:处理json 网页请求 get post ajax 模拟登陆 异常捕获 HTTPError URLError 简单案例： 百度翻译 get及post请求 import urllib.parse import urllib.request import json url = 'http://fanyi.baidu.com/?aldtype=16047#en/zh/' # 模拟浏览器 headers = { 'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2626.76 Safari/537.36' } # 简单get请求 # response=urllib.request.urlopen(url=url) # # content = response.read().decode('utf-8') # # 对返回的数据解析 # # xpath/正则/BeautifulSoup # print(content) # 简单post请求 post_url = 'http://fanyi.baidu.com/sug' # 处理post请求, 表单通过data参数进行设置 data = { 'kw':'baby' } # post请求的data数据必须经过urldecode编码 data = urllib.parse.urlencode(data).encode('utf-8') # 伪装浏览器访问 request = urllib.request.Request(url=post_url,data=data,headers=headers) response = urllib.request.urlopen(request) content = response.read().decode('utf-8') json_str = json.loads(content,encoding='utf-8') print(json_str) # 将请求到的数据保存到result.json # 推荐用with上下文管理，此处不做分析 json.dump(json_str, open('result.json','w',encoding='utf-8'),ensure_ascii=False) 图片、视频、网页 import urllib.request # 使用urlretriev 保存一个网页 urllib.request.urlretrieve(url,'baidu.html') # 图片 img_url = 'http://img4.imgtn.bdimg.com/it/u=2813434517,83142011&fm=200&gp=0.jpg' urllib.request.urlretrieve(img_url,'love.jpg') # 视频下载 vedio_url = 'http://vliveachy.tc.qq.com/om.tc.qq.com/Az8VBMvmOaZ0-uMvijuncOt2WA-hjsfSbUUQjtYZv4Oo/j06552sg6he.p712.1.mp4?sdtfrom=v1105&guid=dabdf0bb81db817f535700efd44a086c&vkey=8722EBFA35A66E250EF94BA72CFA5FC0776AAA68D4855DA156731D9A83B1B6CA3B664F810509D918F5BD577EDFB07C612A8F2760966F776A10554826996CB3A0FBBE8EA6B17450FF173DA43B607B14A3DE1EB9B1EF12D93A1526FE7F8D6571B06C534E987D251DAB8B2B2B7F6D903997DD1D8AC267D77FEF&ocid=2603685292&ocid=760288684' urllib.request.urlretrieve(vedio_url,'vedio.mp4') # 保存网页 url='http://www.baidu.com' response = urllib.request.urlopen(url=url) content = response.read().decode('utf-8') with open('baidu2.html','w',encoding='utf-8') as fp: fp.write(content) KFC(ajax请求) import urllib.request import urllib.parse import json post_url = 'http://www.kfc.com.cn/kfccda/ashx/GetStoreList.ashx?op=cname' # 需求：根据用户输入的页码的起始范围，把每一页的餐厅信息存储为一个独立的json文件 start_page = int(input('请输入起始页')) end_page = int(input('请输入结束页')) headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36', } for page in range(start_page,end_page+1): p = str(page) data = { 'cname': '北京', 'pid': '', 'pageIndex': p, 'pageSize': '10', } data = urllib.parse.urlencode(data).encode('utf-8') request = urllib.request.Request(url=post_url,data=data,headers=headers) response = urllib.request.urlopen(request) content = response.read().decode('utf-8') # 保存为本地json文件 filename = 'KFC/{}.json'.format(p) json.dump(content,open(filename,'w',encoding='utf-8'),ensure_ascii=False) 百度贴吧(复杂get请求) import urllib.request import urllib.parse import os def create_request(barname,page,base_url): headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36' } p = (page-1)*50 data = { 'kw': barname, 'pn': p } data = urllib.parse.urlencode(data) url = base_url + data request = urllib.request.Request(url=url, headers=headers) return request def request_data(request): return urllib.request.urlopen(request) def parse_response(response): return response.read().decode('utf-8') def save_data(content,file_path,file_name): path = os.path.join(file_path,file_name) try: with open(path,'w',encoding='utf-8') as fp: fp.write(content) except Exception as e: print(e) else: print(file_name + '保存成功') if __name__ == '__main__': base_url = 'http://tieba.baidu.com/f?' barname = input('请输入要查询的吧名') start_page = int(input('请输入起始页')) end_page = int(input('请输入结束页')) for page in range(start_page,end_page+1): # 创建一个请求对象 request = create_request(barname,page,base_url) # 处理请求任务 response = request_data(request) # 数据解析 content = parse_response(response) # 实现本地化存储 file_name = str(page) + '.html' file_path = 'tieba' save_data(content,file_path,file_name) print('全部保存成功') 糗百 import urllib.request import urllib.parse import re import os # 请求数据 def request_data(): url = 'https://www.qiushibaike.com/' headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36' } request = urllib.request.Request(url=url,headers=headers) response = urllib.request.urlopen(request) content = response.read().decode('utf-8') return content # 解析数据 def parse_data(content): pp = r'.*?' pattern = re.compile(pp, re.S) src_list = pattern.findall(content) return src_list # 本地保存 def save_data(data_list,sava_path): for data in data_list: if data.startswith(\"http:\"): pass else: data = 'http:' + data file_name = data.split(\"/\")[-1] file_path = os.path.join(sava_path,file_name) urllib.request.urlretrieve(data,file_path) print(file_name+'保存完毕') print('全部下载完成') if __name__ == '__main__': # 请求数据 content = request_data() # 解析数据 data_list = parse_data(content) # 保存数据 save_data(data_list,'qiubai') 异常捕获/处理 对爬虫请求时的一些常见错误进行处理 保证代码能正常执行 import urllib.request from urllib.error import HTTPError,URLError url = \"http://maoyan.com/films1\" try: response = urllib.request.urlopen(url) content = response.read().decode('utf-8') print(content) except HTTPError as e: print('HTTPError') print(e) except URLError as e: print('URLError') print(e) except Exception as e: print('未知异常') print(e) else: print('ok') try: fp = open('1.txt','r',encoding='utf-8') fp.write('hahahaha') except Exception as e: print(e) 2. Cookie登录 cookie简介 cookie是存储在本地终端的一种会话机制，它可以用来记录用户的行为，同时也可以作为一种反爬的手段 思考： 在爬取页面时，需要你登录，当我们把cookie复制到headers里，就可以爬取网页,但我们不能每次都去用抓包复制cooke，这样失去了爬虫的意义. 在这里我们可以使用handler处理 handler handler是一个类，提供HTTPHandler\\HTTPCookieProcesser\\HTTPProxyHandler 结论：如果需要在请求的过程中配置cookie或者代理,就需要使用opener来处理请求 from http.cookiejar import CookieJar # 首先创建一个cookiejar对象，用于自动保存服务器向浏览器写入的cookie信息 cookie = CookieJar() # 1.基本的HTTPHandler的处理方式 # 构建handler对象 handler = urllib.request.HTTPHandler() # 使用handler对象构建opener对象 opener = urllib.request.build_opener(handler) # 使用opener发送请求任务 response = opener.open(request) # urllib.request.urlopen() # open可以自动保存一些浏览器信息 content = response.read().decode('utf-8') 人人网 import urllib.request import urllib.parse from http.cookiejar import CookieJar # 首先创建一个cookiejar对象，用于自动保存服务器向浏览器写入的cookie信息 cookie = CookieJar() # 使用HTTPCookieProcessor类来创建handler对象 handler = urllib.request.HTTPCookieProcessor(cookie) # 每一个人人网的请求，都使用opener.open来处理,因为opener里面包含了cookie信息 opener = urllib.request.build_opener(handler) headers = { \"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36\", } # 先处理一下用户登录 post_url = 'http://www.renren.com/ajaxLogin/login?1=1&uniqueTimestamp=201842926368' data = { # 需要配置你自己的账号信息 \"email\":\"dqsygcz@126.com\", \"origURL\":\"http://www.renren.com/home\", \"domain\":\"renren.com\", \"key_id\":\"1\", \"captcha_type\":\"web_login\", \"password\":\"bc95dbb4e9cfbd8f3b84e6cd1820d210bf6953f4d70630f73a750369c166e61d\", \"rkey\":\"932a8efd469514758b8787a3631dd539\", } data = urllib.parse.urlencode(data).encode('utf-8') post_request = urllib.request.Request(url=post_url,data=data,headers=headers) # 为了记录cookie信息 opener.open(post_request) url = 'http://www.renren.com/224549540/profile' request = urllib.request.Request(url=url,headers=headers) # 在没有登陆的前提下，使用openr查看个人信息，查看结果 # 发现并没有访问成功，说明cookie信息并没有保存 response = opener.open(request) content = response.read().decode('utf-8') with open('xiaonei2.html','w',encoding='utf-8') as fp: fp.write(content) Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"爬虫/线程锁.html":{"url":"爬虫/线程锁.html","title":"线程锁","keywords":"","body":"线程锁 线程锁真的好麻烦啊！！！ 找了几篇博客发现写的都不一样 相关联内容太多不容易理解 所以现在需要理清 什么是线程锁 应用场景 怎么用 优点 1. 什么是线程锁机制 多线程可以同时运行多个任务 但是当多个线程同时访问共享数据时，可能导致数据不同步，甚至错误！ so,不使用线程锁, 可能导致错误 啰嗦两句： 比如你在银行取钱的同时你女朋友用支付宝取钱 不同线程同时访问同一资源 如果资源不加锁可能会导致银行亏本 卡里有100却取出200这种错误 2. 应用场景 I/O密集型操作 需要资源保持同步 3.用法 mutex = threading.Lock() #锁的使用 #创建锁 mutex = threading.Lock() #锁定 mutex.acquire([timeout]) #释放 mutex.release() example import threading import time counter = 0 counter_lock = threading.Lock() #只是定义一个锁,并不是给资源加锁,你可以定义多个锁,像下两行代码,当你需要占用这个资源时，任何一个锁都可以锁这个资源 counter_lock2 = threading.Lock() counter_lock3 = threading.Lock() #可以使用上边三个锁的任何一个来锁定资源 class MyThread(threading.Thread):#使用类定义thread，继承threading.Thread def __init__(self,name): threading.Thread.__init__(self) self.name = \"Thread-\" + str(name) def run(self): #run函数必须实现 global counter,counter_lock #多线程是共享资源的，使用全局变量 time.sleep(1); if counter_lock.acquire(): #当需要独占counter资源时，必须先锁定，这个锁可以是任意的一个锁，可以使用上边定义的3个锁中的任意一个 counter += 1 print \"I am %s, set counter:%s\" % (self.name,counter) counter_lock.release() #使用完counter资源必须要将这个锁打开，让其他线程使用 if __name__ == \"__main__\": for i in xrange(1,101): my_thread = MyThread(i) my_thread.start() 4.优点 保证资源同步 5.扩展（死锁与递归锁） 如果多个线程要调用多个对象，而A线程调用A锁占用了A对象，B线程调用了B锁占用了B对象,A线程不能调用B对象，B线程不能调用A对象，于是一直等待。这就造成了线程“死锁”。 Threading模块中，也有一个类，RLock，称之为可重入锁。该锁对象内部维护着一个Lock和一个counter对象。counter对象记录了acquire的次数，使得资源可以被多次require。最后，当所有RLock被release后，其他线程才能获取资源。在同一个线程中，RLock.acquire可以被多次调用，利用该特性，可以解决部分死锁问题。 就是把 lock=threading.Lock()改成lock = threading.RLock() Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"爬虫/进程线程.html":{"url":"爬虫/进程线程.html","title":"进程线程","keywords":"","body":"线程进程协程 1. 什么是同步，异步，并行和并发？ 同步：在发出一个同步调用时，在没有得到结果之前，该调用就不返回。 异步：在发出一个异步调用后，调用者不会立刻得到结果，该调用就返回了。 并发是指一个时间段内，有几个程序都在同一个CPU上运行，但任意一个时刻点上只有一个程序在处理机上运行。 并行是指一个时间段内，有几个程序都在几个CPU上运行，任意一个时刻点上，有多个程序在同时运行，并且多道程序之间互不干扰。 1.1 如何实现并发 多进程、多线程、协程 1.2 如何实现并行 分布式 1,3 串行 串行是同步的一种执行方式 即一个程序执行完执行下一个 2. 编译 汇编 编译型语言 python是解释性语言，可以直接执行 其他解释性语言 lua， perl 编译语言: c 编译(compilation , compile) 1、利用编译程序从源语言编写的源程序产生目标程序的过程。 2、用编译程序产生目标程序的动作。 编译就是把高级语言变成计算机可以识别的2进制语言，计算机只认识1和0，编译程序把人们熟悉的语言换成2进制的。 汇编语言中，用助记符(Memoni)代替操作码，用地址符号(Symbol)或标号(Label)代替地址码。这样用符号代替机器语言的二进制码，就把机器语言变成了汇编语言。于是汇编语言亦称为符号语言。 用汇编语言编写的程序，机器不能直接识别，要由一种程序将汇编语言翻译成机器语言，这种起翻译作用的程序叫汇编程序，汇编程序是系统软件中语言处理的系统软件。汇编程序把汇编语言翻译成机器语言的过程称为汇编。 3. 线程阻塞 守护线程 守护线程 如果你把一个线程设为守护线程 那说明你认为该线程是不重要的 在进程退出时不需要等待该线程退出 如果你的主线程在退出的时候，不用等待那些子线程完成，那就设置这些线程的daemon属性。 线程阻塞 如果把一个线程设为阻塞线程 那么该线程不执行完 进程不会结束 4. 多进程和多线程 4.1 定义 *这个概念不好定义 欢迎大家补充纠正 多进程:cpu处理多任务时每个任务之间数据，资源独立，不容易受到其他任务干扰 多线程:数据，资源共享 任务之间影响较大 被主程序控制 线程是属于进程的 4.2 应用场景 先引入线程锁的概念 Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"爬虫/进程线程协程简单关系.html":{"url":"爬虫/进程线程协程简单关系.html","title":"进程线程协程简单关系","keywords":"","body":"简单关系 同步IO(一个线程挂起,其余等待)—>处理并发—>多线程(线程太多效率低)—>协程(单线程异步IO处理并发)—>多进程+协程(多核高效) yield将函数变为生成器 可next执行函数 通过send接收生产者的返回值（返回做好汉堡的消息） asyncio可以实现单线程异步IO操作(协程) 其编程模型是个消息循环 从asyncio模块中获取一个EventLoop的引用 把需要执行的协程扔到EventLoop中执行 以此实现异步IO aiohttp可以使asyncio在http服务器端发挥作用 即asyncio的http框架 生成器函数前加async可以将函数标记为coroutine类型 然后在coroutine内部用await调用另一个coroutine实现异步操作 （共同协作 这也就是为什么叫\"协程\"） Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"电子书/《GitHub入门与实践》一书小记.html":{"url":"电子书/《GitHub入门与实践》一书小记.html","title":"《GitHub入门与实践》一书小记","keywords":"","body":"《GitHub入门与实践》一书小记 活学活用，记好笔记 Github Flow的流程 开发流程 令 master 分支时常保持可以部署的状态 进行新的作业时要从 master 分支创建新分支，新分支名称要具有 描述性 在❷新建的本地仓库分支中进行提交 在 GitHub 端仓库创建同名分支，定期 push 需要帮助或反馈时创建 Pull Request，以 Pull Request 进行交流 让其他开发者进行审查，确认作业完成后与 master 分支合并 与 master 分支合并后立刻部署 要注意，没有进行过测试或者测试未通过的代码绝不可以合并到 master 分支。因此势必要用到持续集成等手段。 进行新的作业时要从 master 分支创建新分支 新分支的名称要具有描述性 新分支中进行提交，不要再新分支中做无关的修改，方便pull Request进行审查 定期push 开发过程中最好能让其他开发者看到自己的嗲吗，同时养成积极查看他人代码的习惯 使用Pull Request 不一定非得在合并master分支时才使用pull request，今早创建pull request让其他开发者能进行审查 pull request 可以显示差别及对单行代码插入评论 务必让其他开发者进行审查 审查之后如果认为可以与 master 分支合并，则需要明确地告知对 方。按照 GitHub 的文化，这里会用到“:+1:”或“:shipit:”等表情(图 9.5)。偶尔也会见到 LGTM 的字样，这是 Looks good to me 的简写。 征得多个人同意后，便可找个适当的时机让其他开发者将该分支与 master 分支进行合并。 审查结束后立刻部署 部署 如果一天需多次部署时，可花费少量时间来使部署实现自动化， 避免工作中的费时费力，同时减少失误，并且可以让开发人员以外的人都能参与部署， 这就需要一个好的部署工具，也可以通过web界面进行部署 导入开发时的注意事项 团队人越来越多且成熟度提高，开发速度会增快，往往会出现一个部署尚未完成，下一个pull request结束并开始实施部署，这时，就会难分辨是哪个部署造成的影响，所以在部署过程中可以通过工具上锁，或者在部署时通知整个团队 重视测试 测试自动化 编写测试代码，通过全部测试 维护测试代码 不要让Pull Request中有太多反馈 代码存在多处问 题，进行多次指正和修改后仍然无法达到与 master 分支合并的水准。出 现这种情况大概有两个原因。 一是交流不足。如果创建 Pull Request 的理由没有获得认同，那就 不要通过 Pull Request 进行讨论，而是应该选择其他手段进行交流。最 好的解决途径是直接面谈。 另一个原因是技术或能力不足。如果代码经常被指出问题，那么不 是编程能力方面有问题，就是团队编写代码时没有一个明确的规则。为 避免在无用的讨论上浪费时间，团队应该制定一个最低限度的编程规 则，并且告知每一名团队成员。如果在开发过程中还需要其他规则，可 以将这些规则整合到 Wiki 中，便于阅览及修改 结对编程 组织学习小组共享知识 共享可供参考的资料 不要积攒Pull Request 在以部署为中心的开发流程中，如果总有大量 Pull Request 处于等 待审查或等待修正的状态，会导致长期无法部署，引发严重问题。 如果每一名开发者都在忙于实现各自的新功能，把所有精力都放在 编写代码和创建 Pull Request 上，势必会忽视审查与反馈工作。时间一 长，无法部署的 Pull Request 就会堆积如山。 为防止这一情况发生，建议团队制定一个新的规则:想创建 Pull Request 的人要先去对其他人的 Pull Request 进行审查及反馈，并在可以 部署时及时部署。 这样一来，自己想创建 Pull Request 时必须先处理其他人的 Pull Request，就可以有效避免 Pull Request 堆积的情况发生。 导入git flow前的准备 安装 # mac brew install git-flow # linux sudo apt-get install git-flow 确认运行状况 git flow Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"运维/centos新服务器使用.html":{"url":"运维/centos新服务器使用.html","title":"centos新服务器使用","keywords":"","body":"centos新服务器使用 1.python环境 # 依赖 yum groupinstall \"Development tools\" yum install zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel gdbm-devel db4-devel libpcap-devel xz-devel # pyenv管理python版本 默认是python2 curl -L https://raw.github.com/yyuu/pyenv-installer/master/bin/pyenv-installer | bash 环境变量配置 将这三行复制 写入 ~/.bashrc或~/.profile文件(没有就创建) pyenv install 3.6.4 -v # 设置全局python版本 pyenv global 3.6.4 2. 部署前准备 用git拉下来你的项目到 ～目录下 # 安装虚拟环境管理工具 pip install virtualenv # 在～目录下创建虚拟环境 virtualenv ienv 其中 izone是我git项目(准备部署的) # 进入虚拟环境 source ienv/bin/activate cd izone pip install -r requirements.txt 在安装虚拟环境时报错。关于mysql的，因为新服务器没有安装mysql yum install mysql 安装以后不能启动，查询了一些文档以后看到这么一条 好吧。。拥抱mariadb # 安装 yum install -y mariadb-server # 启动服务 systemctl start mariadb.service (说明：CentOS 7.x开始，CentOS开始使用systemd服务来代替daemon，原来管理系统启动和管理系统服务的相关命令全部由systemctl命令来代替。) # 开机自启动 systemctl enable mariadb.service mysql安装好了，pip还是失败 Command \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-install-fwot3_uw/mysqlclient/ 解决方案 yum install python-devel yum install mysql-devel yum install gcc mysql 1045错误 权限不足 我刚安装的mysql没有设置密码 # 进入mysql mysql -uroot mysql # mysql命令 UPDATE user SET Password=PASSWORD('newpassword') where USER='root'; LUSH PRIVILEGES; quit # 退出以后重启 systemctl restart mariadb.service 想跑起来项目真难... 明天安装supervisor 好多步骤其实之前有写，不过这次趁着刚买了新服务器部署下项目 Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"运维/docker+django.html":{"url":"运维/docker+django.html","title":"Docker Django","keywords":"","body":"1.安装 docker安装 2. 使用 pull一个基本系统镜像到本地 执行以下命令： > sudo docker pull ubuntu:16.04 > sudo docker images 进入ubuntu镜像 执行以下命令： > sudo docker run -it ubuntu:16.04 安装相关程序 执行以下命令： # 修改apt源 > vi /etc/apt/source.list # 输入以下源： deb http://mirrors.aliyun.com/ubuntu/ trusty main restricted universe multiverse deb http://mirrors.aliyun.com/ubuntu/ trusty-security main restricted universe multiverse deb http://mirrors.aliyun.com/ubuntu/ trusty-updates main restricted universe multiverse deb http://mirrors.aliyun.com/ubuntu/ trusty-proposed main restricted universe multiverse deb http://mirrors.aliyun.com/ubuntu/ trusty-backports main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ trusty main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ trusty-security main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ trusty-updates main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ trusty-proposed main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ trusty-backports main restricted universe multiverse > apt-get update > apt-get install vim > apt-get install python3-pip > pip3 install uwsgi // 安装uwsgi > apt-get install nginx //安装nginx > pip3 install django==1.11.0 4.创建一个django项目 > sudo cd root > sudo django-admin startproject DockerDjangoTest 5.创建uwsgi配置文件 > vim /root/docker-django-test-uwsgi.ini 输入以下内容： [uwsgi] master = true chdir = /root/DockerDjangoTest processes = 4 http = 0.0.0.0:8888 daemonize = /root/uwsgi.log wsgi-file = DockerDjangoTest/wsgi.py daemonize = /root/docker-django-test-uwsgi.log 6.配置nginx > sudo vim /etc/nginx/conf.d/docker_django_test_nginx.conf 输入以下内容： upstream docker-django-test { server 0.0.0.0:8888; } server { listen 8010; server_name 0.0.0.0; charset utf-8; client_max_body_size 75M; # Django media location /media { alias /root/DockerDjangoTest/media; } location /static { alias /root/DockerDjangoTest/static; } location / { proxy_pass http://docker-django-test; } } 7.重启nginx > sudo service nginx restart 8.写启动脚本 sudo vim /root/start-django.sh 输入以下内容： #! / bin/sh service nginx restart; uwsgi --ini /root/docker-django-test-uwsgi.ini; tail -100f /var/log/nginx/access.log; 9.修改启动脚本权限 sudo chmod 777 /root/start-django.sh 10.退出容器，并commit容器为镜像 > exit > sudo docker ps -a 找到最新一条记录的id > sudo docker commit -m \"init django env on ubuntu:16.04\" [id] [name]/[image-name] 11.在主机中重新运行该容器 > sudo docker run -d -p 0.0.0.0:8010:8010 [name]/[image-name] sh /root/start-django.sh 12.浏览器中访问 浏览器访问：http://127.0.0.1:8010 若是服务器上配置的，则访问：http://your server ip:8010 番外篇 如何将自己的docke镜像上传到dockerhub 首先需要在https://hub.docker.com/注册账号 在主机上执行sudo docker login按照提示登录自己注册的账号 执行sudo docker images查看本地已有的镜像 执行sudo docker push [image]:tag //比较慢 完毕，可登录 https://hub.docker.com/查看和设置自己上传的镜像 提供一个基本版django生产环境的docker镜像 https://hub.docker.com/r/kering/django-env/ 删除所有镜像 docker ps -a | grep \"Exited\" | awk '{print $1 }'|xargs docker stop docker ps -a | grep \"Exited\" | awk '{print $1 }'|xargs docker rm docker images|grep none|awk '{print $3 }'|xargs docker rmi # 停止所有正在运行的容器 docker ps|awk '{print $1}'|xargs docker stop # 获取正在运行的postgresql的docker进程id docker ps|grep \"postgres\"|awk '{print $1}' 解决静态资源无法加载问题 python manage.py runserver --insecure 本地资源同步到远程 rsync -a -v super root@zskin.xin:/root/super 基础命令 # 运行进程 docker ps # 所有进程 docker ps -a # 列出进程ip docker inspect 镜像id |grep IP # 容器内部进程运行情况 docker top # 映射本地文件到docker容器运行，推出时删除容器 docker run --name redis3 --rm -v /tmp/redis:/data redis # 后台启动 -d 指定端口 -p docker run -d -p 0.0.0.0:8000:8000 zskin/xpc sh /root/run.sh # 交互模式进入 -it -v映射文件 docker run -it -v /data/xpc:/usr/src zskin/xpc /bin/bash 打包镜像 1.创建dockerfile FROM django:1.11 WORKDIR /usr/src COPY . /usr/src RUN pip install -r requirements.txt -i http://mirrors.tencentyun.com/pypi/simple --trusted-host=mirrors.tencentyun.com EXPOSE 8000 CMD ['/usr/local/bin/gunicorn', '-b', '0.0.0.0:8000', 'web.wsgi'] 2.打包 Docker build -t name/image . Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"运维/Doker了解.html":{"url":"运维/Doker了解.html","title":"Doker了解","keywords":"","body":"Docker 一、Docker是什么? Docker 属于 Linux 容器的一种封装，提供简单易用的容器使用接口。它是目前最流行的 Linux 容器解决方案。 Docker 将应用程序与该程序的依赖，打包在一个文件里面。运行这个文件，就会生成一个虚拟容器。程序在这个虚拟容器里运行，就好像在真实的物理机上运行一样。有了 Docker，就不用担心环境问题。 总体来说，Docker 的接口相当简单，用户可以方便地创建和使用容器，把自己的应用放入容器。容器还可以进行版本管理、复制、分享、修改，就像管理普通的代码一样。 Doker是基于GO语言实现的云开源项目 通过对应用组件的封装、分发、部署、运行等生命周期的管理，达到应用组件级别的“一次封装，到处运行”这里的应用组件 既可以是web应用 也可以是一套数据库服务 甚至是一个操作系统或编译器 二、 Docker的用途? Docker 的主要用途，目前有三大类。 （1）提供一次性的环境。比如，本地测试他人的软件、持续集成的时候提供单元测试和构建的环境。 （2）提供弹性的云服务。因为 Docker 容器可以随开随关，很适合动态扩容和缩容。 （3）组建微服务架构。通过多个容器，一台机器可以跑多个服务，因此在本机就可以模拟出微服务架构。 三、Docker的优势 更快速的交付和部署(使用docker，开发人员可以用镜像来快速构建一套标准的开发环境;开发完成之后，测试和运维人员可以直接使用相同环境来部署代码。) 更轻松的迁移和扩展(docker容器几乎可以在任意平台上运行，包括物理机、虚拟机、公有云、私有云、个人电脑、服务器等。可以在不同的平台轻松地迁移应用) 更简单的更新管理(使用Dockerfile，只需要修改小小的配置，就可以替代以往大量的更新工作) 更快速的启动时间 更高效的利用系统资源 与传统虚拟机对比 场景示例-传统开发流程 场景示例-docker环境开发流程 四、 Docker的安装 Docker 是一个开源的商业产品，有两个版本：社区版（Community Edition，缩写为 CE）和企业版（Enterprise Edition，缩写为 EE）。企业版包含了一些收费服务，个人开发者一般用不到。下面的介绍都针对社区版。 Docker CE 的安装请参考官方文档。 Mac Windows Ubuntu curl -fsSL https://get.docker.com | bash -s docker --mirror Aliyun Debian CentOS Fedora 其他 Linux 发行版 安装完成后，运行下面的命令，验证是否安装成功。 $ docker --version Docker version 1.12.3, build 6b644ec $ docker-compose --version docker-compose version 1.8.1, build 878cff1 $ docker-machine --version docker-machine version 0.8.2, build e18a919 如果 docker version、docker info 都正常的话，可以运行一个 Nginx 服务器： 1 $ docker run -d -p 80:80 --name webserver nginx 服务运行后，可以访问 http://localhost，如果看到了 \"Welcome to nginx!\"，就说明 Docker for Mac 安装成功了。 要停止 Nginx 服务器并删除执行下面的命令： 1 $ docker stop webserver 2 $ docker rm webserver Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"运维/Linux Pro.html":{"url":"运维/Linux Pro.html","title":"Linux Pro","keywords":"","body":"Linux Pro 熟悉些运维命令或是管道等骚操作，慢慢深入，慢慢更新 适用人群：了解基础命令，想来看看其他命令有什么奇淫技巧可以挖掘的人 目录 [TOC] 感觉比较常用或有用的liunx命令 # 一年中的第几天 date \"+%j\" # 下载文件 wget -c 断点续传 -p 页面资源下载 -r 递归 -P 指定下载目录 -b 后台下载 # 查看进程 ps -a 所有进程 -u 用户及其他信息 -x 未控制终端的进程 # top 动态监视进程活动与系统负载 top # pidof 查询某个指定服务进程的id # 例： pidof sshd > 2156 # 杀死进程 kill pid # 杀死httpd服务的所有进程（包括子进程） killall httpd Liunx中，5中常见进程状态 R 运行 S 中断 D 不可中断 Z 僵死（进程已停止 描述符仍在） T 停止 运维相关 运维人员最基本要了解的是服务器的 网卡网络 系统内核 系统内核 系统负载 内存使用情况 当前启动终端数量 历史登录记录 命令执行记录 救援诊断 1 ifconfig 用于获取网卡配置及网络状态，主要看网卡名称。ip地址,ether参数后边的网卡地址以及RX\\TX的数据接收包和发送数据包的个数及累计流量 2 uname 查看系统内核和系统版本 当前系统的内核名称、主机名、内核发行版本、节点名、系统时间、硬件名称、硬件平台、处理器类型及操作系统 3 uptime 查看系统的负载信息、格式为uptime 系统当前时间、系统已运行时间、启用终端数量、平均负载 4 free 显示当前系统中内存使用量free [-h] mac没有这个命令。。 5 who 查看当前登入主机的用户终端信息 用户名、终端设备、登入时间 6 last 查看所有系统登录记录 很容易被篡改 7 history 显示历史执行过的命令，默认是1000条，可以通过自定义/etc/profile中HISRTSIZE变量值来修改 也可以通过![num]来执行某条命令 history -c清空 8 sosreport 收集系统配置及架构信息并输出诊断文档、格式为socreport 加粗的部分是收集好的资料压缩 文件以及校验码，出现问题时将其发送给技术支持人员即可: 工作目录切换命令 pwd 查看当前所处的目录 ls -ld 查看指定目录信息 文本文件编辑命令 more 查看内容较多的纯文本文件 tail 实时查看最新日志文件 tail -f 文件名 tr 替换文本文件中的字符tr[old][new] wc 统计指定文本行数、字数、字节数wc[参数]文本 统计系统当前有多少用户 stat 查看文件具体存储信息和时间 cut 按列提取文本cut[参数]文本 列出系统中的用户名 dd 复制或转换指定大小的文件 生成一个大小500M,名为x_file的文件 file 查看文件类型 file 文件名 打包压缩 tar 注: -f参数必须放在最后一位,是压缩或解压的软件包名 常用做法: tar -czvf 压缩包名.tar.gz 打包目录 tar -xzvf 压缩包名.tar.gz grep 关键词搜索 工作中常用两个参数 -v 反选 -n 显示行号 find 查找。。这个是最常用但是最不会用的命令了 find [查找路径] 寻找条件 操作 重点说明一下 -exec参数，类似于管道操作 获取该目录中所有以host开头的文件列表 管道符、重定向、环境变量 这块应该算是重点了，用好了会达到事半功倍的效果 重定向 这块给我的感觉像是日志，错误输出、标准输出等等 例：man bash > readme.txt 标准输出重定向 将打印内容写入readme.txt 覆盖 追加 写入报错信息 ls -l xxx 2> xxx 管道 命令A | 命令B 用翻页的方式查看 ls -l /etc/ | more 通过把管道符和 passwd 命令的--stdin 参数相结合，我们可以用一条 命令来完成密码重置操作 echo \"linuxprobe\" | passwd --stdin root 通配符 这个像是正则，很容易理解 最常用的转义字符 需要某个命令的输出值时,可以使用``将命令包起来 =============== 待更,困了 =============== Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"运维/LinuxPro（二）.html":{"url":"运维/LinuxPro（二）.html","title":"LinuxPro（二）","keywords":"","body":"LinuxPro（二） [TOC] 配置主机名称 为了便于在局域网中查找某台特定的主机，或者对主机进行区分，除了要有 IP 地址外， 还要为主机配置一个主机名，主机之间可以通过这个类似于域名的名称来相互访问 主机名大多保存在/etc/hostname中 修改主机名 进入，修改 :wq保存退出 修改后不会立即生效,重启下 sudo reboot 配置网卡 网卡 IP 地址配置的是否正确是两台服务器是否可以相互通信的前提。在 Linux 系统中， 一切都是文件，因此配置网络服务的工作其实就是在编辑网卡配置文件 在 RHEL 5、RHEL 6 中，网卡配置文件的前缀为 eth，第 1 块网卡为 eth0，第 2 块网卡为 eth1;以此类推。而在 RHEL 7 中，网卡配置文件的前缀则以 ifcfg 开始， 加上网卡名称共同组成了网卡配置文件的名字，例如 ifcfg-eno16777736;好在除了文件名变 化外也没有其他大的区别。 现在有一个名称为 ifcfg-eno16777736 的网卡设备，我们将其配置为开机自启动，并且 IP 地址、子网、网关等信息由人工指定，其步骤应该如下所示。 1.切换到/etc/sysconfig/network-scripts目录中（存放网卡的配置文件） 2.vim编辑网卡文件写入下面配置（每台设备硬件及架构不同，因此使用ifconfig命令确认网卡默认名称） 3.重启网络服务并测试网络连接 cd /etc/sysconfig/network-scripts vim ifcfg-eno16777736 。。。我照着操作没找到sysconfig目录，网卡信息也有些不一样，就先不操作这个了 编写SHELL脚本 面试里可能会出现让在bash里输出个列表什么的，被问到我也是一脸蒙蔽，接着往下看吧 shell脚本命令的工作方式有两种 交互式: 用户每输入一条命令就立即执行 批处理: 编好一个完整脚本，一次性执行 先写个简单的 创建个目录来存放写的脚本 输入下面语句，保存退出 运行bash example.sh 也可以通过./example.sh来执行 Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"运维/osi模型：应表会传网数物 七层协议模型.html":{"url":"运维/osi模型：应表会传网数物 七层协议模型.html","title":"osi模型：应表会传网数物 七层协议模型","keywords":"","body":"osi模型： 应表会传网数物 七层协议模型 Tcp/ip四层概念模型: 应用层、传输层、网络层、数据链路层 应用层需掌握 http\\ftp\\smtp\\pop3\\imap4 一次http请求过程: 域名解析dns-》发起TCP的三次握手-〉建立tcp连接后发起http请求-> 服务器响应http请求，浏览器得到html代码->浏览器解析html代码-》页面渲 1.域名解析：以谷歌为例。如果浏览器缓存没有找到-〉去操作系统缓存找-》还没找到会去hosts文件找，如果都没有找到的话。就会向运营商发起DNS解析请求，运营商会先从自身缓存找 没找到的话使用UDP协议向53端口发送请求，请求返回该域名的ip，会依次经过根域、顶级域查找www.xx.com的ip 最终返回ip 2.tcp3次握手: 拿到ip后 user-agent以一个随机端口向服务器的web程序80端口发起tcp连接请求，请求到达服务器端后进入网卡，然后进入内核的tcp/ip协议栈 会将请求一层层剥开，可能会通过防火墙的过滤， 最终到达web程序，最终建立tcp/ip连接 服务器端web程序接受到http请求之后，就开始处理该请求，处理之后返回给浏览器html文件 负载均衡： 早期开始使用的是dns分流策略 dns做负载，解析不同的ip给客户端，让客户端的流量直接到达各个服务器，但这个方法的缺点就是延时性，且调度策略简单，满足不来业务需求，因此出现了负载均衡 负载均衡通过一台服务器做负载，通过轮询算法将客户端的流量分给其他服务器，并且对集群中的服务器进行健康检查，如果发现故障节点会动态的将该节点从集群中剔除，以此来保证应用的高可用 负载均衡分四层负载均衡和七层负载均衡，四层指的是osi模型的传输层，负责的是转发，七层指的是应用层 负责代理 反向代理: 传统的代理是服务器位于浏览器一端，代理浏览器将http请求发到互联网上， 而反向代理的服务器是位于网站机房一侧。代理web服务器接受http请求 百万级数据库优化 1.查询优化 1避免全表扫描，对where和order by使用多的字段建索引 2.索引不能建太多，一张表建议5-6个，过多会对插入和修改引起不必要的麻烦 3考虑索引是否有必要，避免where语句对字段进行null判断，这样会放弃索引而去全表扫描 4 尽量使用数值类型而非字符类型，字符长度减少 5 使用like也会全表查询，可以考虑全文检索 2.数据库优化 1、 减少数据访问（减少磁盘访问） 2、 返回更少数据（减少网络传输或磁盘访问） 3、 减少交互次数（减少网络传输） 4、 减少服务器CPU开销（减少CPU及内存开销） 5、 利用更多资源（增加资源） Django-orm优化 1、 当只获取一两列数据时，把使用values、values_list，它就会返回字典或元组，2⃣而不是python对象 2、 对获取的数据进行缓存，使用redis， 3、 处理很多记录。 当你获得一个queryset时，django会缓存这些数据，如果你对查询结果进行多次循环，则这些缓存很有意义，但如果只循环一次，那就很耗费资源了。这个时候可以使用iterator() for book in Books.objects.all(): do_stuff(book) # 修改后 for book in Books.objects.all().iterator(): do_stuff(book) csrf： 跨站请求伪造攻击、通过伪造网站访问请求，使携带cookie信息的用户进行一些危险的操作 tornado与django优点对比 django： ORM、内置后台管理系统、session tornado 异步编程、web sockets、http服务器 Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"运维/shell，解放我的双手.html":{"url":"运维/shell，解放我的双手.html","title":"shell，解放我的双手","keywords":"","body":"shell，解放我的双手 这两天才开始接触shell编写脚本，真的是相见恨晚。一个小小的demo，就节省了我很多时间 web后端，应用场景: 前后端项目都在git上托管，docker部署 每次前端修改之后我都要拉下来，打包、然后覆盖后端项目dist文件，再git push，部署 真的累了 直到... #!/bin/sh cd /Users/mac/WebstormProjects/fontend/ git pull origin master && echo \"拉取成功\" sleep 5 cd /Users/mac/PycharmProjects/flask/backend rm -rf dist/ cp -R /Users/mac/WebstormProjects/fontend/dist ./ echo \"拷贝成功\" echo \"-----准备提交-----\" git add . git commit -m $1 echo \"git 提交注释：$1\" git push origin dev echo \"-----提交结束-----\" 脚本内容就不一一解释了， 就是后端经常在命令行里敲的，只是集中起来放到一个.sh文件里 就上面一个脚本，执行一次，就省去了自己操作的那么多时间 操作流程 # 创建shell脚本 touch run.sh # 输入自己要执行的命令 vim run.sh # :wq保存退出 # 更改脚本权限为可执行 sudo chmod +x run.sh # 运行 ./run.sh 以后多学些其他知识，提升工作效率 Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"运维/ssh免密登陆.html":{"url":"运维/ssh免密登陆.html","title":"ssh免密登陆","keywords":"","body":"服务器间/本地远程服务器免密登陆 # 生成密钥对 ssh-keygen # 进入.ssh文件夹下查看密钥 cd ~/.ssh/ # 使用scp传输将本机公钥传给另一台主机 scp id_rsa.pub user@xx.xx.xx.xx:~/ # 登录另一台主机将传过来的公钥写入authorized_keys文件 cat id_rsa.pub >> .ssh/authorized_keys Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"运维/supervisor.html":{"url":"运维/supervisor.html","title":"Supervisor","keywords":"","body":"supervisor的使用 安装 # 使用pip来安装，目前supervisor只支持在python2.x环境下 pip install supervisor # 也可以通过系统自带的包管理工具来安装 yum install supervisor apt install supervisor 安装完成以后，通过命令行查看一下版本。 supervisord --version 看一下supervisor有哪些命令： supervisord # 启动服务 supervisorctl # 启动客户端 echo_supervisord_conf # 输出配置文件 配置 首先查看一下/etc/supercisord.conf文件是否存在，如果不存在，则使用下面的命令创建： echo_supervisord_conf > /etc/supervisord.conf 该配置文件以分号（;）作为注释 surpervisor查找配置文件的顺序是这样的： /usr/etc/supervisord.conf /usr/supervisord.conf ./supervisord.conf, ./etc/supervisord.conf /etc/supervisord.conf /etc/supervisor/supervisord.conf 如果在以上目录都未能查找到配置文件则supervisor启动失败 我们可以手动指定配置文件的路径 supervisord -c /tmp/supervisord.conf 配置文件 [unix_http_server] ; 服务器socket文件路径 file=/tmp/supervisor.sock [supervisord] ; 日志文件路径 logfile=/tmp/supervisord.log ; 进程ID文件路径 pidfile=/tmp/supervisord.pid ; 定义了一个名为tinyproxy的程序 [program: tinyproxy] ; command后面就是启动这个程序的具体的命令 command=tail -f /var/log/tinyproxy/tinyprox.log 使用客户端连接 连接上supervisor控制台 再输入reload命令 或者直接重新启动supervisor ```bash # 它会重启supervisor内所有的程序 supervisorctl reload 如果只是修改了program的配置，可以使用update命令 supervisorctl update 其他的客户端子命令 # 查看所有的程序的状态 supervisorctl status # 查看程序的标准输出 supervisorctl tail [program_name] supervisorctl tail -f [program_name] # 启动、停止、重启相关程序 supervisorctl [start|stop|restart] [program_name] [program:discovery1] command=/root/.pyenv/shims/scrapy crawl discovery directory=/root/xinpianchang/xpc redirect_stderr=true ; redirect proc stderr to stdout (default false) stdout_logfile=/tmp/discovery1.log ; stdout log path, NONE for none; default AUTO import redis r=redis.Redis() for proxy in settings.getlist('PROXIES'): r.sadd('discovery:proxies', proxy) Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"运维/supervisor管理进程.html":{"url":"运维/supervisor管理进程.html","title":"supervisor管理进程","keywords":"","body":"supervisor管理进程 配置文件位置: /etc/supervisord.conf 进程 ; 命令格式[program:程序名称] [program:discovery1] ; 运行的命令，注意命令的路径要写全 command=/usr/bin/python mongo ; 你运行命令的时候所在的目录 directory=/home/ubuntu/test ; 把stderr(报错信息)重定向至标准输出 redirect_stderr=true ; 把标准输出重定向到/tmp/discovery1.log文件 stdout_logfile=/tmp/discovery1.log ; 定义一个程序组 [group:spiders] # 定义该组有哪些程序，对应上面program的名字 programs=discovery1 启动 supervisord 杀死进程，关闭supervisord ps -e|grep super|awk '{print $1}'|xargs kill -9 Nginx配置 配置文件 /etc/nginx.conf # http client_max_body_size 75m; Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"运维/使用dockerfile自定义镜像.html":{"url":"运维/使用dockerfile自定义镜像.html","title":"使用dockerfile自定义镜像","keywords":"","body":"Dockerfile该怎么写？ [TOC] 目前我会的docker运行方式就是简单粗暴的docker pull下来别人的基础环境， 然后把自己的项目丢进拉下来的基础环境运行， 最终保存退出，打包成自己的镜像push到自己的docker hub中以供项目复用 有朋友问我是不是懂docker，我觉得略懂，然后问我如何写Dockerfile，我也是一脸懵x，本着好学的心态去了解了一下Dockerfile的生成 docker? 项目运行依赖环境是必需的，我的python项目 nginx+redis+mysql+python+flask， 让别人在他的电脑上如何快速运行我的项目？各种找资源安装依赖？No， docker就是解决这个问题的，以容器+镜像的形式存储依赖环境，就像虚拟机一样，让别人快速拥有能运行项目的环境 Dockerfile? 就是将你的项目依赖打包成docker镜像的一个文件，开头的D必须大写 # 打开终端,创建一个新目录存放Dockerfile cd ~ mkdir docker-demo cd docker-demo touch Dockerfile 这样就准备好了，进入你的项目把依赖文件拷贝一份，requirements.txt或者package.js等等 cp requirements.txt ~/ cd ~/docker-demo vim Dockerfile 开始写了 FROM tiangolo/uwsgi-nginx-flask:python3.6 COPY ./requirements.txt /tmp/ RUN pip install -r /tmp/requirements.txt 这就写完了 然后执行 # 最后的.不要省略 # nginx是镜像名，v1是tag，都是自己起的 # 如果后面要上传到自己的镜像仓库dockerhub， 这块的镜像名最好起成 dockerhub的账号名/镜像 # 如 dockershi/django-demo docker build -t nginx:v1 . 这样就构建好啦，docker images可以查看到生成的镜像 来说下需要注意的地方， FROM 后边跟的是基础镜像。我找了一个相对比较全的基础镜像，基础镜像可以去docker hub去找 COPY 就是拷贝 当前目录下的依赖文件. 这里要说的是尽量把依赖文件单独拎出来，因为这个./ 的意思是上下文，是相对路径,所以你用../req. /root/app/req都没用 RUN 就是按层级执行，层级尽量不要太多 比如 RUN apt-get install redis RUN apt-get install mongo RUN apt-get install gcc 就直接写成 RUN buildDeps = apt-get install redis \\ && apt-get install mongo \\ && apt-get install gcc \\ $buildDeps $buildDeps结束 \\换行承接上文 Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"运维/使用supervisor进行scrapy-redis部署.html":{"url":"运维/使用supervisor进行scrapy-redis部署.html","title":"使用supervisor进行scrapy-redis部署","keywords":"","body":"redis存放IP地址 #如果你是使用的pyenv,请这些使用，并且要注意，下面这个地址你要写到supervisor的配置文件中 root@iZuf:~/xpc_redis# /root/.pyenv/versions/3.6.4/bin/scrapy shell # 当然你也可以在虚拟机这样 # (env) duke@ubuntu:~/myproject/mycrawl/xpc_redis$ scrapy shell >>> import redis >>> from xpc.settings import PROXIES >>> r = redis.Redis(host='127.0.0.1') >>> for proxy in PROXIES: r.sadd('discovery:proxies',proxy) supervisor的使用 安装 # 使用pip来安装，目前supervisor只支持在python2.x环境下 pip install supervisor # 也可以通过系统自带的包管理工具来安装 yum install supervisor apt install supervisor安装完成以后，通过命令行查看一下版本。 配置 首先查看一下/etc/supercisord.conf文件是否存在，如果不存在，则使用下面的命令创建： echo_supervisord_conf > /etc/supervisord.conf 该配置文件以分号（;）作为注释 配置文件 root@iZuf:~/xpc_redis# vim /etc/supervisord.conf [inet_http_server] ; 绑定的IP和端口 port=0.0.0.0:9001 ;用户名和密码 username=user password=123 [unix_http_server] ; 服务器socket文件路径 file=/tmp/supervisor.sock [supervisord] ; 日志文件路径 logfile=/tmp/supervisord.log ; 进程ID文件路径 pidfile=/tmp/supervisord.pid ; 定义了一个名为tinyproxy的程序 [program: tinyproxy] ; command后面就是启动这个程序的具体的命令 command=tail -f /var/log/tinyproxy/tinyprox.log ; 命令格式[program:程序名称] [program:discovery1] ; 实际要运行的命令，注意命令的路径要写全 command=/root/.pyenv/versions/3.6.4/bin/scrapy crawl discovery ; 你运行命令的时候所在的目录 directory=/root/xpc_redis ; 把stderr(报错信息)重定向至标准输出 redirect_stderr=true ; 把标准输出重定向到/tmp/discovery1.log文件 stdout_logfile=/tmp/discovery1.log ; 定义一个程序组 [group:spiders] # 定义该组有哪些程序，对应上面program的名字 programs=discovery1 启动 root@iZuf:~/xpc_redis# supervisord 访问下面的地址 记得要开放你的阿里云安全组9001端口 http://你阿里云ip地址:9001/ 用户名：user 密码：123 给redis起始地址 root@iZuf:~/xpc_redis# redis-cli 127.0.0.1:6379> lpush discovery:start_urls http://www.xinpianchang.com/channel/index/type-0/sort-like/duration_type-0/resolution_type-/page-4 class CreateLesson(Resource): \"\"\"创建课时,获取所有课时\"\"\" def get(self): try: result=[] lesson = Lessons.objects.all() for i in lesson: result.append({ \"num\":i['num'], \"level\":i[\"level\"], \"name\":i[\"name\"], \"about\":i[\"about\"] }) return {\"msg\":\"请求成功\", \"code\":0, \"data\": result} except Exception as e: current_app.logger.error(e) return {'msg':'获取失败','code':1} def post(self): id = str(round(time.time()*pow(10,6))) data = request.get_json() try: num = int(data['num']) if numCopyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"运维/开发模式.html":{"url":"运维/开发模式.html","title":"开发模式","keywords":"","body":"开发模式 敏捷开发 以人为核心、迭代、循序渐进的开发方式 简化文档，提取文档重点，主要在于人与人之间的沟通， 对开发产品进行迭代，最终完成开发。 迭代：迭代是指把一个复杂且开发周期很长的开发任务，分解为很多小周期可完成的任务，这样的一个周期就是一次迭代的过程；同时每一次迭代都可以生产或开发出一个可以交付的软件产品。 敏捷开发的一种实现方式就是Scrum方式 Scrum开发流程中的三个项目角色 产品负责人(PO) 主要负责确定产品的功能和达到要求的标准，指定软件的发布日期和交付的内容，同时有权力接受或拒绝开发团队的工作成果。 流程管理员（SM） 主要负责整个Scrum流程在项目中的顺利实施和进行，以及清除挡在客户和开发工作之间的沟通障碍，使得客户可以直接驱动开发。 开发团队(ST) 主要负责软件产品在Scrum规定流程下进行开发工作，人数控制在5~10人左右，每个成员可能负责不同的技术方面，但要求每成员必须要有很强的自我管理能力，同时具有一定的表达能力；成员可以采用任何工作方式，只要能达到Sprint的目标 sprint 是短距离赛跑的意思，这里面指的是一次迭代，而一次迭代的周期是1个月时间（即4个星期），也就是我们要把一次迭代的开发内容以最快的速度完成它，这个过程我们称它为 Sprint 1.首先我们需要确认一个 PB ( Product Backlog , 即按优先顺序排列的一个产品需求列表) ，这是由 PO（Product Owner） 负责的 2.ST（Scrum Team） 会根据 PB 列表，进行工作量的预估和安排 3.有了 PB 列表，我们需要通过 Sprint Planning Meeting( Sprint 计划会议)来从中挑选出一个 Story 作为本次迭代完成的目标，这个目标的时间周期是1~4个星期，然后把这个Story进行细化，形成一个Sprint Backlog 4.Sprint Backlog 是由 ST 去完成的，每个成员根据Sprint Backlog再细化成更小的任务（细到每个任务的工作量在2天内能完成） 5.在Scrum Team完成计划会议上选出的Sprint Backlog过程中，需要进行 Daily Scrum Meeting（每日站立会议），每次会议控制在15分钟左右，每个人都必须发言，并且要向所有成员当面汇报你昨天完成了什么，并且向所有成员承诺你今天要完成什么，同时遇到不能解决的问题也可以提出，每个人回答完成后，要走到黑板前更新自己的 Sprint burn down（Sprint燃尽图） 6.做到每日集成，也就是每天都要有一个可以成功编译、并且可以演示的版本；很多人可能还没有用过自动化的每日集成，其实TFS就有这个功能，它可以支持每次有成员进行签入操作的时候，在服务器上自动获取最新版本，然后在服务器中编译，如果通过则马上再执行单元测试代码，如果也全部通过，则将该版本发布，这时一次正式的签入操作才保存到TFS中，中间有任何失败，都会用邮件通知项目管理人员 7.当一个Story完成，也就是Sprint Backlog被完成，也就表示一次Sprint完成，这时，我们要进行 Srpint Review Meeting（演示会议），也称为评审会议，产品负责人和客户都要参加（最好本公司老板也参加），每一个Scrum Team的成员都要向他们演示自己完成的软件产品（这个会议非常重要，一定不能取消） 8.最后就是 Sprint Retrospective Meeting（回顾会议），也称为总结会议，以轮流发言方式进行，每个人都要发言，总结并讨论改进的地方，放入下一轮Sprint的产品需求中 下面是运用 Scrum 开发流程中的一些场景图： （每日站立会议，参会人员可以随意姿势站立，任务看板要保证让每个人看到，当每个人发言完后，要走到任务版前更新自己的燃尽图） （任务看版包含 未完成、正在做、已完成 的工作状态，假设你今天把一个未完成的工作已经完成，那么你要把小卡片从未完成区域贴到已完成区域。每个人的工作进度和完成情况都是公开的，如果有一个人的工作任务在某一个位置放了好几天，大家都能发现他的工作进度出现了什么问题（成员人数最好是5~7个，这样每人可以使用一种专用颜色的标签纸，一眼就可以从任务版看出谁的工作进度快，谁的工作进度慢） （计划纸牌，它的作用是防止项目在开发过程中，被某些人所领导。 怎么用的呢？比如A程序员开发一个功能，需要5个小时，B程序员认为只需要半小时，那他们各自取相应的牌，藏在手中，最后摊牌，如果时间差距很大，那么A和B就可以讨论A为什么要5个小时） 瀑布式开发 严格按照需求文档,明确个人目标的一种开发模式 需求非常明确， 工作量十分可控， 对质量要求比较低， 业务建模也比较简单， 功能构成较少 这种开发模式如果范围控制和风险控制做的比较好的话，就真的如瀑布一般，‘飞流直下三千尺’，迅速完成客户期望，部署运行，一般都是在外包公司常见 阶段 计划 需求分析 概要设计 详细设计 编码 单元测试 测试 运维 有个缺点就是瀑布模型的周期是环环相扣的。每个周期中交互点都是一个里程碑，上一个周期的结束需要输出本次活动的工作结果，本次的活动的工作结果将会作为下一个周期的输入。这样，当某一个阶段出现了不可控的问题的时候，就会导致返工，返回到上一个阶段，甚至会延迟下一个阶段。 V型开发 开发过程与验证过程对应的对称型结构， V模型从整体上看起来，就是一个V字型的结构，由左右两边组成。左边的下划线分别代表了需求分析、概要设计、详细设计、编码。右边的上划线代表了单元测试、集成测试、系统测试与验收测试。看起来V模型就是一个对称的结构，它的重要意义在于，非常明确的表明了测试过程中存在的不同的级别，并且非常清晰的描述了这些测试阶段和开发阶段的对应关系。 螺旋型开发 尤其注重风险分析阶段，适用于庞大且复杂，高风险的项目， 通常由四个阶段组成 制定计划 风险分析 实施工程 客户评估 螺旋模型中，发布的第一个模型甚至可能是没有任何产出的，可能仅仅是纸上谈兵的一个目标，但是随着一次次的交付，每一个版本都会朝着固定的目标迈进，最终得到一个更加完善的版本。 原型开发 原型化模型第一步就是创建一个快速原型，能够满足项目干系人与未来的用户可以与原型进行交互，再通过与相关干系人进行充分的讨论和分析，最终弄清楚当前系统的需求，进行了充分的了解之后，在原型的基础上开发出用户满意的产品。在实际的项目过程中，借助于组织过程资产以及快速模型软件，一般在需求分析的时候，就可以建立一些简单的原型，例如在第一家YH公司中，因为是“行业软件提供商”，所以拥有各个地域的行业解决软件方案，惯用的伎俩就是将其他地市的项目拿到本次项目实施地，作为原型化模型。原型化模型是极具意义的项目实践 Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"运维/管理容器.html":{"url":"运维/管理容器.html","title":"管理容器","keywords":"","body":"优雅的使用docker 管理容器 当docekr容器比较多时，删除起来就会很麻烦 原理：先用docker ps -a -q 输出所有容器的container id(-f 表示过滤参数或者输出格式)，然后作为docker rm 的参数进行批量删除 输出所有容器的name： sh-4.2# docker ps --format='{{.Names}}' test-env test-args test-run 输出所有容器名包含test的容器，并打印容器名 sh-4.2# docker ps -f name=test --format='{{.Names}}' test-env test-args test-run 查看退出状态的容器，并打印容器名 sh-4.2# docker ps -f status=exited --format=\"{{.Names}}\" thirsty_brahmagupta clever_mestorf hopeful_morse stoic_morse elated_williams tender_jepsen reverent_mirzakhani 删除所有容器： sh-4.2# docker rm -f -v $(docker ps -a -q) 删除/启动所有退出的容器： sh-4.2# docker rm/start $(docker ps -qf status=exited) 删除所有镜像： sh-4.2# docker rmi $(docker images -q) 查看悬挂镜像: sh-4.1# docker images -qf dangling=true 只查看镜像或者容器指定的信息(在docker1.10之后才支持的) 只列出镜像的id以及仓库名称： sh-4.2# docker images --format \"{{.ID}}: {{.Repository}}\" 67591570dd29: centos 0a18f1c0ead2: rancher/server 只列出容器的相关id,image,status和name sh-4.2# docker ps --format \"{{.ID}}: {{.Image}} : {{.Status}} : {{.Names}}\" 66b60b72f00e: centos : Up 7 days : pensive_poincare 或者自己重新定义列,就和原生差不多: sh-4.2# docker ps --format \"table {{.ID}}\\t{{.Image}}\\t{{.Status}}\\t{{.Names}}\" CONTAINER ID IMAGE STATUS NAMES 66b60b72f00e cento 使用label 在实际运维过程中，大量的容器可能会一些运维上的挑战，通过使用label，可以很好的将容器分类。label贯穿于docker的整个过程。 这个label可以作为你区分业务，区分模板各种区分容器的标识，通过标识，可以将容器更好的进行分组 sh-4.2# docker run -itd --name volume-test --storage-opt size=70G --label zone=test 172.25.46.9:5001/centos6.8-jdjr-test-app c3772397e58e663095c2c0fd8d688b3d41b494097999ec2b6d6b7c509d23a138 创建容器的时候定义一个label，表示该容器在test这个区域 使用定义的label进行快速检索容器，并进行下一步操作(比如删除啦，更新啦) sh-4.2# docker ps -qf label=zone=test c3772397e58e sh-4.2# docker ps -f label=zone=test --format='{{.Names}}' volume-test 通过命令删除/停止所有容器 # grep 'exited' 列出所有包含exited字符的行 # awk ‘{print $1}’ 列出第一列，就是容器id # xargs docker stop 将所列出的容器id 作为参数 传给停止命令 docker ps -a | grep \"Exited\" | awk '{print $1 }'|xargs docker stop docker ps -a | grep \"Exited\" | awk '{print $1 }'|xargs docker rm 同理，删除所有images docker images|grep none|awk '{print $3 }'|xargs docker rmi Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"面试点/基础面试.html":{"url":"面试点/基础面试.html","title":"基础面试","keywords":"","body":"技术面 [@TOC] 先做下自我介绍 做Python几年了？为什么选择Python？ 学历？大学什么专业？ 除了Python以外对其他语言有没有了解？ 了解过php和javascrpit，js是与浏览器交互的脚本语言，php也是一门面向web的语言，与apache mysql搭配效果也更好 你对Python这门语言的看法？ 单说python这门语言的话我们首先得了解它的特性与优势：python是一门具有解释性、编译性、互动性和面向对象的脚本语言 python有很强的可读性，对初学者十分友好 因为开源，所以也有很丰富的扩展库、与Linux Unix Windows兼容良好,可移植性好，现在像mac终端已经自带python了 python与php相比可以处理多进程多线程任务 语法没有java那么严格 java的可移植性是需要代价的 在学习Python过程中有没有令你影响深刻的事 有，在自己学习python的过程中经常会让我有一种惊叹，还有这种骚操作～ 比如django开发的时候，python manage的各种命令 django的filter ORM,restframework都省去了很多复杂的工序，大大提高了开发效率,还有我开始对python产生兴趣的首先就是爬虫,在面试第一家web后端工作时我就用爬虫爬过boss直聘，智联，拉勾等网站上的招聘信息，并把这些数据整合起来方便自己去找工作 字符串、字典、元组、列表常用方法？ 字符串常用的是split、replace、join、find、strip、just 列表常用 pop 、append、 remove、 insert 、clear 、len 、sort、 reverse 字典常用 get、 index、 keys、 values、 update pep8规范？ 函数闭包的理解？ 是指函数中嵌套函数 且外层函数的返回值也是函数 闭包中需要注意的是变量的作用域，内层函数使用外层函数的值需要加nolocal 使用全局变量的值需要加global Python函数的作用域？ 函数传入参数时要注意什么？如： func(arg,names=[]):… 形参与实参是一一对应的、形参的长度也是可变的、形参带可以把实参作为列表传入 两个星会作为字典 什么是装饰器？应用场景？ 装饰器的特点是返回值和参数都是函数 装饰器的目的就是对已封装函数进行操作，为其加上新的功能或一系列运算 。 在django中有middleware中间件，它其实就是高级的装饰器用法， def outside(func): def inside(str): func(str) #函数体 return inside @outside def hello(str): #函数体 hello(str) 生成器、迭代器和可迭代对象区别和应用？ 能使用for遍历的就叫迭代对象 ，能用next函数的叫迭代器 请一行写出 9*9 乘法表 深浅拷贝？ 线程、进程和协成？应用？ IO多路复用？ with 上下文机制原理？ Python内存管理? TCP与UDP协议区别? Python2x与Python3x的区别? Unicode. python3是默认utf-8编码的 除法运算 两个整数相除返回浮点数 而在2x版本可以整除的返回整数 捕获异常机制也从except exc,var 改为 except exc as var python3取消了xrange函数。xrange更像是个生成器 它是惰性取值的 去掉了long类型。只用int表示整型 不等运算符去掉了<> 只存在！= 去掉了``repx表达式 面向对象部分 面向对象优缺点： 解决了程序的扩展性，对某个对象单独修改 会立刻反映到整个体系，在调用时只需要实例化一个对象，对象就会拥有所有的属性 缺点是可控性差，无法像面向过程的程序设计一样精准的预测问题的处理结果 三大特性以及解释？ 封装性多态性 继承性 多态性就是在不考虑实例的类型的情况下使用实例 封装性 它可以隐藏对象的属性和实现细节，仅对外提供公共访问方式 面向对象继承时要注意什么？ 继承是一种创建新类的方式，在python中，新建的类可以继承一个或多个父类 继承分单继承和多继承 多继承需要注意继承的顺序， 而且子类继承父类后如果方法命名与父类相同 则会覆盖 但在python3中。可以使用super函数来调用父类方法 深度优先和广度优先是什么？ python的类可以继承多个类 如果继承多个 则寻找方式就有两种 深度优先 广度优先 经典类按深度优先方式查找 新式类按广度优先方式查找 面向对象中的一些特殊方法都记得那些？如，call… str\\ getattr\\ setattr\\ getatttribute 如何理解元类 元类就是负责帮你创建类的类 ， new和 init 方法？ 单例模式 ----------- 单例模式是一个对象只能有一个类。一个单例类只能有一个实例 staticmethod、classmethod,property property是一种特殊属性，访问它时会执行一段功能然后访问值 staticmethod本质就说实现了get set delete方法 经典类和新式类 经典类和新式类在写法上的区别是是否继承了object类。新式的必然有很多新的功能 数据库部分 数据库引擎？ innob引擎 Memory引擎 Merge引擎 innob引擎一大特点就是支持外键。 内存和空间大 支持事务 数据库锁？ 由于数据库是多用户共享资源，所以需要处理并发问题，而数据库锁的机制就是为了处理这一问题，当出现并发的时候，如果不做控制就会出现各种问题 比如脏数据。修改丢失等 所有数据库并发需要事务来控制，事务并发问题需要数据库锁来控制 事务四个特性。持久型 原子性 一致性 隔离性 数据库锁有 乐观锁 悲观锁 死锁。活锁 行锁 表锁 页级锁 排它锁有称为写锁。共享锁又称为读锁 你怎么理解ORM ?ORM相关操作 selected_related和prefetch_related是什么？ Q和F queryset的常用方法 user（用户）,group（部门）,role（角色）三个表： 查询年龄大于18的人 查询年龄不等于18的人 查询 IT部 和 运维部的所有人？ 查询 IT部 或 运维部的所有人？ 查询角色是 “管理员” 的所有人？ 设计数据库：会议室预定 设计数据库：员工、部门、角色； 查询 “IT”部门所有人 查询 每个部门 的员工数量？ 查询 每个部门 年龄不等于 18的人 查询部门人数不满 5 的部门？ 数据库优化方案？ 尽量避免使用select * 能用字段名就用字段名。避免查询无用字段 select count(*)会查全表 尽量避免 建表时字段类型能用varchar/nvarchar就不要用char/ncahr 避免频繁的创建和删除临时表 会耗费性能资源 产生大量log 如果使用临时表 在使用的最后一定要显示删除 先trancate table 再drop table 尽量避免大事务操作，提高并发效率 避免向客户端返回大数据量 数据量过大 应考虑需求是否合理 比如你在一个在线网站使用delete和update操作。必然会引发数据库锁 数据库索引以及注意点？ 什么情况下建索引？ 数据库索引种类？ delete和truncate区别？ 数据库中出现乱码？如何解决？ 执行计划和慢日志？ 数据库读写分离？ 用过什么ORM框架？ ORM缺点和优点？ 前端部分 前端是自己写？还是有前端开发？ 了解的前端框架？ js的面向对象有没有了解？ js作用域？ js中的this要注意什么？ 跨域是什么？解决方案？ Web框架部分 Http协议 列举Http请求方法？ 列举Http常用请求头？ 列举Http状态码？ Django请求生命周期？ 什么是wsgi？ 请求： 请求方法 请求地址 请求内容 请求头 请求环境 响应： 状态吗 响应数据 响应头 wsgi的作用就是将上面数据在server端和python程序间进行传递 它是一个标准 一个需要遵从的规范 才能正常工作 Django django遵从mvc框架 是由python编写的开源WEB框架 目录。urls.py 路由分发 将url与view一一映射 wsgi.py 关于程序与服务器端的规范 或者说统一的接口 Django信号作用？应用？ 有没有用过单元测试？ Django 中间件作用？应用？ 中间件是服务器端与应用程序的一个中间层，它将个管道一样。将接受到的请求进行一些处理。然后传递到客户端 然后把客户端处理的结果再返回 它的应用场景是 根据url把请求给到不同的客户端程序 允许多个客户端 负载均衡和远程处理 应答的过滤处理 Django处理并发？ FBV和CBV cookie和session区别以及实现原理？ Rest Framework 什么是rest api？ restful framework框架：认证、权限和访问频率 如果控制版本？ 项目部分 项目开发周期？ 项目组人数？ 如何部署？ Git WebSocket 其他 消息队列 爬虫 你接触过爬虫吗 使用过哪些爬虫框架？ 你爬过些哪些内容 爬过哪些app 扩展 Docker是什么 用过命令 什么好处 进程与线程的关系 1）一个线程只能属于一个进程，而一个进程可以有多个线程，但至少有一个线程 （2）资源分配给进程，进程是程序的主体，同一进程的所有线程共享该进程的所有资源 （3）cpu分配给线程，即真正在cpu上运行的是线程 （4）线程是最小的执行单元，进程是最小的资源管理单元 协程的优缺点： 优点： 　　上下文切换消耗小 　　方便切换控制流，简化编程模型 　　高并发，高扩展性，低成本 缺点： 　　无法利用多核 　　进行阻塞操作时会阻塞掉整个程序 　　单纯的协程是没有意义的，只是人为的控制执行一下这个，执行一下那个，如果想监测是否有IO操作，需要结合IO多路复用 Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"面试点/目前掌握的技术.html":{"url":"面试点/目前掌握的技术.html","title":"目前掌握的技术","keywords":"","body":"目前掌握的技术 1、git熟练 2、docker熟悉 3、django、flask熟悉 4、能快速构建项目及项目所需api，并针对不同项目作出技术选型 5、前端vue掌握、react熟悉 6、做过支付、crm系统 7、熟练应用mysql、redis、postgres、mongo等数据库 8、linux、centos、ubuntu、macos系统基本操作 9、掌握nginx部署 Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"面试点/直击痛点的python面试题.html":{"url":"面试点/直击痛点的python面试题.html","title":"直击痛点的python面试题","keywords":"","body":"python 面试 网上找的面试题大都不痛不痒的，感觉是一个开发人员必须掌握的知识点，还停留在概念层面，本文将其换种问法，以应用层的角度来剖析自己，为自己以后的面试打下基础。 目录 [TOC] 正文 WEB开发 1.你做过哪些项目？说说你的项目的开发流程。 项目： 相信大家自己也准备了，略 开发流程： 我是在一家产品公司工作的，公司采用的是敏捷开发模式。 首先，（提出需求）Boss提出产品规划路线，明确各季度目标，每个季度最低迭代，并提出迭代需求，细化到每次迭代的功能点 （产品开发沟通）产品经理与开发人员讨论具体细节（功能实现难点，数据库建表难点，前端开发所需时间），明确各自任务，通过任务看板记录各人任务完成情况 （开始立项）产品写需求文档，画原型图，UI与产品沟通所需素材。。。（这块不是很了解）后端搭建服务器，数据库，GIT，测试环境，生产环境以及开发环境，前端搭建前端框架（其实这段时间感觉前端蛮闲的） （产品开发沟通）其实后边沟通很多的，因为防止返工，开发之间需要确认好编码风格，统一命名风格等（写成文档），前端确定UI框架，前端编写测试demo，测试后端接口。 （测试阶段）测试人员提交bug（什么黑白盒自动化测试这个真不懂） （迭代完成，部署上线）最终测试产品完美运行时（无bug？不可能的，有些问题特别难解决但又不是严重错误的，比如前端对ie兼容性的bug，就说服测试），发布上线。 顺便提一下外包公司的开发流程，大部分采用瀑布式开发，简单粗暴 1，运维给出客户需求，报价，工时，素材，竞品（就是模仿哪家的产品，也可以没有。不过价钱要+） 2、技术总监/产品经理（也可以身兼数职）与客户/运维沟通，给出一份细致的产品方案，给开发人员也准备了一份（对开发更友好）的需求文档。 3、开始立项，严格按照需求文档要求（一般都会很全，需求至上） 4、边开发边测试，开发及时处理测试出的bug 5、完成项目雏形，满足部分需求，让客户看，满意就继续，不满意商量接下来的解决方案（加钱，因为严格按需求来的） 6、完成测试 部署上线，交接项目 项目的开发模式 2.项目中遇到的印象较深的bug，如何解决？ 我相信大家都遇到过卡住很久的bug，但猛的问起来，大脑一片空白 下边的面试题有些可以作为这个问题的答案，继续往下看 3.使用过git吗，如何回退版本，如何解决提交冲突？ 肯定用过，作为后端开发，git没用过说不过去了，即使公司常用svn，对git也该熟悉的 先说下回退版本的应用场景 开发人员A，B，两人往一个git项目提交代码，A提交了一个错误的版本，导致B git pull以后出现问题，现在需要回退到上个好的版本 git checkout b_branch //先回到自己的分支 git reflog //接着看看当前的commit id,例如:0bbbbb git reset --hard B1 //回到被覆盖的那次提交B1 git checkout -b b_backup //拉个分支，用于保存之前因为回退版本被覆盖掉的提交B1 git checkout b_branch //拉完分支，迅速回到自己分支 git reset --hard 0bbbbbb //马上回到自己分支的最前端 git checkout master //切换到master git merge tony_backup //再合并一次带有B1的分支到master 简单来说就是先查看错误提交之前的版本id reset命令回到正确版本 拉个分支保存正确版本 然后回到自己分支将正确版本合并 提交代码冲突的应用场景 A、B同时修改了一块代码，A提交之后，B也提交，发现有修改需要先pull ，Bpull代码发现报错 提示merge有冲突 git pull origin master --allow-unrelated-histories git status 这种做法是将代码拉下来手动修改冲突部分 Git高级教程二 版本回退 4. 使用过Docker吗？说说其原理 Doker是Linux开发环境的一种容器，是目前最流行的 Linux 容器解决方案。 Docker 将应用程序与该程序的依赖，打包在一个文件里面。运行这个文件，就会生成一个虚拟容器。程序在这个虚拟容器里运行，就好像在真实的物理机上运行一样。有了 Docker，就不用担心环境问题。 docker简介 docker启动django 5. 在项目中用的web开发框架是什么，听过xxx框架吗？ 我在项目中用的是Django框架，因为它大而全，如果考虑到开发效率，我会选择用它。并且其内置的Django一些中间件也解决了web安全问题，对扩展库的支持也很好，ORM对数据库的操作也很方便。典型的MVC框架。 听过或用过就简单聊两句，介绍下其优缺点，使用场景，没听过在后边也表明自己的阅读文档能力较强，上手快。 Python的WEB框架有Django、Tornado、Flask 等多种，Django相较与其他WEB框架其优势为：大而全，框架本身集成了ORM、模型绑定、模板引擎、缓存、Session等诸多功能 这块也顺便去了解下MVC架构 django中间件 django的MVC(MTV) 6. 你会些前端知识吗？有没有了解一些他们的框架 现在大部分web开发项目都是前后端分离的，主要是让后端更注重服务器和数据层，前端更注重视图，前后端各司其职。但是后端如果不懂前端代码的话遇到问题是没有办法更高效的解决的。所以我平时有关注前端的一些知识。不过我更关注于JS这些业务逻辑，也了解了VUE框架，因为本身Vue框架融合了Angular和React等主流前端WEB框架的一些特性, 所以其他框架也容易看懂。毕竟前端有时候会问一些框架问题。所以自然而然的也会一些。 vue与其他框架的对比 7.使用的版本控制工具是什么？ 这个问题其实就是问把代码提交到哪托管,！一般公司不会把项目托管到github上去的 我们公司用的是bitbucket，这是个国外的代码托管平台， 有个tracup平台用来管理项目成员，发布任务，也可以把bitbucket上的bug，任务，功能片段放在上面 而且在移动端也有相配套的app用来查看托管的代码 国内平台有coding.net，码云等（最近用coding.net有些坑，版本更新之后用的很不舒服） 8.会用ajax吗？你们项目使用的是什么获取数据？ 其实这个问题前后端都会问。。毕竟是交互的重点，后端偶尔也会使用来测试接口 会用，项目用的是axios，axios结合前端的es语法，代码简洁，功能强大，支持异步、get,post,delete多种请求方式 9.了解pep8规范吗？你们项目中有用到其他规范吗？ pep8规范的话算是比较熟悉了，一开始了解一门语言掌握了它的基本语法之后就要开始注重他们的规范了，pep8是python的一种书写规范，它大大增强了代码的可读性。 比如缩进是使用tab还是空格，导入依赖的上下文顺序，类与函数之间的间隔，字符串过长使用换行等。 项目组开发人员的书写规范都是统一的，只要不影响可读性可扩展性，都是可以的。 pep8规范 概念层面 1.一行代码写出99乘法表 print('\\n'.join([' '.join(['%s*%s=%s' % (y,x,x*y) for y in range(1,x+1)]) for x in range(1,10)])) 2.session与cookie的区别 Cookie是保存在用户浏览器端的键值对，Session是保存在服务器端的键值对；Cookie做用户验证的时，敏感信息不适合放在Cookie中，别人可以分析存放在本地的Cookie并进行Cookie欺骗，考虑到安全应当使用Session；用户验证时两者要结合使用，Session可保存到文件，内存，数据库任意地方 session在django中的使用 cookie在django中的使用 3.tcp和udp的区别 TCP：面向连接、传输可靠(保证数据正确性,保证数据顺序)、用于传输大量数据(流模式)、速度慢，建立连接需要开销较多(时间，系统资源)。 UDP：面向非连接、传输不可靠、用于传输少量数据(数据包模式)、速度快。 Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "},"面试点/项目面试.html":{"url":"面试点/项目面试.html","title":"项目面试","keywords":"","body":"1.restful api 定义：rest是一种系统架构设计风格 一种分布式系统的应用层解决方案， 目的：其目的是实现client和server端进一步解藕、 协议：https协议 ​ 一般将api部署在专用域名下,路径，版本号 http://api.example.com/v1/dir?filter 返回结果 {\"link\": { \"rel\": \"collection https://www.example.com/zoos\", \"href\": \"https://api.example.com/zoos\", \"title\": \"List of zoos\", \"type\": \"application/vnd.yourformat+json\" }} 你在项目中建过哪些表？数据库优化？查询优化？索引需要注意些什么？ 网站的优化，哪些性能？怎么优化？ WAF安全了解多少 项目中处理的数据量有多少？ 第三方授权登录怎么实现 分页排序如何实现 站内信功能怎么实现 Django paginator怎么实现的分页 redis与memcached缓存怎么用 csrf是什么 django中怎么实现csrf 密码加盐‘ 爬虫抓去各粉丝数量有哪些坑 为什么不让前端来分析图表数据 MVC MVVM MTV理解 FLASK Django区别、JINJA2是什么 restful是什么 apache nginx各自优缺点 线程池、断点续传怎么实现的 遇到过哪些反爬虫 如何反反爬虫 selenuim git svn 熟悉的命令 Docker部署过程 Copyright © housegod.cn 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-12-12 16:22:44 "}}