#10 分布式爬虫
## 10.1 分布式爬虫的优点
可以充分地利用多个电脑的资源。
## 10.2 如何实现分布式爬虫
  scrapy本身支持分布式吗？
  不支持的！！！
  为什么不支持呢？
  scrapy的url队列存在哪里？  （单机内存）
  如何实现分布式呢？
  替换url队列的存储介质    （redis支持分布式的内存数据库）

  为scrapy做一个新的调度器(redis)，替换scapy的默认调度器,  从而实现分布式功能。

## 10.3 scrapy-redis 
  scrapy-redis是scrapy的一个组件（插件），和 scrapy  、redis配合。从而实现支持分布式爬虫
  start_urls=  ['http://www.dushu.com' ]   # 以前
  redis-cli lpush myspider:start_urls 'http://www.dushu.com' # 使用分布式



# 使用Scrapy框架+Scrapy-redis组件实现分布式爬虫

## scrapy-redis是什么？

scrapy-redis是大神们写的一个scrapy的组件，主要用来负责分布式爬虫的调度任务它依赖于Scrapy和redis。

scrapy-redis提供了几个新的组件（新类）用来补充scrapy不能实现分布式的问题，它们分别是Scheduler、Dupefilter、Pipeline和Spider。

scrapy用类似Python中collection.deque的对象来保存待爬取的urls，

scrapy-redis用redis数据库来保存待爬取的urls（redis支持分布式，支持队列共享）

![scrapy_with_redis](data/scrapy_with_redis.png)

**redis在爬虫系统中的作用：**

1. 存储链接
2. 和scrapy一起工作，redis用来调度spiders（多个spider共用一个redis队列，即分布式处理）

**充分利用redis结构的作用：**

set：set中没有重复元素，利用这个特性，可以用来过滤重复元素；

list：实现队列和栈的功能；

sorted set：元素需要排序的时候用；

hash：存储的信息字段比较多时，可以用hash；





## 使用scrapy-redis实现分布式处理的步骤

## 创建项目

```Ruby
scrapy  startproject example-project
cd example-project
scrapy genspider dmoz dmoz.org
scrapy genspider myspider_redis  dmoz.org
scrapy genspider mycrawler_redis dmoz.org
```

### 编写代码

（这里我们直接使用官方网站的演示代码，演示分布式的配置、运行等）

使用scrapy redis，需要注意以下几个区别：

1. 传统的spiders中，每个spider类继承的是scrapy.Spider类或Scrapy.spiders.CrawlSpider类，而分布式写法每个spider继承的是scrapy_redis.spiders.RedisSpider或scrapy_redis.spiders.RedisCrawlSpider类。

   ```python
   from scrapy_redis.spiders import RedisCrawlSpider
   class MyCrawler(RedisCrawlSpider):
       ......
   ```

   

2. 在分布式写法中，start_urls=[]换成了 redis_key = 'myspider:start_urls'

   ```python
   class MyCrawler(RedisCrawlSpider):
       """Spider that reads urls from redis queue (myspider:start_urls)."""
       name = 'mycrawler_redis'
       redis_key = 'mycrawler:start_urls'
   ```

3. 在分布式写法中，  allowed_domains = ['dmoz.org']  换成了

   ```python
   domain = kwargs.pop('domain', '')
   self.allowed_domains = filter(None, domain.split(',')) 
   ```

### 搭建分布式爬虫环境

环境准备：4台服务器，一个做master，另外3个做slave。

scrapy、scrapy-redis、redis



**master服务器的配置：**

1. 安装scrapy、scrapy-redis、redis。

2. 修改master的redis配置文件redis.conf：

   1）将 bind 127.0.0.1 修改为bind 0.0.0.0。（注意防火墙设置）

   2）重启redis-server。

3. 在爬虫项目中的setting.py文件中添加配置信息：

   ```python
   REDIS_HOST = 'localhost'
   REDIS_PORT = 6379
   ```

**slave端的配置：**

1. 在爬虫项目中的setting.py文件中添加配置信息：

   ```python
   REDIS_URL = 'redis://your ip:6379'
   ```

**master和slave端中共同的配置**

在setting.py中启用redis存储

```python
ITEM_PIPELINES = {
    'scrapy_redis.pipelines.RedisPipeline': 400,
}

```

**运行分布式爬虫**

```
scrapy runspider myspider_redis.py
```

```
redis-cli> lpush myspider_redis:start_urls [the url]
```





详细参考官方文档：https://github.com/rmax/scrapy-redis

 