### 一. csrf模拟跨站请求

```html
<form>
	<input name="username">
    <input name="password">
    {% csrf %}
</form>
```

其中这个csrf其实就说csrf中间件做的登录验证中重要的一环 

```html
{%csrf%} == <input value=Token>  (input不会显示传输)
```



将登录时server端生成的token与csrf传的value值做比对

 如果相等则登录

不相等则报403

所以为了安全  尽量不要注释/删除 csrfmidlleware哦

### 二. web常用工具

#### 发送request

1. urllib
2. requests
3. Aiohttp（异步io）

#### 解析responese

1. lxml
2. bs4
3. re

####Cookie的处理

#### header信息的伪装

#### Scrapy

### 三. 模拟浏览器

<a href="https://www.cnblogs.com/BigFishFly/p/6380024.html">Selenuim</a>

<a href="http://www.51testing.com/zhuanti/webdriver.htm">Webdriver</a>

### 四. 多线程爬虫

```python
from queue import Queue
import threading
import requests
import re
import redis

start_url = "http://www.geyanw.com"
# 队列,存放要爬取的url
# urls_queue = Queue()
# 注意:如果脚本在slaver端运行,host应指向Master的IP
# 如果脚本在Master端运行,host直接写localhost即可.
urls_queue = redis.Redis(host='要修改的', port=6379, db=0)
# 并发下载线程数
DOWNLOADER_NUM = 2
# 线程池
thread_pool = []


def fetch(url):
    """
    根据url获取url对应的内容,并从网页中提取要爬取的url。
    把提取的url put 到队列。
    Args：
        url：要下载的页面的地址
    Returns:
        text:网页的内容
    """
    try:
        r = requests.get(url)
        html = r.content
        text = html.decode('gb2312')
        # text = html.decode('ISO-8859-1')
        return text
    except Exception as e:
        print(e)
    else:
        # 检测http返回的状态码是否正常
        r.raise_for_staus()


def parse(html):
    """
    对一级页面进行解析，解析html源码中的内容。
    Args：
        html：网页的源码, html的类型是str。
    """
    pattern = re.compile(r'href="(/[a-z0-9-/]+(.html)?)"')
    urls = pattern.findall(html)
    for url in urls[:5]:
        print(url[0])
        urls_queue.sadd('links', start_url + url[0])
        # urls_queue.put(start_url + url[0])


def parse_detail(html):
    """
    解析详情页中的内容，从详情页中抽取数据。
    Args:
        html:详情页的源代码。
    """
    from bs4 import BeautifulSoup
    soup = BeautifulSoup(html, "lxml")
    print(soup.text)


def downloader():
    """
    从url队列中提取url，下载url对应的页面。
    每个url都是一个详情页的地址。
    Returns:
        None
    """
    # 不停地从url队列中取url，如果url不是None，下载url页面，并进行解析。
    while True:
        # url = urls_queue.get()
        url = urls_queue.spop('links')
        if url is not None:
            print(url)
            html = fetch(url)
            parse_detail(html)


def main():
    """
    在主线程中初始化url队列。
    根据DOWNLOAD_NUM的设置，启动多个线程，多个线程并发地从
    url队列中获取url，执行url页面的下载。
    """
    # 主线程中初始化队列
    # 初始化的操作只在Master端运行即可.
    # html = fetch(start_url)
    # parse(html)
    # 启动多个子线程
    for i in range(DOWNLOADER_NUM):
        t = threading.Thread(target=downloader)
        t.start()
        thread_pool.append(t)
    # 阻塞队列，直到队列中没有任何url
    # urls_queue.join()
    # 阻塞线程，直到所有线程结束
    for t in thread_pool:
        t.join()


if __name__ == '__main__':
    main()

```

###五. 分布式

- 什么是分布式

- 为什么使用？

  可以实现并行

- 如何搭建

  一台电脑作为控制端（master）其他作为普通任务执行者(Slaver)

  一台服务器开启redis 其余服务器去连接  除了host之外代码都相同

- 如何实现分布式爬虫

  - 需要一个分布式队列 用来存放urls队列



### 六 附件

<a href="http://blog.fens.me/linux-redis-install/">redis在windows ubuntu上安装配置测试</a>

都配置好了多台服务器开启就行了   

需要说明的是

1. mac下redis安装 brew install redis
2. 注意master防火墙是否开启6379端口