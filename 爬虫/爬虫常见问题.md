###爬虫常见问题

###Request回调没有反应

> 因为domain不匹配默认被过滤掉，
>
> 可在domain中加入url或在request请求中加入`dont_filter=True`

###`403`响应

```ruby
[scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
[scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
[scrapy.core.engine] DEBUG: Crawled (403) <GET http://zipru.to/robots.txt> (referer: None) ['partial']
[scrapy.core.engine] DEBUG: Crawled (403) <GET http://zipru.to/torrents.php?category=TV> (referer: None) ['partial']
[scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 http://zipru.to/torrents.php?category=TV>: HTTP status code is not handled or not allowed
[scrapy.core.engine] INFO: Closing spider (finished)
```

- 如果手动访问该链接正常 首先检查UserAgent参数
- 可以将`settings.py`文件中的

```python
# Crawl responsibly by identifying yourself (and your website) on the user-agent
#USER_AGENT = 'zipru_scraper (+http://www.yourdomain.com)'
```

修改为

```python
USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.95 Safari/537.36'
```

为了使我们的爬虫访问表现得更像人类的操作，让我们降低请求速率(原理上借助[AutoThrottle 拓展](https://link.jianshu.com/?t=https://doc.scrapy.org/en/latest/topics/autothrottle.html))，在`settings.py`中继续添加

```
CONCURRENT_REQUESTS = 1
DOWNLOAD_DELAY = 5
```

###`302`重定向

- 因为请求次数过多 所以有时会被重定向到其他页面 甚至会需要填验证码

  对于爬虫代码当然不会填写验证 所以爬虫就此中断

- 我们可以定制`redirect middleware`（重定向中间件）,在遇到302时我们希望它绕过这些防范措施 为会话添加访问Cookie，最终请求到原始网页

```python
# middlewares.py
import os, tempfile, time, sys, logging
logger = logging.getLogger(__name__)

import dryscrape
import pytesseract
from PIL import Image

from scrapy.downloadermiddlewares.redirect import RedirectMiddleware

class ThreatDefenceRedirectMiddleware(RedirectMiddleware):
    def _redirect(self, redirected, request, spider, reason):
        # 如果没有特殊的防范性重定向那就正常工作
        if not self.is_threat_defense_url(redirected.url):
            return super()._redirect(redirected, request, spider, reason)

        logger.debug(f'Zipru threat defense triggered for {request.url}')
        request.cookies = self.bypass_threat_defense(redirected.url)
        request.dont_filter = True # 防止原始链接被标记为重复链接
        return request

    def is_threat_defense_url(self, url):
        return '://zipru.to/threat_defense.php' in url
```

我们继承了`RedirectMiddleware`中间件，使得我们可以复用内置的重定向处理操作，而只需要将我们的代码插入`_redirect(redirected, request, spider, reason)`方法，一旦构建了重定向请求，它会被`process_response(request, response, spider)`方法调用。对于非防范性重定向，我们调用父类的标准方法处理。这里，我们还未实现`bypass_threat_defense(url)`方法，但是它任务很明确，就是返回访问cookies，使得我们可以刷新原始请求的cookies，而重新处理原始请求。

为了启用我们的新中间件，要在`zipru_scraper/settings.py`中添加如下内容：

 ```python
DOWNLOADER_MIDDLEWARES = {
    'scrapy.downloadermiddlewares.redirect.RedirectMiddleware': None,
    'zipru_scraper.middlewares.ThreatDefenceRedirectMiddleware': 600,
}
 ```

它禁用了默认的重定向中间件，并将我们自己的中间件插入到相同的位置。此外，我们还需要安装一些需求包：

```Ruby
pip install dryscrape # headless webkit 无头webkit
pip install Pillow # image processing 图像处理
pip install pytesseract # OCR 字符识别
```

要注意，这些包都具有pip不能处理的外部依赖。如果安装出错，你需要访问[dryscrape](https://link.jianshu.com?t=http://dryscrape.readthedocs.io/en/latest/installation.html)，[Pillow](https://link.jianshu.com?t=http://pillow.readthedocs.io/en/3.4.x/installation.html)以及 [pytesseract](https://link.jianshu.com?t=https://github.com/madmaze/pytesseract)来获取安装指导。

- 接下来，我们只需要实现`bypass_thread_defense(url)`。我们可以解析JavaScript来获取需要的变量，并在python中重建逻辑，不过那显得琐碎而麻烦。让我们选择笨拙而简单的做法，使用无头webkit实例。这有不少选择，不过我独爱[dryscrape](https://link.jianshu.com?t=https://dryscrape.readthedocs.io/en/latest/index.html)（之前安装的）。
- 首先，在我们的中间件构造器里初始化一个dryscrape会话。

```python
    def __init__(self, settings):
        super().__init__(settings)

        # start xvfb to support headless scraping
        if 'linux' in sys.platform:
            dryscrape.start_xvfb()

        self.dryscrape_session = dryscrape.Session(base_url='http://zipru.to')
```

你可以把这个会话当作一个浏览器标签，它会做所有浏览器通常所做的事（如获取外部资源，获取脚本）。我们可以在选项卡中导航到新的URL，点击按钮，输入文本以及做其它各类事务。Scrapy支持请求和项目处理的并发，但响应的处理是单线程的。这意味着我们可以使用这个单独的dryscrape会话，而不用担心线程安全。

- 现在我们来看一下绕过服务器防御的基本逻辑。

```python
    def bypass_threat_defense(self, url=None):
        # 有确实的url则访问
        if url:
            self.dryscrape_session.visit(url)

        # 如果有验证码则处理
        captcha_images = self.dryscrape_session.css('img[src *= captcha]')
        if len(captcha_images) > 0:
            return self.solve_captcha(captcha_images[0])

        # 点击可能存在的重试链接
        retry_links = self.dryscrape_session.css('a[href *= threat_defense]')
        if len(retry_links) > 0:
            return self.bypass_threat_defense(retry_links[0].get_attr('href'))

        # 否则的话，我们是在一个重定向页面上，等待重定向后再次尝试
        self.wait_for_redirect()
        return self.bypass_threat_defense()

    def wait_for_redirect(self, url = None, wait = 0.1, timeout=10):
        url = url or self.dryscrape_session.url()
        for i in range(int(timeout//wait)):
            time.sleep(wait)
            # 如果url发生变化则返回
            if self.dryscrape_session.url() != url:
                return self.dryscrape_session.url()
        logger.error(f'Maybe {self.dryscrape_session.url()} isn\'t a redirect URL?')
        raise Exception('Timed out on the zipru redirect page.')
```

- 这里我们处理了在浏览器访问时可能遇到的各种情况，并且做了一个正常人类会做出的操作。任何时刻采取的行动取决于当前的页面，代码以一种优雅的方式顺序处理变化的情况。

###验证码问题

- 最后一个谜题是解决验证码。有不少[解决验证码服务](https://link.jianshu.com?t=https://anti-captcha.com/)的API供你在紧要关头使用，但是这里的验证码足够简单我们可以用OCR来解决它。使用pytesseract做字符识别，我们最终可以添加`solve_captcha(img)`方法来完善我们的`bypass_threat_defense()`。

```Python
 def solve_captcha(self, img, width=1280, height=800):
        # 对当前页面截图
        self.dryscrape_session.set_viewport_size(width, height)
        filename = tempfile.mktemp('.png')
        self.dryscrape_session.render(filename, width, height)

        # 注入javascript代码来找到验证码图片的边界
        js = 'document.querySelector("img[src *= captcha]").getBoundingClientRect()'
        rect = self.dryscrape_session.eval_script(js)
        box = (int(rect['left']), int(rect['top']), int(rect['right']), int(rect['bottom']))

        # 解决截图中的验证码
        image = Image.open(filename)
        os.unlink(filename)
        captcha_image = image.crop(box)
        captcha = pytesseract.image_to_string(captcha_image)
        logger.debug(f'Solved the Zipru captcha: "{captcha}"')

        # 提交验证码结果
        input = self.dryscrape_session.xpath('//input[@id = "solve_string"]')[0]
        input.set(captcha)
        button = self.dryscrape_session.xpath('//button[@id = "button_submit"]')[0]
        url = self.dryscrape_session.url()
        button.click()

        # 如果我们被重定向到一个防御的URL，重试
        if self.is_threat_defense_url(self.wait_for_redirect(url)):
            return self.bypass_threat_defense()

        # 否则就可以返回当前的cookies构成的字典
        cookies = {}
        for cookie_string in self.dryscrape_session.cookies():
            if 'domain=zipru.to' in cookie_string:
                key, value = cookie_string.split(';')[0].split('=')
                cookies[key] = value
        return cookies
```

可以看到，如果验证码解析失败，我们会回到`bypass_threat_defense()`。这样我们拥有多次尝试的机会，直到成功一次。
 看起来我们的爬虫应该成功了，可是它陷入了无限循环中：

```Python
[scrapy.core.engine] DEBUG: Crawled (200) <GET http://zipru.to/robots.txt> (referer: None)
[zipru_scraper.middlewares] DEBUG: Zipru threat defense triggered for http://zipru.to/torrents.php?category=TV
[zipru_scraper.middlewares] DEBUG: Solved the Zipru captcha: "UJM39"
[zipru_scraper.middlewares] DEBUG: Zipru threat defense triggered for http://zipru.to/torrents.php?category=TV
[zipru_scraper.middlewares] DEBUG: Solved the Zipru captcha: "TQ9OG"
[zipru_scraper.middlewares] DEBUG: Zipru threat defense triggered for http://zipru.to/torrents.php?category=TV
[zipru_scraper.middlewares] DEBUG: Solved the Zipru captcha: "KH9A8"
...
```

> 看起来我们的中间件至少成功解决了验证码，然后重新发起请求。问题在于新的请求重又触发了防御机制。我一开以为bug在解析与添加cookies，可再三检查无果。这是另一个*“唯一可能不同的东西是请求头”*的情况。
>
>  Scrapy和dryscrape的请求头显然都绕过了触发`403`的第一层过滤器，因为我们没有得到任何`403`响应。但这肯定是因为某种请求头的差异造成的问题。我的猜测是，其中一个加密的访问Cookie包含了完整的原始访问请求头的哈希值，如果两次请求头不匹配，将触发威胁防御机制。这里的意图可能是防止某人直接将浏览器的cookies复制到爬虫中，但也只是增加了点小麻烦。

- 所以让我们在`zipru_scraper/settings.py`明确指定我们的请求头：

```python
DEFAULT_REQUEST_HEADERS = {
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    'User-Agent': USER_AGENT,
    'Connection': 'Keep-Alive',
    'Accept-Encoding': 'gzip, deflate',
    'Accept-Language': 'en-US,*',
}
```

注意这里，我们显式地使用之前定义的`USER_AGENT`赋值给`User-Agent`，虽然它已经被用户代理中间件自动添加，但是这样做会便于我们复制请求头到dryscrape中。下面修改我们的`ThreatDefenceRedirectMiddleware`的初始化函数为：

```python
def __init__(self, settings):
    super().__init__(settings)

    # start xvfb to support headless scraping
    if 'linux' in sys.platform:
        dryscrape.start_xvfb()

    self.dryscrape_session = dryscrape.Session(base_url='http://zipru.to')
    for key, value in settings['DEFAULT_REQUEST_HEADERS'].items():
        # seems to be a bug with how webkit-server handles accept-encoding
        if key.lower() != 'accept-encoding':
            self.dryscrape_session.set_header(key, value)
```

现在`scrapy crawl zipru -o torrents.jl`命令行运行，成功了！数据流不断涌出！并且都记录到了我们的`torrents.jl`文件里。

### Ip被封

> 小编之前在爬boss直聘时 ip就被封了 显示页面403，24小时后自动解封

这里可以用爬虫ip代理池

### 总结

- 解决了
  - Useragent过滤
  - 模糊的js重定向
  - 验证码
  - 请求头一致性检查

 

 

{'company': '第伍区科技',
 'job': 'Python 数字货币量化交易研发工程师',
 'money': '15k-30k',
 'people': [<Selector xpath='string(.)' data='智能硬件不需要融资500-999人'>,
            <Selector xpath='string(.)' data='数据服务不需要融资20-99人'>,
            <Selector xpath='string(.)' data='互联网不需要融资20-99人'>,
            <Selector xpath='string(.)' data='计算机软件A轮100-499人'>,
            <Selector xpath='string(.)' data='数据服务天使轮0-20人'>,
            <Selector xpath='string(.)' data='互联网金融未融资20-99人'>,
            <Selector xpath='string(.)' data='互联网已上市1000-9999人'>,
            <Selector xpath='string(.)' data='移动互联网不需要融资20-99人'>,
            <Selector xpath='string(.)' data='生活服务天使轮0-20人'>,
            <Selector xpath='string(.)' data='互联网B轮10000人以上'>,
            <Selector xpath='string(.)' data='移动互联网不需要融资20-99人'>,
            <Selector xpath='string(.)' data='数据服务已上市500-999人'>,
            <Selector xpath='string(.)' data='互联网已上市10000人以上'>,
            <Selector xpath='string(.)' data='互联网A轮20-99人'>,
            <Selector xpath='string(.)' data='数据服务B轮100-499人'>,
            <Selector xpath='string(.)' data='计算机软件已上市10000人以上'>,
            <Selector xpath='string(.)' data='基金不需要融资20-99人'>,
            <Selector xpath='string(.)' data='移动互联网已上市10000人以上'>,
            <Selector xpath='string(.)' data='互联网未融资20-99人'>,
            <Selector xpath='string(.)' data='互联网A轮20-99人'>,
            <Selector xpath='string(.)' data='计算机软件已上市10000人以上'>,
            <Selector xpath='string(.)' data='计算机软件已上市10000人以上'>,
            <Selector xpath='string(.)' data='互联网天使轮0-20人'>,
            <Selector xpath='string(.)' data='移动互联网不需要融资20-99人'>,
            <Selector xpath='string(.)' data='计算机软件已上市10000人以上'>,
            <Selector xpath='string(.)' data='互联网已上市100-499人'>,
            <Selector xpath='string(.)' data='移动互联网天使轮20-99人'>,
            <Selector xpath='string(.)' data='互联网B轮100-499人'>,
            <Selector xpath='string(.)' data='互联网未融资20-99人'>,
            <Selector xpath='string(.)' data='计算机软件A轮100-499人'>],
 'rongzi': [<Selector xpath='string(.)' data='智能硬件不需要融资500-999人'>,
            <Selector xpath='string(.)' data='数据服务不需要融资20-99人'>,
            <Selector xpath='string(.)' data='互联网不需要融资20-99人'>,
            <Selector xpath='string(.)' data='计算机软件A轮100-499人'>,
            <Selector xpath='string(.)' data='数据服务天使轮0-20人'>,
            <Selector xpath='string(.)' data='互联网金融未融资20-99人'>,
            <Selector xpath='string(.)' data='互联网已上市1000-9999人'>,
            <Selector xpath='string(.)' data='移动互联网不需要融资20-99人'>,
            <Selector xpath='string(.)' data='生活服务天使轮0-20人'>,
            <Selector xpath='string(.)' data='互联网B轮10000人以上'>,
            <Selector xpath='string(.)' data='移动互联网不需要融资20-99人'>,
            <Selector xpath='string(.)' data='数据服务已上市500-999人'>,
            <Selector xpath='string(.)' data='互联网已上市10000人以上'>,
            <Selector xpath='string(.)' data='互联网A轮20-99人'>,
            <Selector xpath='string(.)' data='数据服务B轮100-499人'>,
            <Selector xpath='string(.)' data='计算机软件已上市10000人以上'>,
            <Selector xpath='string(.)' data='基金不需要融资20-99人'>,
            <Selector xpath='string(.)' data='移动互联网已上市10000人以上'>,
            <Selector xpath='string(.)' data='互联网未融资20-99人'>,
            <Selector xpath='string(.)' data='互联网A轮20-99人'>,
            <Selector xpath='string(.)' data='计算机软件已上市10000人以上'>,
            <Selector xpath='string(.)' data='计算机软件已上市10000人以上'>,
            <Selector xpath='string(.)' data='互联网天使轮0-20人'>,
            <Selector xpath='string(.)' data='移动互联网不需要融资20-99人'>,
            <Selector xpath='string(.)' data='计算机软件已上市10000人以上'>,
            <Selector xpath='string(.)' data='互联网已上市100-499人'>,
            <Selector xpath='string(.)' data='移动互联网天使轮20-99人'>,
            <Selector xpath='string(.)' data='互联网B轮100-499人'>,
            <Selector xpath='string(.)' data='互联网未融资20-99人'>,
            <Selector xpath='string(.)' data='计算机软件A轮100-499人'>],
 'time': '发布于昨天',
 'url': 'https://www.zhipin.com/job_detail/564a190c33d4a05f1XV509-4FlY~.html?ka=search_list_2_blank&lid=U596cSZTDJ.search',
 'workyear': [<Selector xpath='string(.)' data='杭州  3-5年本科'>,
              <Selector xpath='string(.)' data='杭州  不限不限'>,
              <Selector xpath='string(.)' data='杭州  应届生大专'>,
              <Selector xpath='string(.)' data='杭州  3-5年本科'>,
              <Selector xpath='string(.)' data='杭州  1-3年本科'>,
              <Selector xpath='string(.)' data='杭州  3-5年大专'>,
              <Selector xpath='string(.)' data='杭州  3-5年本科'>,
              <Selector xpath='string(.)' data='杭州  3-5年大专'>,
              <Selector xpath='string(.)' data='杭州  1-3年大专'>,
              <Selector xpath='string(.)' data='杭州  1-3年不限'>,
              <Selector xpath='string(.)' data='杭州  1年以内本科'>,
              <Selector xpath='string(.)' data='杭州  1-3年大专'>,
              <Selector xpath='string(.)' data='杭州  不限本科'>,
              <Selector xpath='string(.)' data='杭州  1-3年本科'>,
              <Selector xpath='string(.)' data='杭州  1-3年大专'>,
              <Selector xpath='string(.)' data='杭州  1-3年本科'>,
              <Selector xpath='string(.)' data='杭州  应届生本科'>,
              <Selector xpath='string(.)' data='杭州  1-3年本科'>,
              <Selector xpath='string(.)' data='杭州  不限大专'>,
              <Selector xpath='string(.)' data='杭州  3-5年本科'>,
              <Selector xpath='string(.)' data='杭州  1-3年本科'>,
              <Selector xpath='string(.)' data='杭州  1-3年本科'>,
              <Selector xpath='string(.)' data='杭州  1-3年本科'>,
              <Selector xpath='string(.)' data='杭州  不限本科'>,
              <Selector xpath='string(.)' data='杭州  1-3年本科'>,
              <Selector xpath='string(.)' data='杭州  1-3年本科'>,
              <Selector xpath='string(.)' data='杭州  1年以内本科'>,
              <Selector xpath='string(.)' data='杭州  不限本科'>,
              <Selector xpath='string(.)' data='杭州  不限不限'>,
              <Selector xpath='string(.)' data='杭州  5-10年本科'>],
 'xueli': [<Selector xpath='string(.)' data='杭州  3-5年本科'>,
           <Selector xpath='string(.)' data='杭州  不限不限'>,
           <Selector xpath='string(.)' data='杭州  应届生大专'>,
           <Selector xpath='string(.)' data='杭州  3-5年本科'>,
           <Selector xpath='string(.)' data='杭州  1-3年本科'>,
           <Selector xpath='string(.)' data='杭州  3-5年大专'>,
           <Selector xpath='string(.)' data='杭州  3-5年本科'>,
           <Selector xpath='string(.)' data='杭州  3-5年大专'>,
           <Selector xpath='string(.)' data='杭州  1-3年大专'>,
           <Selector xpath='string(.)' data='杭州  1-3年不限'>,
           <Selector xpath='string(.)' data='杭州  1年以内本科'>,
           <Selector xpath='string(.)' data='杭州  1-3年大专'>,
           <Selector xpath='string(.)' data='杭州  不限本科'>,
           <Selector xpath='string(.)' data='杭州  1-3年本科'>,
           <Selector xpath='string(.)' data='杭州  1-3年大专'>,
           <Selector xpath='string(.)' data='杭州  1-3年本科'>,
           <Selector xpath='string(.)' data='杭州  应届生本科'>,
           <Selector xpath='string(.)' data='杭州  1-3年本科'>,
           <Selector xpath='string(.)' data='杭州  不限大专'>,
           <Selector xpath='string(.)' data='杭州  3-5年本科'>,
           <Selector xpath='string(.)' data='杭州  1-3年本科'>,
           <Selector xpath='string(.)' data='杭州  1-3年本科'>,
           <Selector xpath='string(.)' data='杭州  1-3年本科'>,
           <Selector xpath='string(.)' data='杭州  不限本科'>,
           <Selector xpath='string(.)' data='杭州  1-3年本科'>,
           <Selector xpath='string(.)' data='杭州  1-3年本科'>,
           <Selector xpath='string(.)' data='杭州  1年以内本科'>,
           <Selector xpath='string(.)' data='杭州  不限本科'>,
           <Selector xpath='string(.)' data='杭州  不限不限'>,
           <Selector xpath='string(.)' data='杭州  5-10年本科'>]} 

 

 

 

 

 

 

 