###1.美国各州人口数据分析

```python
import numpy as np
import pandas as pd
from pandas import Series,DataFrame
# 导入数据
abb = pd.read_csv('../data/state-abbrevs.csv')
areas = pd.read_csv('../data/state-areas.csv')
pop = pd.read_csv('../data/state-population.csv')

# 合并pop与abbrevs两个DataFrame，分别依据state/region列和abbreviation列来合并。

# 为了保留所有信息，使用外合并。
pop_abb = pd.merge(pop,abb,left_on='state/region',right_on='abbreviation',how='outer')

# 去除abbreviation的那一列（axis=1）
# inplace 修改原始数据
pop_abb.drop('abbreviation',axis=1,inplace=True)

# 查看存在缺失数据的列。
# 使用.isnull().any()，只有某一列存在一个缺失数据，就会显示True。
pop_abb.isnull().any()

# 查看缺失数据
null_data = pop_abb[pop_abb.isnull().any(axis=1)]
null_data

# 根据数据是否缺失情况显示数据，如果缺失为True，那么显示
# 找到有哪些state/region使得state的值为NaN，使用unique()查看非重复值
pop_abb[pop_abb['state'].isnull()]['state/region'].unique()
# 先查看state/region == PR的行
condition1 = (pop_abb['state/region'] == 'PR')
# 赋值操作
pop_abb.loc[condition1,'state'] = 'PUERTO'

# 为找到的这些state/region的state项补上正确的值，从而去除掉state这一列的所有NaN！
condition2 = (pop_abb['state/region'] == 'USA')
pop_abb.loc[condition2,'state'] = 'America'
# 查看结果
pop_abb[pop_abb['state/region'] == 'USA']
pop_abb[pop_abb['state/region'] == 'PR']
# 处理后的结果
pop_abb.isnull().any()

# 合并各州面积数据areas，使用左合并。
# 因为pop_abb是一个相对完整的表，如果只有面积没有人口是没有意义的，所以以left为准
total = pd.merge(pop_abb,areas,how='left')

# 继续寻找存在缺失数据的列
total.isnull().any()

# 我们会发现area(sq.mi)这一列有缺失数据，为了找出是哪一行，我们需要找出是哪个state没有数据
total[total['area (sq. mi)'].isnull()]['state'].unique()
drop_indexs = total[total['area (sq. mi)'].isnull()]['state'].index

# 去除含有缺失数据的行
total.drop(drop_indexs,inplace=True)
# 查看数据是否缺失
total.isnull().any()

# 找出2010年的全民人口数据,df.query(查询语句)
pop_2010 = total.query("ages == 'total' & year == 2010")
# 对查询结果进行处理，以state列作为新的行索引:set_index
pop_2010.set_index('state',inplace=True)

# 计算人口密度。注意是Series/Series，其结果还是一个Series。
pop_2010

#排序，并找出人口密度最高的五个州sort_values()
(pop_2010['population']/pop_2010['area (sq. mi)']).sort_values()
```

###2.美国2012政治献金

> months = {'JAN' : 1, 'FEB' : 2, 'MAR' : 3, 'APR' : 4, 'MAY' : 5, 'JUN' : 6,
>           'JUL' : 7, 'AUG' : 8, 'SEP' : 9, 'OCT': 10, 'NOV': 11, 'DEC' : 12}
>
> of_interest = ['Obama, Barack', 'Romney, Mitt', 'Santorum, Rick', 
>                'Paul, Ron', 'Gingrich, Newt']
> parties = {
>   'Bachmann, Michelle': 'Republican',
>   'Romney, Mitt': 'Republican',
>   'Obama, Barack': 'Democrat',
>   "Roemer, Charles E. 'Buddy' III": 'Reform',
>   'Pawlenty, Timothy': 'Republican',
>   'Johnson, Gary Earl': 'Libertarian',
>   'Paul, Ron': 'Republican',
>   'Santorum, Rick': 'Republican',
>   'Cain, Herman': 'Republican',
>   'Gingrich, Newt': 'Republican',
>   'McCotter, Thaddeus G': 'Republican',
>   'Huntsman, Jon': 'Republican',
>   'Perry, Rick': 'Republican'           
>  }

<a href="https://blog.csdn.net/weixin_38748717/article/details/78861607">政治献金</a>

###3.城市气候与海洋的关系

```python
import numpy as np
import pandas as pd
from pandas import Series,DataFrame
import matplotlib.pyplot as plt
%matplotlib inline

# 读数据
ferrara1 = pd.read_csv('./ferrara_150715.csv')
ferrara2 = pd.read_csv('./ferrara_250715.csv')
ferrara3 = pd.read_csv('./ferrara_270615.csv')
ferrara = pd.concat([ferrara1,ferrara2,ferrara3],ignore_index=True) 

torino1 = pd.read_csv('./torino_150715.csv')
torino2 = pd.read_csv('./torino_250715.csv')
torino3 = pd.read_csv('./torino_270615.csv')
torino = pd.concat([torino1,torino2,torino3],ignore_index=True) 

mantova1 = pd.read_csv('./mantova_150715.csv')
mantova2 = pd.read_csv('./mantova_250715.csv')
mantova3 = pd.read_csv('./mantova_270615.csv')
mantova = pd.concat([mantova1,mantova2,mantova3],ignore_index=True) 

milano1 = pd.read_csv('./milano_150715.csv')
milano2 = pd.read_csv('./milano_250715.csv')
milano3 = pd.read_csv('./milano_270615.csv')
milano = pd.concat([milano1,milano2,milano3],ignore_index=True) 

ravenna1 = pd.read_csv('./ravenna_150715.csv')
ravenna2 = pd.read_csv('./ravenna_250715.csv')
ravenna3 = pd.read_csv('./ravenna_270615.csv')
ravenna = pd.concat([ravenna1,ravenna2,ravenna3],ignore_index=True)

asti1 = pd.read_csv('./asti_150715.csv')
asti2 = pd.read_csv('./asti_250715.csv')
asti3 = pd.read_csv('./asti_270615.csv')
asti = pd.concat([asti1,asti2,asti3],ignore_index=True)

bologna1 = pd.read_csv('./bologna_150715.csv')
bologna2 = pd.read_csv('./bologna_250715.csv')
bologna3 = pd.read_csv('./bologna_270615.csv')
bologna = pd.concat([bologna1,bologna2,bologna3],ignore_index=True)

piacenza1 = pd.read_csv('./piacenza_150715.csv')
piacenza2 = pd.read_csv('./piacenza_250715.csv')
piacenza3 = pd.read_csv('./piacenza_270615.csv')
piacenza = pd.concat([piacenza1,piacenza2,piacenza3],ignore_index=True)

cesena1 = pd.read_csv('./cesena_150715.csv')
cesena2 = pd.read_csv('./cesena_250715.csv')
cesena3 = pd.read_csv('./cesena_270615.csv')
cesena = pd.concat([cesena1,cesena2,cesena3],ignore_index=True)

faenza1 = pd.read_csv('./faenza_150715.csv')
faenza2 = pd.read_csv('./faenza_250715.csv')
faenza3 = pd.read_csv('./faenza_270615.csv')
faenza = pd.concat([faenza1,faenza2,faenza3],ignore_index=True)

# 查看列数
faenza.columns
# 去除没用的列
citys = [ferrara,torino,mantova,milano,ravenna,asti,bologna,piacenza,cesena,faenza]
for city in citys:
    city.drop('Unnamed: 0',axis=1,inplace=True)
    
# 显示最高温度与离海远近的关系
max_temps = []
distances = []
for city in citys:
    max_temps.append(city['temp'].max())
    distances.append(city['dist'].mean())

plt.scatter(distances,max_temps)
# 把距离按照从小到大进行排序，使用线性图来查看结果
df = DataFrame(data = {
    'temp':max_temps,
    'distance':distances
})
sort_df = df.sort_values('distance')
plt.plot(sort_df.distance,sort_df.temp)

#观察发现，离海近的可以形成一条直线，离海远的也能形成一条直线。
#首先使用numpy：把列表转换为numpy数组，用于后续计算。
#分别以100公里和50公里为分界点，划分为离海近和离海远的两组数据
temps_np = np.array(max_temps)
dist_np = np.array(distances)
# 近海城市
near_dists = dist_np[dist_np<100]
near_temps = temps_np[dist_np<100]
# 远海城市
far_dists = dist_np[dist_np>50]
far_temps = temps_np[dist_np>50]

display(near_dists,near_temps)
display(far_dists,far_temps)
```

。。。歇一会

继续

```python
# 使用支持向量机计算回归参数
# 使用线性回归模型
from sklearn.linear_model import LinearRegression
# 构建算法模型
near = LinearRegression()
# 训练机器学习模型
# 训练【样本集】（自变量）必须是列向量
near.fit(near_dists.reshape(-1,1),near_temps)

# 获取预测数据
near_test = np.linspace(near_dists.min()-10,near_dists.max()+10,100).reshape(-1,1)
near_temps = near.predict(near_test)

far = LinearRegression()
far.fit(far_dists.reshape(-1,1),far_temps)
far_test = np.linspace(far_dists.min()-10,far_dists.max()+10,100).reshape(-1,1)
far_temps = far.predict(far_test)

plt.plot(near_test,near_temps,color='blue',label='near_predict')
plt.plot(far_test,far_temps,color='green',label='far_predict')
plt.scatter(distances,max_temps,color='red',label='true_data')
plt.legend()
```

查看最低温度与海洋距离的关系

最低湿度与海洋距离的关系

最高湿度与海洋距离的关系

平均湿度与海洋距离的关系

思考：模仿最高温度，得到平均湿度与海洋距离的回归曲线

风向与风速的关系

在子图中，同时比较风向与湿度和风力的关系

```python
# 查看某一个城市的风向跟风力的关系
plt.figure(figsize=(6,6))
axes1 = plt.subplot(2,1,1)
axes1.scatter(milano['wind_deg'],milano['wind_speed'])
axes1.set_title('deg-speed')

axes2 = plt.subplot(2,1,2)
axes2.scatter(milano['wind_deg'],milano['humidity'])
axes2.set_title('deg-humidity')
# 玫瑰图
plt.axes(polar=True)
plt.bar(milano['wind_deg'],milano['wind_speed'])
# 直方图
plt.bar(milano['wind_deg'],milano['wind_speed'])

deg = milano['wind_deg']
speed = milano['wind_speed']

mean_speeds = []
for d in np.arange(0,360,45):
    mean_speeds.append(speed[(deg<d+45) & (deg >=d)].mean())

index = np.linspace(0,2*np.pi,8,endpoint=False)
plt.figure(figsize=(6,6))
plt.axes(polar=True)
colors = np.random.random(size=(8,3))
plt.bar(x=index,height=mean_speeds,color=colors,align='edge')
plt.title('milano')

# 湿度跟风向的关系
mean_hum = []
humidities = milano['humidity']
for d in np.arange(0,360,45):
    mean_hum.append(humidities[(deg<d+45) & (deg >=d)].mean())
    
plt.figure(figsize=(6,6))
plt.axes(polar=True)
colors = np.random.random(size=(8,3))
plt.bar(x=index,height=mean_hum,color=colors,align='edge')
plt.title('milano')


```

由于风向是360度，我们可以考虑使用玫瑰图（极坐标条形图）

首先自定义一个画图函数

用numpy创建一个直方图，将360度划分为8个面元，将数据分类到这8个面元中

计算米兰各个方向上的风速

将各个方向的风速保存在列表中

画出各个方向的风速

将上面步骤写成函数

```python
def showbyrose(degs,others,title):
    # 求平均值（others）
    mean_others = []
    for d in np.arange(0,360,45):
        mean_others.append(others[(degs<d+45) & (degs >=d)].mean())
        
    plt.figure(figsize=(6,6))
    plt.axes(polar=True)
    colors = np.random.random(size=(8,3))
    plt.bar(x=index,height=mean_others,color=colors,align='edge')
    plt.title(title)
    
showbyrose(ferrara['wind_deg'],ferrara['wind_speed'],'deg-speed')
```

###4.人脸识别

```python
import numpy as np
import pandas as pd
from pandas import Series,DataFrame
from sklearn.datasets import fetch_lfw_people
# 降维 pca
from sklearn.decomposition import PCA
# 集成学习
from sklearn.ensemble import VotingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV
import matplotlib.pyplot as plt
%matplotlib inline

# 读取数据
faces = fetch_lfw_people(resize=0.4,min_faces_per_person=70)
# 拆分数据
data = faces.data
target = faces.target
names = faces.target_names
# 不涉及分类，使用pca
pca = PCA(n_components=0.9)
train = pca.fit_transform(data)
# 集成学习
estimators = []

model1 = LogisticRegression()
model2 = KNeighborsClassifier()
model3 = DecisionTreeClassifier()
model4 = SVC()

estimators.append(("LogisticRegression",model1))
estimators.append(("KNeighborsClassifier",model2))
estimators.append(("DecisionTreeClassifier",model3))
estimators.append(("SVC",model4))

# 生成算法集成对象
enstimator = VotingClassifier(estimators)

# 交叉验证(选择一个更加稳定的算法、其次看算法评分)
kfold = KFold(n_splits=10)

results = cross_val_score(enstimator,train,target,cv=kfold)
# 使用svc测量
result1 = cross_val_score(SVC(kernel='linear'),train,target,cv=kfold)
# 使用logistic测量
result2 = cross_val_score(LogisticRegression(),train,target,cv=kfold)
# 使用knn测量
cross_val_score(KNeighborsClassifier(),train,target,cv=kfold)
# 使用决策树测量
cross_val_score(DecisionTreeClassifier(),train,target,cv=kfold)
# 通过比较可以选择logistic或者svc进行测量
# 对比logistci和svc哪一个更稳定
print("方差比--SVC:{} Logistic:{}".format(result1.std(),result2.std()))
print("平均分--SVC:{} Logistic:{}".format(result1.mean(),result2.mean()))

# 最终选择Logistic回归
# 网格搜索找到最优参数
param_dic = {
    'C':[0.00001,0.001,0.1,10,100,1000,10000],
    'penalty':['l1','l2']
}

logistic = LogisticRegression()
result = GridSearchCV(logistic,param_grid=param_dic)

# 拆分数据集
from sklearn.model_selection import train_test_split
# X_train,X_test,y_train,y_test = train_test_split(train,target,random_state=1)
# 考虑到要展示预测结果，所以样本集的拆分，采用有序的方式进行
X_train = train[:1100]
y_train = target[:1100]
X_test = train[1100:]
y_test = target[1100:]
result.fit(X_train,y_train)
best_logistic = result.best_estimator_
# 使用最好的模型来进行预测，查看评分
best_logistic.score(X_test,y_test)
# 查看人脸图片
plt.imshow(data[0].reshape((50,37)),cmap='gray')
name = names[target[0]]
plt.title(name)
# 展示预测结果（把前100张预测数据的真实值和预测值一起展示）
true_test = data[1100:]
true_target = target[1100:]
predict_target = best_logistic.predict(X_test)

for i,name in enumerate(names):
    names[i] = name.split(' ')[-1]

# 最终展示
plt.figure(figsize=(16,24))
for i in range(100):
    axes = plt.subplot(10,10,i+1)
    data= true_test[i].reshape((50,37))
    plt.imshow(data,cmap='gray')
    pre_name = names[predict_target[i]]
    true_name = names[true_target[i]]
    title = 'T:'+true_name + '\nP:'+pre_name
    axes.set_title(title)
    axes.axis('off')
    
# 处理网络图片，查看预测结果
bush = plt.imread('bush.jpg')
bush = bush.max(axis=2)
small_bush = bush[65:165,240:314]
plt.imshow(small_bush,cmap='gray')
from skimage.transform import resize
# import scipy.misc as misc

resize_bush = resize(small_bush,(50,37))
plt.imshow(resize_bush)
bush_data = pca.transform(resize_bush.reshape(1,-1))
best_logistic.predict(bush_data)
names[1]
```

###5.病马死亡

```python
import numpy as np
import pandas as pd
from pandas import Series,DataFrame
import matplotlib.pyplot as plt
%matplotlib inline

# 读取数据
part1 = pd.read_table('./data/horseColicTraining.txt',header=None)
part2 = pd.read_table('./data/horseColicTest.txt',header=None)
# 先把所有数据级联到一起
samples = pd.concat((part1,part2))
# 拆分
train = samples.values[:,:-1]
target = samples.values[:,-1]
# 训练数据
from sklearn.preprocessing import StandardScaler,MinMaxScaler,Normalizer

train1 = StandardScaler().fit_transform(train)
train2 = MinMaxScaler().fit_transform(train)
train3 = Normalizer().fit_transform(train)

trains = [train1,train2,train3]
feature_project_names = ['StandardScaler','MinMaxScaler','Normalizer']

from sklearn.model_selection import train_test_split


# 预测不同的特征处理，对算法的影响
# 使用此函数找到一个好的特征处理方案
def score_with_model(model,trains,target,feature_project_names):
    for i,train in enumerate(trains):
        X_train,X_test,y_train,y_test = train_test_split(train,target,random_state=1)
        score = model.fit(X_train,y_train).score(X_test,y_test)
        print("{} 特征处理{} 得分:{}".format(model.__class__.__name__,feature_project_names[i],score))
        
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier

score_with_model(KNeighborsClassifier(),trains,target,feature_project_names)
score_with_model(LogisticRegression(),trains,target,feature_project_names)
score_with_model(DecisionTreeClassifier(),trains,target,feature_project_names)
score_with_model(SVC(),trains,target,feature_project_names)

# 交叉验证，获得好的算法
# 先使用MinMaxScaler处理
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score

def select_best_model(model,train,target):
    kfold = KFold(n_splits=10)
    results = cross_val_score(model,train,target,cv=kfold)
    print("{}算法 平均值是{}，方差是{}，最大值{},最小值{}".format(model.__class__.__name__,results.mean(),results.std(),results.max(),results.min()))
    print(results)
    
select_best_model(KNeighborsClassifier(),train2,target)
select_best_model(LogisticRegression(),train2,target)
select_best_model(DecisionTreeClassifier(),train2,target)
select_best_model(SVC(),train2,target)

# 拆分样本集
X_train,X_test,y_train,y_test = train_test_split(train2,target,test_size=0.1)

# 综合考虑，选择SVC算法，使用标准化对数据特征进行预处理
# 算法调参，使用GridSearchCV
from sklearn.model_selection import GridSearchCV

svc = SVC()
param_dic = {
    'kernel':['linear','rbf','poly'],
    'C':[0.001,0.01,0.1,1,10,100],
    'gamma':np.arange(0,100,10)
}

gridCV = GridSearchCV(svc,param_grid=param_dic)

gridCV.fit(X_train,y_train)

best_svc = gridCV.best_estimator_
best_svc.score(X_test,y_test)

X_train,X_test,y_train,y_test = train_test_split(train,target,test_size=0.1)
SVC(C=0.001,kernel='poly',gamma=10).fit(X_train,y_train).score(X_test,y_test)

```

后续

```python
# 特征没选好，原始数据比特征处理过的数据表现要好
# 特征选择
# 降维

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA

# 因为要使用lda进行有监督的降维处理，所以，先使用pca查看大概有多少个特征起主导作用
train4 = PCA(n_components=0.97).fit_transform(train)

X_train,X_test,y_train,y_test = train_test_split(train4,target,test_size=0.1)
SVC(C=0.001,kernel='rbf',gamma=10).fit(X_train,y_train).score(X_test,y_test)

X_train,X_test,y_train,y_test = train_test_split(train4,target,test_size=0.1)
SVC().fit(X_train,y_train).score(X_test,y_test)
```

