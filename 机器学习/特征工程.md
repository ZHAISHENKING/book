## 特征工程

> **思考**
>
> 机器学习的算法最终预测结果很大程度与特征的筛选，清洗等有很大的关系，如何使特征的选取有章可循？
>
> 这就是本文特征工程的重点

1. 特征的使用方案

   > - 原则上根据业务，尽可能找出对因变量有影响的所有自变量
   > - 可用性评估:获取难度、覆盖率、准确率

2. 特征处理

   > - 特征清洗：包括清洗异常样本、采样（数据不均衡，样本权重）
   > - 特征预处理（重点）

3. 特征监控

   > - 特征有效性分析
   > - 监控重要特征，防止特征质量下降，影响模型效果

我们将特征处理中的`预处理`与特征如何`选择`作为重点来分析

### 数据预处理

```python
from sklearn.preporcessing
```

**无量纲化**:

- 区间缩放法

  > 区间缩放法利用了边界值信息，将特征的取值区间缩放到某个特点的范围，例如[0, 1]等
  >
  > 返回值为缩放到[0, 1]区间的数据
  >
  > ```python
  > from sklearn.preprocessing import MinMaxScaler
  > MinMaxScaler().fit_transform(iris.data)
  > ```

- 标准化

  > 标准化的前提是`特征值服从正态分布`，标准化后，其转换成标准正态分布。
  >
  > 标准化需要计算特征的均值和标准差
  > 其中S为标准差
  >
  > ```python
  > from sklearn.preprocessing import StandardScaler
  > StandardScaler().fit_transform(iris.data)
  > ```

- 归一化

  > `标准化`是依照特征矩阵的`列`处理数据，其通过求z-score的方法，将样本的特征值转换到同一量纲下。`归一化`是依照特征矩阵的`行`处理数据，其目的在于样本向量在点乘运算或其他核函数计算相似性时，拥有统一的标准，也就是说都转化为“单位向量”。
  >
  > ```python
  > from sklearn.preprocessing import Normalizer
  > Normalizer().fit_transform(iris.data)
  > ```

**对定量特征二值化**

> 定量特征二值化的核心在于设定一个阈值，大于阈值的赋值为1，小于等于阈值的赋值为0
>
> ```python
> from sklearn.preprocessing import Binarizer
> ```
>
> 

**对定性特征哑编码 (独热编码)**

> ```python
> from sklearn.preprocessing import OneHotEncoder
> # 对文本数据进行数字编码
> from sklearn.preprocessing import LabelEncoder
> ```

**缺失值计算**

> 缺失值计算，返回值为计算缺失值后的数据
> 参数missing_value为缺失值的表示形式，默认为NaN
> 参数strategy为缺失值填充方式，默认为mean（均值）
>
> ```python
> from numpy import vstack, array, nan
> from sklearn.preprocessing import Imputer
> # 例
> Imputer().fit_transform(vstack((array([nan, nan, nan, nan]), iris.data)))
> ```
>
> 

### 特征选择

- 按照`特征是否发散`选择：类似于方差，数据越集中越稳定
- 按照特征与目标的`相关性强弱`选择
- 依据经验选择

**实现方式**

> ```python
> from sklearn import feature_selection
> ```
>
> - Filter（过滤法）
>
>   ```python
>   # 按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征
>    from sklearn.feature_selection import VarianceThreshold
>   
>   # 方差选择法，返回值为特征选择后的数据
>   # 参数threshold为方差的阈值
>   VarianceThreshold(threshold=0.5).fit_transform(iris.data)
>   ```
>
>   
>
> - Wrapper(包装法)
>
>   根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。
>
>   ```python
>   from sklearn.feature_selection import RFE
>   from sklearn.linear_model import LogisticRegression
>   
>   #递归特征消除法，返回特征选择后的数据
>   #参数estimator为基模型
>   #参数n_features_to_select为选择的特征个数
>   RFE(estimator=LogisticRegression(), n_features_to_select=2).fit_transform(iris.data, iris.target)
>   ```
>
>   
>
> - Embedded(集成法)
>
>   先使用某些机器学习的算法和模型进行训练，得到各个特征的`权值系数`，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。
>
>   - 基于惩罚项的特征选择法
>
>     使用带惩罚项的基模型，除了筛选出特征外，同时也进行了降维。
>
>     ```python
>     from sklearn.feature_selection import SelectFromModel
>     from sklearn.linear_model import LogisticRegression
>     
>     #带L1惩罚项的逻辑回归作为基模型的特征选择
>     
>     # 最终选择的特征个数由C决定
>     SelectFromModel(LogisticRegression(penalty="l1", C=0.1)).fit_transform(iris.data, iris.target)
>     ```
>
>     
>
>   - 基于树模型的特征选择法
>
>     树模型中GBDT也可用来作为基模型进行特征选择，使用feature_selection库的SelectFromModel类结合GBDT模型
>
>     ```python
>     from sklearn.feature_selection import SelectFromModel
>     from sklearn.ensemble import GradientBoostingClassifier
>     
>     #GBDT作为基模型的特征选择
>     SelectFromModel(GradientBoostingClassifier()).fit_transform(iris.data, iris.target)
>     ```
>
>     *算法思想*
>
>     GBDT的思想可以用一个通俗的例子解释，假如有个人30岁，我们首先用20岁去拟合，发现损失有10岁，这时我们用6岁去拟合剩下的损失，发现差距还有4岁，第三轮我们用3岁拟合剩下的差距，差距就只有一岁了。如果我们的迭代轮数还没有完，可以继续迭代下面，每一轮迭代，拟合的岁数误差都会减小。
>
>   

### 数据挖掘流程

<img src="http://qiniu.s001.xin/knn/wajue.jpg" width="600px">

